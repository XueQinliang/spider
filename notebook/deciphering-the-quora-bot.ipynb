{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"17736c5a-70df-b876-2003-fe7609a2cdf6"},"source":"Getting common words\n--------------------\n\nNo syntax and semantics analysis here. Going simple to analyse all the common words in the two question sets and visualizing them."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a16ca6f-1e77-79a8-d0b3-747999396253"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntry:\n    t_file = pd.read_csv('../input/questions.csv', encoding='ISO-8859-1')\n    t1_file = pd.read_csv('../input/questions.csv', encoding='ISO-8859-1')\n    print('File load: Success')\nexcept:\n    print('File load: Failed')"},{"cell_type":"markdown","metadata":{"_cell_guid":"76b1d364-c0bd-135c-86cc-bf760048b5d4"},"source":"**Removing stop words from ntlk**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"115d6aa1-6f06-4015-47e8-eed3627185a6"},"outputs":[],"source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\nprint(stop)"},{"cell_type":"markdown","metadata":{"_cell_guid":"33dabeea-fa7c-388e-3630-be04a0bc1c87"},"source":"CLEANING\n--------\n\n**Simply dropped null values, split each of the question strings and removed stops**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eedadf8f-ff45-b37e-c90c-a81cb8069269"},"outputs":[],"source":"t_file = t_file.dropna()\nt_file['question1'] = t_file['question1'].str.lower().str.split()\nt_file['question2'] = t_file['question2'].str.lower().str.split()\nt_file['question1'] = t_file['question1'].apply(lambda x: [item for item in x if item not in stop])\nt_file['question2'] = t_file['question2'].apply(lambda x: [item for item in x if item not in stop])"},{"cell_type":"markdown","metadata":{"_cell_guid":"b85e19e5-d019-08b1-2332-0ee424802cae"},"source":"**Finding common word percentage and average word lengths**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de3034a8-f471-6bf5-abee-2bd977592adf"},"outputs":[],"source":"t_file['Common'] = t_file.apply(lambda row: len(list(set(row['question1']).intersection(row['question2']))), axis=1)\nt_file['Average'] = t_file.apply(lambda row: 0.5*(len(row['question1'])+len(row['question2'])), axis=1)\nt_file['Percentage'] = t_file.apply(lambda row: row['Common']*100.0/row['Average'], axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"fcdfdb3c-5cf7-fb51-9f8f-9296016d4308"},"source":"**True and False plotting of data**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19969e4e-3808-9753-b6eb-66f662f1ea77"},"outputs":[],"source":"y = t_file['Percentage'][t_file['is_duplicate']==0].values\nx = t_file['Average'][t_file['is_duplicate']==0].values\n\nfig, axs = plt.subplots(ncols=2, sharey=True, figsize=(7, 4))\nfig.subplots_adjust(hspace=0.5, left=0.07, right=0.93)\nax = axs[0]\nhb = ax.hexbin(x, y, gridsize=70, bins='log', cmap='inferno')\nax.axis([0, 20, 0, 100])\nax.set_title(\"Duplicates\")\ncb = fig.colorbar(hb, ax=ax)\ncb.set_label('log10(N)')\n\n\ny = t_file['Percentage'][t_file['is_duplicate']==1].values\nx = t_file['Average'][t_file['is_duplicate']==1].values\nax = axs[1]\nhb = ax.hexbin(x, y, gridsize=70, bins='log', cmap='inferno')\nax.axis([0, 20, 0, 100])\nax.set_title(\"Not duplicates\")\ncb = fig.colorbar(hb, ax=ax)\ncb.set_label('log10(N)')\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f101da5a-db34-9ef5-9b1f-1a4f8b45c143"},"source":"**Scatters are a must**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad8a079a-0809-dc38-96df-022bad586df3"},"outputs":[],"source":"x = t_file['Percentage'][t_file['is_duplicate']==0].values\ny = t_file['qid1'][t_file['is_duplicate']==0].values\narea = t_file['Average'][t_file['is_duplicate']==0].values\n\nplt.scatter(x, y, s=area*3, c='r', alpha=0.1)\n\nx = t_file['Percentage'][t_file['is_duplicate']==1].values\ny = t_file['qid1'][t_file['is_duplicate']==1].values\narea = t_file['Average'][t_file['is_duplicate']==1].values\n\nplt.scatter(x, y, s=area*3, c='b', alpha=0.1)\n\nplt.ylabel('Question IDs')\nplt.xlabel('Percentage of common words')\n\nplt.title(\"Percentages of common words in questions\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"86ead85c-21cc-e3ac-c8a8-cbe7d0e49ff5"},"source":" Cosine, Jaccard and Shingling\n--------------------------------\n\nThe first, naive approach towards identifying question pairs -- Strip the stopwords, stem the remaining and do a simple Cosine/Jaccard Test. K-Shingling is also a popular technique, where continuous subsets of \"k\" words are matched between the two documents.\nHowever, a major drawback with the above is that of a lack of semantic understanding -- There might be two questions with a high percentage of common words, but different meanings."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0e39a2a-0c78-37d2-d3b5-bb26fcf83edb"},"outputs":[],"source":"from collections import Counter\nimport re, math\ndef get_cosine(vec1, vec2):\n    vec1 = Counter(vec1)\n    vec2 = Counter(vec2)\n    intersection = set(vec1.keys()) & set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e253607b-da1c-afae-05e2-74734161264f"},"outputs":[],"source":"t_file['Cosine'] = t_file.apply(lambda row: get_cosine(row['question1'],row['question2']), axis=1)\nprint(t_file.head(n=5))"},{"cell_type":"markdown","metadata":{"_cell_guid":"3047f9ae-af4f-e5bd-250c-1fb9d801f4a9"},"source":"Jaccard Similarity\n------------------\n\nJaccard Similarity is given by s=p/(p+q+r)\nwhere,\n\n- p = # of attributes positive for both objects \n- q = # of attributes 1 for i and 0 for j \n- r = # of attributes 0 for i and 1 for j "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f23a24a-1fa0-9a19-6399-be6155c48a65"},"outputs":[],"source":"t_file['Jaccard'] = t_file.apply(lambda row: 0 if (len(row['question1'])+len(row['question2'])-row['Common']) == 0  else float(row['Common'])/((len(row['question1'])+len(row['question2'])-row['Common'])), axis=1)\nprint(t_file.head(n=5))"},{"cell_type":"markdown","metadata":{"_cell_guid":"b92796f1-9d98-b937-ffeb-c4b2dbe9864f"},"source":"Semantic Similarity via Wordnet\n-------------------------------\n\nWordnet is a huge library of synsets for almost all words in the English dictionary. The synsets for each word describe its meaning, part of speeches, and synonyms/antonyms. The synonyms help in identifying the semantic meaning of the sentence, when all words are taken together.\n\n[This][1] paper describes how wordnet is used to calculate a matrix similarity between two sentences. Later a thresholding for paraphrases is done, they could come up with a F-Score of 82.4 on the Microsoft Research Paraphrase Corpus, the industry standard.\n\n\n  [1]: http://staffwww.dcs.shef.ac.uk/people/S.Fernando/pubs/clukPaper.pdf\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"51be3697-e590-d993-9156-cfd0eee44cb5"},"source":"So what are the most common words? Let's take a look at a word cloud.\n---------------------------------------------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc90100c-4993-823e-9104-71ec0fad3999"},"outputs":[],"source":"train_qs = pd.Series(t1_file['question1'].tolist() + t1_file['question2'].tolist()).astype(str)\nfrom wordcloud import WordCloud\ncloud = WordCloud(width=1440, height=1080).generate(\" \".join(train_qs.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')"},{"cell_type":"markdown","metadata":{"_cell_guid":"f781d340-ef9f-b0d1-8849-5b3ca25f8a90"},"source":"Word Embeddings\n---------------\n\nA recent trend in the Deep NLP community, starting with the famous Word2Vec and CBOW, and now Doc2Vec, Paragraph2Vec, skip-thought vectors coming along! These are extremely powerful models which have changed the scope of NLP models in the last 3-4 years.\n\n- CNN over Word Embeddings: [This][1] research paper explains the approach of applying Convolutional Neural Nets over the word embeddings (using a large collection of unlabeled data), building vector representations for question pairs. They tested their results over AskUbuntu, witnessing a 92.4% test accuracy!\n\n- Skip-thought Vectors: This model backs upon its ability to semantically understand a sentence, thus the transition from the old skip-gram to skip-thought. Ryan Kiros is the lead developer behind this model, do have a look at his [Github Repo][2] and its application on Paraphrase Detection. And let me know if you can replicate his results!\n\n\n  [1]: https://aclweb.org/anthology/K15-1013\n  [2]: https://github.com/ryankiros/skip-thoughts"},{"cell_type":"markdown","metadata":{"_cell_guid":"035b5cc5-a0e3-3a59-1893-66c408c8cb43"},"source":"Observations\n------------\n\nFrom the final plot it is pretty clearly visible that the ones that are clustered towards the 100% mark are nearly all blue. This states the fact that questions having a lot of common strings are termed as equivalent more often than not.\n\nAlso as seen in the hex plot,  non duplicates are clustered towards the 100% area more than the duplicate ones.\n\nHave we decoded the Quora bot? Not at all. "},{"cell_type":"markdown","metadata":{"_cell_guid":"14a2ca87-0321-ec97-1614-920066b495eb"},"source":"**Special mention : [Script][1] by [Shubh24][2]**\n\n\n  [1]: https://www.kaggle.com/shubh24/d/quora/question-pairs-dataset/everything-you-wanna-know\n  [2]: https://www.kaggle.com/shubh24"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}