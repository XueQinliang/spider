{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"7a40ffc3-9df9-8a26-209c-2a098f986b07"},"source":"# AUC Explorations\nIn this notebook we try to establish what the difference is when you calculate an AUC score for several models, between\n  1. calculating the AUC on the combined model outputs (\"Combined AUC\"), or\n  1. calculating the AUC on each model and take the average (\"Average AUC\")\n \nThere is a lot of discussion on the forums about the relative validity of these two approaches.\n  \nWe investigate this question by simulating submission data and calculating the AUC by the two methods above. The simulation is based on a model that generates prediction probabilities for three patients. The model captures these features:\n  * the class probabilities (what fraction of samples are of class 1)\n  * prediction accuracy\n \nTo model the prediction accuracy, we need to pick a distribution over [0, 1], since the predictions are probabilities. We choose the beta distribution mostly because it looks about right. (See the plots below to convince yourself of this.)\n\n$$ \\mathrm{pdf}_{\\mathrm{beta}}(x; a, b) = \\frac{\\Gamma(a+b) \\, x^{(a-1)} \\, (1-x)^{(b-1)}}{\\Gamma(a) \\, \\Gamma(b)} $$\n\nThe general idea is that when presented with a class 0 sample, the prediction is a draw from a beta distribution that is skewed toward 0. For class 1, the skew is toward 1. By modifying the parameters of the beta distribution we can adjust its skew and kurtosis in a way that plausibly captures the prediction accuracy of a solution.\n\nWe use the same parameters $a, b$ in both cases, but swap them, using the property of the beta distribution that\n\n$$ \\mathrm{pdf}_{\\mathrm{beta}}(1-x;a,b) = \\mathrm{pdf}_{\\mathrm{beta}}(x;b,a) $$\n\nIn other words, we make the simplifying assumption that the accuracies for class 0 and class 1 are the same.\n\nThe code below is set up for three experiments:\n\n  1. different class probabilities, same accuracies\n  1. same class probabilities, different accuracies\n  1. different class probabilities, different accuracies"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01cf7403-7515-309d-91a3-31cab152b418"},"outputs":[],"source":"%matplotlib inline\n\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import beta\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score\n\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"678deb45-55b2-28b2-c0fc-2497ad85f443"},"outputs":[],"source":"def quantile_normalize(trg, ref):\n    \"\"\"Quantile normalize target ndarray to match distribution of reference ndarray\"\"\"\n    \n    # We sort the target in a way that lets us unsort it later\n    idx = np.argsort(trg)\n    \n    # Calculate the percentile points of the target\n    n_trg = len(trg)\n    percs = np.arange(n_trg) * 100 / (n_trg - 1)\n    \n    # Calculate the probabilities at the same percentile points in the reference\n    probs = np.percentile(ref, percs)\n    \n    # Now unsort those probabilities\n    idx2 = np.argsort(idx)\n    trg2 = probs[idx2]\n    \n    return trg2"},{"cell_type":"markdown","metadata":{"_cell_guid":"d826aa1f-7b5a-6ea2-8cd1-8eb5e08f05e4"},"source":"## Set the model parameters\nChange the `experiment` variable below to use one of the preset experiments, or tweak the parameters yourself.\n\nSmaller values of `b` correspond to less accurate models. The value of `a` is probably best left at 2."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a0ca7e9-0cc1-d9c4-0ff1-5f261924c128"},"outputs":[],"source":"n_patients = 3\nexperiment = 3\nif experiment == 1:  # different class probs, same accuracy\n    class_probs = [.1, .2, .3]\n    a = [2, 2, 2]\n    b = [3, 3, 3]\nelif experiment == 2:  # same class probs, different accuracies\n    class_probs = [.15, .15, .15]\n    a = [2, 2, 2]\n    b = [2, 4, 6]\nelif experiment == 3:  # different class probs, different accuracies\n    class_probs = [.15, .20, .25]\n    a = [2, 2, 2]\n    b = [3.5, 4.0, 4.5]"},{"cell_type":"markdown","metadata":{"_cell_guid":"dac47c47-e23e-d2aa-5888-db219019b0f7"},"source":"The plots below are just a sanity check to show the distribution for class 0 predictions generated by our model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1971041-136d-dcbc-a8dc-73685afc9980"},"outputs":[],"source":"# Sanity check: generate draws and plot\nfig = plt.figure(figsize=(9, 3))\nax = {}\nfor p in range(n_patients):\n    if p == 0:\n        ax[p] = plt.subplot(1, 3, p + 1)\n    else:\n        ax[p] = plt.subplot(1, 3, p + 1, sharey=ax[0])\n    plt.xlim(0, 1)\n    plt.title('Patient {} - Class 0'.format(p + 1))\n    probs = beta.rvs(a[p], b[p], size=1000)\n    sns.distplot(probs, kde=None, fit=beta)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9b2dd53b-2093-f069-1360-069978ef9fad"},"source":"## Generate sample points\nWe generate predictions for class 0 and class 1 points in proportion to the class probabilites of the model. As described above, the predictions are generated by drawing from the beta distribution. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3506c5f-e77b-bcd8-c202-ad6459b4c239"},"outputs":[],"source":"n_points = [1000, 2000, 4000]\ny_score = []\ny_true = []\nfor p in range(n_patients):\n    n_1 = int(n_points[p] * class_probs[p] + 0.5)\n    n_0 = n_points[p] - n_1\n    \n    # Class 0 predictions\n    y_s = list(beta.rvs(a[p], b[p], size=n_0))\n    y_t = [0] * n_0\n    \n    # Class 1 predictions\n    y_s += list(beta.rvs(b[p], a[p], size=n_1))\n    y_t += [1] * n_1\n    \n    y_score.append(y_s)\n    y_true.append(y_t)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a6100e34-e067-aaca-87bb-d5faaa33fa61"},"source":"And again, here are some plots to sanity check that the distribution of probabilities (for both classes now) look plausible. If you're not convinced, plot some of your submission accuracies and compare."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf6b3652-d0b6-e60a-1f60-63e50588e010"},"outputs":[],"source":"fig = plt.figure(figsize=(9, 3))\nax = {}\nfor p in range(n_patients):\n    plt.subplot(1, 3, p + 1)\n    if p == 0:\n        ax[p] = plt.subplot(1, 3, p + 1)\n    else:\n        ax[p] = plt.subplot(1, 3, p + 1, sharey=ax[0])\n    plt.xlim(0, 1)\n    plt.title('Patient {}'.format(p + 1))\n    sns.distplot(y_score[p])"},{"cell_type":"markdown","metadata":{"_cell_guid":"a221a13e-c6b5-88ea-d054-ae84b38ab58f"},"source":"## Calculate AUC"},{"cell_type":"markdown","metadata":{"_cell_guid":"e18e71c5-d0af-994a-502e-2b7ad1fc2cb2"},"source":"### Method 1: Average of individual AUC scores"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8bb3ba18-645d-dafb-cc5a-2df6b73e24c5"},"outputs":[],"source":"auc = {}\nfor p in range(n_patients):\n    auc[p] = roc_auc_score(y_true[p], y_score[p])\n    print('Patient {} AUC: {:.3f}'.format(p + 1, auc[p]))\nauc_mean = sum(auc.values()) / len(auc)\nprint('--------------------')\nprint('Average AUC:   {:.3f}'.format(auc_mean))"},{"cell_type":"markdown","metadata":{"_cell_guid":"f8885769-e027-8e33-46b5-5620c7383a6e"},"source":"### Method 2: Combined AUC score"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f4778fc-4383-3919-1937-a86c92ff5347"},"outputs":[],"source":"y_score_all = np.concatenate(y_score)\ny_true_all = np.concatenate(y_true)\nauc_all = roc_auc_score(y_true_all, y_score_all)\nprint('Combined AUC:  {:.3f}'.format(auc_all))"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd3b2720-f702-980a-95d2-58b7d1b6ed87"},"source":"### Method 3: Normalize before calculating combined AUC score\nWe use quantile normalization to give the same distribution to each of the individual patient's predictions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5fe2dbe5-25da-9531-04a7-fb96d3d2eefb"},"outputs":[],"source":"y_score_norm = []\nfor p in range(n_patients):\n    y_score_norm.append(quantile_normalize(y_score[p], y_score[2]))\ny_score_all_norm = np.concatenate(y_score_norm)\nauc_all_norm = roc_auc_score(y_true_all, y_score_all_norm)\nprint('Normed AUC:    {:.3f}'.format(auc_all_norm))"},{"cell_type":"markdown","metadata":{"_cell_guid":"77c326e6-5d90-254f-195d-9c4ad5559800"},"source":"## Discussion"},{"cell_type":"markdown","metadata":{"_cell_guid":"5616b2fa-f344-2ec8-de65-9d574b155d3f"},"source":"For most combinations of class probabilities and model accuracies, `Combined AUC > Normed AUC > Average AUC`. "},{"cell_type":"markdown","metadata":{"_cell_guid":"1541b11c-9107-e3fc-a5b6-4711f03c18df"},"source":"## Conclusion"},{"cell_type":"markdown","metadata":{"_cell_guid":"d0bc7930-5646-7671-1934-fc4dd094f32d"},"source":"Even if you believe that the combined AUC is a flawed scoring system, you probably can't fix it by normalizing the distributions of the individual patient's predictions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66ddee0e-744d-d983-635a-50e04de623df"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}