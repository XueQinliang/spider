{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"4f614d81-4b42-cc37-fb34-4b7a18d18783"},"source":"# Predicting the Number of Daily Trips"},{"cell_type":"markdown","metadata":{"_cell_guid":"fda92f92-4efc-8f4f-14c3-6c59289a23e4"},"source":"After performing an exploratory analysis on the bike sharing services in San Francisco and Seattle (https://github.com/Currie32/Bike-Sharing-in-SF-and-Seattle), I wanted to follow this up by building a predictive model. The goal for this report is to create a model that can accurately predict the number of trips taken, on a given day, with San Francisco's bike sharing service. I will only be using information that the bike sharing company could know at the start of the day, i.e. weather report, number of bikes availble, type of day (business day vs holiday vs weekend). "},{"cell_type":"markdown","metadata":{"_cell_guid":"968cae86-76d9-f5fa-dc21-39f6db920dab"},"source":"### Load the Packages"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6c98489-6f9f-6a0b-d9a1-497cf0ffa327"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats.stats import pearsonr  \nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom pandas.tseries.offsets import CustomBusinessDay\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.cross_validation import KFold\nfrom sklearn.metrics import mean_squared_error, median_absolute_error\nimport xgboost as xgb"},{"cell_type":"markdown","metadata":{"_cell_guid":"9307c62b-d56e-9714-f822-0214122f4690"},"source":"### Import the Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a59ee42-9e68-e453-f6f5-bfe3d34a370f"},"outputs":[],"source":"df = pd.read_csv(\"../input/trip.csv\")\nweather = pd.read_csv(\"../input/weather.csv\")\nstations = pd.read_csv(\"../input/station.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"9fe0d513-5196-60e2-971f-3d330b649117"},"source":"## Explore the Trips data frame\n\nIf you would like to see a comprehensive exploration of this data, please visit my other report: https://github.com/Currie32/Bike-Sharing-in-SF-and-Seattle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c9c4810-3e50-73bc-74e2-2dd5f7aeb0a9"},"outputs":[],"source":"df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"742d0e33-951b-2473-a60c-8779649d5ddb"},"outputs":[],"source":"df.isnull().sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9990604b-c6d4-e131-447e-6ab8c89fb8e0"},"outputs":[],"source":"df.duration.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2155087f-eba8-5d86-7659-0920db808027"},"outputs":[],"source":"#Change duration from seconds to minutes\ndf.duration /= 60"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c248376-bbef-3f1d-526c-75f4a709d54f"},"outputs":[],"source":"df.duration.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3cc1cebf-6586-6722-237b-3bc9b58d9e1c"},"outputs":[],"source":"#I want to remove major outliers from the data; trips longer than 6 hours. This will remove less than 0.5% of the data.\ndf['duration'].quantile(0.995)\ndf = df[df.duration <= 360]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7dc7ef14-7146-1ef0-fa06-f42b514d027d"},"outputs":[],"source":"df.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f6296fc2-26df-1754-375e-c2e1f1812e37"},"outputs":[],"source":"#Convert to datetime so that it can be manipulated more easily\ndf.start_date = pd.to_datetime(df.start_date, format='%m/%d/%Y %H:%M')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b89f173-2a9b-bb64-fa61-557fe85e96fe"},"outputs":[],"source":"#Extract the year, month, and day from start_date\ndf['date'] = df.start_date.dt.date"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1a384313-9853-2068-63a2-cc184461f7d3"},"outputs":[],"source":"#Each entry in the date feature is a trip. \n#By finding the total number of times a date is listed, we know how many trips were taken on that date.\ndates = {}\nfor d in df.date:\n    if d not in dates:\n        dates[d] = 1\n    else:\n        dates[d] += 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"285f07ed-e98e-e6bb-c4fc-e35214538147"},"outputs":[],"source":"#Create the data frame that will be used for training, with the dictionary we just created.\ndf2 = pd.DataFrame.from_dict(dates, orient = \"index\")\ndf2['date'] = df2.index\ndf2['trips'] = df2.ix[:,0]\ntrain = df2.ix[:,1:3]\ntrain.reset_index(drop = True, inplace = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9019d87-b962-ed15-570f-3bfabb46acfd"},"outputs":[],"source":"train"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c02cd20-a7c1-3011-47a7-6345ef17f8ff"},"outputs":[],"source":"#All sorted now!\ntrain = train.sort('date')\ntrain.reset_index(drop=True, inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"868ae0f1-ba66-643b-7545-99f520e43905"},"source":"## Explore the Weather data frame"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a0197f99-9a38-9667-394d-45ddb9fee182"},"outputs":[],"source":"weather.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f9c1ffc-8eb3-c4f4-d268-65671bf91e85"},"outputs":[],"source":"weather.isnull().sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27a06ba0-bdc2-964b-106c-ea514f891585"},"outputs":[],"source":"weather.date = pd.to_datetime(weather.date, format='%m/%d/%Y')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd3f8441-a973-6789-9f3a-a0cd4bf25a36"},"outputs":[],"source":"#The weather data frame is 5 times as long as the train data frame, \n#therefore there are 5 entries per date.\nprint (train.shape)\nprint (weather.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"62413ca2-1e98-04e6-9b8e-65753a3463fb"},"outputs":[],"source":"#It seems we have one entry per zip code\nweather.zip_code.unique()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1c5b7c7e-eeb0-4545-b651-db1586596dd3"},"outputs":[],"source":"#Let's see which zip code has the cleanest date.\nfor zc in weather.zip_code.unique():\n    print (weather[weather.zip_code == zc].isnull().sum())\n    print ()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"820f17f0-f671-0f8e-645a-13b4173d7cae"},"outputs":[],"source":"#I used this zip code for my other report as well. It is missing only a bit of data and is formatted rather well.\nweather = weather[weather.zip_code == 94107]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"908c00a6-0d9e-8ec5-b63e-dfe780a5b780"},"outputs":[],"source":"weather.events.unique()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"14b0a357-5929-a187-8d43-35be5ef06cc0"},"outputs":[],"source":"weather.loc[weather.events == 'rain', 'events'] = \"Rain\"\nweather.loc[weather.events.isnull(), 'events'] = \"Normal\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd276779-e065-1401-7f3a-1e7eacb0946e"},"outputs":[],"source":"weather.events"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c8cf344-496a-6d9a-402c-cb73050e3d4a"},"outputs":[],"source":"events = pd.get_dummies(weather.events)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3ed0574-c4f8-ae58-da5c-71f75e8a44b4"},"outputs":[],"source":"weather = weather.merge(events, left_index = True, right_index = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b72a08f5-810f-9b1a-0814-677db2bd3478"},"outputs":[],"source":"#Remove features we don't need\nweather = weather.drop(['events','zip_code'],1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a954cbfa-800c-3f2d-507e-e834df36fdb4"},"outputs":[],"source":"#max_wind and max_gust are well correlated, so we can use max_wind to help fill the null values of max_gust\nprint (pearsonr(weather.max_wind_Speed_mph[weather.max_gust_speed_mph >= 0], \n               weather.max_gust_speed_mph[weather.max_gust_speed_mph >= 0]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7687525a-5155-9cbb-b481-dd81a8118d56"},"outputs":[],"source":"#For each value of max_wind, find the median max_gust and use that to fill the null values.\nweather.loc[weather.max_gust_speed_mph.isnull(), 'max_gust_speed_mph'] = weather.groupby('max_wind_Speed_mph').max_gust_speed_mph.apply(lambda x: x.fillna(x.median()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"929fdbed-776f-ef1b-90c2-a594c959dc51"},"outputs":[],"source":"weather.isnull().sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5fed584-a598-3385-a09a-a9d87a6b8f80"},"outputs":[],"source":"#Change this feature from a string to numeric.\n#Use errors = 'coerce' because some values currently equal 'T' and we want them to become NAs.\nweather.precipitation_inches = pd.to_numeric(weather.precipitation_inches, errors = 'coerce')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c460b22b-ece2-3d0c-81cc-920b9e199a52"},"outputs":[],"source":"#Change null values to the median, of values > 0, because T, I think, means True. \n#Therefore we want to find the median amount of precipitation on days when it rained.\nweather.loc[weather.precipitation_inches.isnull(), \n            'precipitation_inches'] = weather[weather.precipitation_inches.notnull()].precipitation_inches.median()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"086e0d51-f0d9-e32b-58f0-7ff1a6cfe548"},"outputs":[],"source":"train = train.merge(weather, on = train.date)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"343eee58-420d-0cc0-139b-09959f7fbe38"},"outputs":[],"source":"#Need to remove the extra date columns, otherwise good!\ntrain.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abef6d97-51d2-6df8-4092-745cda066482"},"outputs":[],"source":"train['date'] = train['date_x']\ntrain.drop(['date_y','date_x'],1, inplace= True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a8d94bf0-3e6e-723f-6072-3e7f26a9e010"},"source":"## Explore the Stations data frame"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"125b3b8f-012c-8aae-8de1-6b81957f315f"},"outputs":[],"source":"stations.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"181a1450-d772-1abe-140c-383b3da85865"},"outputs":[],"source":"#Good, each stations is only listed once\nprint (len(stations.name.unique()))\nprint (stations.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30b216ce-cf6e-8515-8177-00b4236980f6"},"outputs":[],"source":"stations.installation_date = pd.to_datetime(stations.installation_date, format = \"%m/%d/%Y\").dt.date"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed3ab6dd-a3cd-8c19-2645-11f63682224e"},"outputs":[],"source":"#The min date is before any in the train data frame, therefore stations were installed before the first trips (good).\n#The max date is before the end of the train data frame, therefore the service has not been adding new stations recently.\nprint (stations.installation_date.min())\nprint (stations.installation_date.max())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23569f0c-39e7-2013-30b2-d7a86b11c5eb"},"outputs":[],"source":"#For each day in train.date, find the number of docks (parking spots for individual bikes) that were installed \n#on or before that day.\ntotal_docks = []\nfor day in train.date:\n    total_docks.append(sum(stations[stations.installation_date <= day].dock_count))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bad3b7d5-b8af-add8-6245-c15c95260c31"},"outputs":[],"source":"train['total_docks'] = total_docks"},{"cell_type":"markdown","metadata":{"_cell_guid":"f0d3e458-cb1c-2c00-55b1-13313992e47a"},"source":"## Add Special Date Features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ad5f307-419a-b3e7-6ef9-14343eeaed14"},"outputs":[],"source":"#Find all of the holidays during our time span\ncalendar = USFederalHolidayCalendar()\nholidays = calendar.holidays(start=train.date.min(), end=train.date.max())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fb4fc7aa-2f77-c189-09e2-bb8d4bfb042d"},"outputs":[],"source":"holidays"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c02c54a-54ea-102a-b956-a5bb58a09bc9"},"outputs":[],"source":"#Find all of the business days in our time span\nus_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\nbusiness_days = pd.DatetimeIndex(start=train.date.min(), end=train.date.max(), freq=us_bd)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bcf11cc6-d360-7b38-7b38-086d2cd487ea"},"outputs":[],"source":"business_days"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e8f5e726-2468-5873-629a-45ed2d21d9ac"},"outputs":[],"source":"business_days = pd.to_datetime(business_days, format='%Y/%m/%d').date\nholidays = pd.to_datetime(holidays, format='%Y/%m/%d').date"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09733fa5-77b3-e556-0591-d193ba162e52"},"outputs":[],"source":"#A 'business_day' or 'holiday' is a date within either of the respected lists.\ntrain['business_day'] = train.date.isin(business_days)\ntrain['holiday'] = train.date.isin(holidays)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8e56a80-53e9-4ab2-f18e-cbc7623ae0b1"},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d0b2a1c-de0c-126e-7d67-5bb1f5a1be2d"},"outputs":[],"source":"#Convert True to 1 and False to 0\ntrain.business_day = train.business_day.map(lambda x: 1 if x == True else 0)\ntrain.holiday = train.holiday.map(lambda x: 1 if x == True else 0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a70b91ff-f8d2-2c6e-b361-1a630ae1f6a1"},"outputs":[],"source":"#Convert date to the important features, year, month, weekday (0 = Monday, 1 = Tuesday...)\n#We don't need day because what it represents changes every year.\ntrain['year'] = pd.to_datetime(train['date']).dt.year\ntrain['month'] = pd.to_datetime(train['date']).dt.month\ntrain['weekday'] = pd.to_datetime(train['date']).dt.weekday"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9540c13a-4e14-8eb5-b7ad-9c9a8c75c38c"},"outputs":[],"source":"labels = train.trips\ntrain = train.drop(['trips', 'date'], 1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6d4baf27-ae91-8db5-2095-3fd1729b6cac"},"source":"## Train the Model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0d3b5bf-76ea-03b1-ebbd-d93d99757f95"},"outputs":[],"source":"X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.2, random_state = 2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c202e38a-aa6d-679d-9afc-732c46a1e048"},"outputs":[],"source":"#15 fold cross validation. Multiply by -1 to make values positive.\n#Used median absolute error to learn how many trips my predictions are off by.\n\ndef scoring(clf):\n    scores = cross_val_score(clf, X_train, y_train, cv=15, n_jobs=1, scoring = 'neg_median_absolute_error')\n    print (np.median(scores) * -1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ac3c7c0-cffa-4d85-af0b-a549af4ec510"},"outputs":[],"source":"rfr = RandomForestRegressor(n_estimators = 55,\n                            min_samples_leaf = 3,\n                            random_state = 2)\nscoring(rfr)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3c6047d-a966-31e3-1a2f-0cc1b3a7627f"},"outputs":[],"source":"gbr = GradientBoostingRegressor(learning_rate = 0.12,\n                                n_estimators = 150,\n                                max_depth = 8,\n                                min_samples_leaf = 1,\n                                random_state = 2)\nscoring(gbr)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db42e97e-daaf-645f-00ed-142c95e4162a"},"outputs":[],"source":"dtr = DecisionTreeRegressor(min_samples_leaf = 3,\n                            max_depth = 8,\n                            random_state = 2)\nscoring(dtr)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01fc5a4a-cfa8-f5ea-e5b6-33e639f2ef53"},"outputs":[],"source":"abr = AdaBoostRegressor(n_estimators = 100,\n                        learning_rate = 0.1,\n                        loss = 'linear',\n                        random_state = 2)\nscoring(abr)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"848fad40-2fa0-23b5-0287-15c4d6300f18"},"outputs":[],"source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nrandom_state = 2\nparams = {\n        'eta': 0.15,\n        'max_depth': 6,\n        'min_child_weight': 2,\n        'subsample': 1,\n        'colsample_bytree': 1,\n        'verbose_eval': True,\n        'seed': random_state,\n    }\n\nn_folds = 15 #number of Kfolds\ncv_scores = [] #The sum of the mean_absolute_error for each fold.\nearly_stopping_rounds = 100\niterations = 10000\nprintN = 50\nfpred = [] #stores the sums of predicted values for each fold.\n\ntestFinal = xgb.DMatrix(X_test)\n\nkf = KFold(len(X_train), n_folds=n_folds)\n\nfor i, (train_index, test_index) in enumerate(kf):\n    print('\\n Fold %d' % (i+1))\n    Xtrain, Xval = X_train.iloc[train_index], X_train.iloc[test_index]\n    Ytrain, Yval = y_train.iloc[train_index], y_train.iloc[test_index]\n    \n    xgtrain = xgb.DMatrix(Xtrain, label = Ytrain)\n    xgtest = xgb.DMatrix(Xval, label = Yval)\n    watchlist = [(xgtrain, 'train'), (xgtest, 'eval')] \n    \n    xgbModel = xgb.train(params, \n                         xgtrain, \n                         iterations, \n                         watchlist,\n                         verbose_eval = printN,\n                         early_stopping_rounds=early_stopping_rounds\n                        )\n    \n    scores_val = xgbModel.predict(xgtest, ntree_limit=xgbModel.best_ntree_limit)\n    cv_score = median_absolute_error(Yval, scores_val)\n    print('eval-MSE: %.6f' % cv_score)\n    y_pred = xgbModel.predict(testFinal, ntree_limit=xgbModel.best_ntree_limit)\n    print(xgbModel.best_ntree_limit)\n\n    if i > 0:\n        fpred = pred + y_pred #sum predictions\n    else:\n        fpred = y_pred\n    pred = fpred\n    cv_scores.append(cv_score)\n\nxgb_preds = pred / n_folds #find the average values for the predictions\nscore = np.median(cv_scores)\nprint('Median error: %.6f' % score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5cb9ef34-978d-a63e-2888-6dba173192c7"},"outputs":[],"source":"#Train and make predictions with the best models.\nrfr = rfr.fit(X_train, y_train)\ngbr = gbr.fit(X_train, y_train)\n\nrfr_preds = rfr.predict(X_test)\ngbr_preds = gbr.predict(X_test)\n\n#Weight the top models to find the best prediction\nfinal_preds = rfr_preds*0.32 + gbr_preds*0.38 + xgb_preds*0.3\nprint (\"Daily error of trip count:\", median_absolute_error(y_test, final_preds))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af7d6618-ac73-8625-683f-fbe27f5a506a"},"outputs":[],"source":"#A reminder of the range of values in number of daily trips.\nlabels.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cba68138-16ec-5c72-e89b-a01d637b2756"},"outputs":[],"source":"y_test.reset_index(drop = True, inplace = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"83006cd3-cec3-9502-4513-ac9b21c57afa"},"outputs":[],"source":"fs = 16\nplt.figure(figsize=(8,5))\nplt.plot(final_preds)\nplt.plot(y_test)\nplt.legend(['Prediction', 'Acutal'])\nplt.ylabel(\"Number of Trips\", fontsize = fs)\nplt.xlabel(\"Predicted Date\", fontsize = fs)\nplt.title(\"Predicted Values vs Actual Values\", fontsize = fs)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"00b454d1-40cd-d7ea-770e-118b93ab43e2"},"outputs":[],"source":"#Create a plot that ranks the features by importance.\ndef plot_importances(model, model_name):\n    importances = model.feature_importances_\n    std = np.std([model.feature_importances_ for feature in model.estimators_],\n                 axis=0)\n    indices = np.argsort(importances)[::-1]    \n\n    # Plot the feature importances of the forest\n    plt.figure(figsize = (8,5))\n    plt.title(\"Feature importances of \" + model_name)\n    plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\n    plt.xticks(range(X_train.shape[1]), indices)\n    plt.xlim([-1, X_train.shape[1]])\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb8b91b4-2782-e7d9-86cc-f28c2bad9009"},"outputs":[],"source":"# Print the feature ranking\nprint(\"Feature ranking:\")\n\ni = 0\nfor feature in X_train:\n    print (i, feature)\n    i += 1\n    \nplot_importances(rfr, \"Random Forest Regressor\")\nplot_importances(gbr, \"Gradient Boosting Regressor\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"316f4f41-db59-7019-42ce-b905032f5290"},"source":"The feature importance ranking for the random forest regressor makes more sense to me than for the gradient boosting regressor. Features, such as 'business_day', 'total_docks', and 'month' match better with my exploratory analysis than 'wind_dir_degrees' and 'max_sea_level_pressure_inches.' Although I have not looked at the data yet, perhaps wind from a particular direction correlates with worse weather/cycling conditions."},{"cell_type":"markdown","metadata":{"_cell_guid":"7cf6c9cf-ffb7-28ad-7ab5-9b6c7d289842"},"source":"## Summary"},{"cell_type":"markdown","metadata":{"_cell_guid":"a98ddbd2-bc56-e4ce-dfa6-6e37cc14bc3f"},"source":"I believe that I have made a good model to predict how many trips will occur with San Francisco's bike sharing service. My model has a median absolute error of almost 47 trips per day. This should give the company operating this service a good, general estimate of the traffic that will occur each day. \n\nI like how my model can provide a good estimate while only using information that is available to the company at the start of the day, i.e. weather forecast, type of day (business day, holiday, etc), and number of bikes that are available. There are a number of ways to make this model, or a similar model, more practical/useful, including: predicting the number of daily trips to/from each station, using the number of trips in the morning to predict the number of trips in the afternoon, and predicting when a station will run out of bikes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1a77e9d6-d499-3b35-cb0d-e39c418bc515"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}