{"cells":[{"metadata":{"_uuid":"6ba896a333e40051bb1a2ea5e5f7d74a67941f60"},"cell_type":"markdown","source":"# Andrew Ng Machine Learning  with Python - 1\nThis kernel is  for following Medium Article\n\nWritter is Srikar.\n\nhttps://medium.com/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-1-6b8dd1c73d80\n"},{"metadata":{"_uuid":"7689a5e71294af59255367eccc52baa3c3b661bb"},"cell_type":"markdown","source":"A few months ago I had the opportunity to complete Andrew Ng¡¯s Machine Learning MOOC taught on Coursera. It serves as a very good introduction for anyone who wants to venture into the world of AI/ML. But the catch¡­.this course is taught in Octave.\n\nI always wondered how amazing this course could be if it were in Python. I finally decided to re-take the course but only this time I would be completing the programming assignments in Python.\n\nIn these series of blog posts, I plan to write about the Python version of the programming exercises used in the course. I¡¯m doing this for a few reasons:\n\nIt will help anyone who wanted a Python version of the course (that includes me as well)\nIt will hopefully benefit R users who are willing to learn about the Pythonic implementation of the algorithms they are already familiar with\nPre-requisites\nIt¡¯s highly recommended that first you watch the week 1 video lectures.\nShould have basic familiarity with the Python ecosystem.\nIn this section, we will look at the simplest Machine Learning algorithms.\n\n #  Linear Regression with One Variable\nFirst some context on the problem statement.\nHere we will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.\nThe file ex1data1.txt (available under week 2's assignment material) contains the dataset for our linear regression exercise. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss.\n\nFirst, as with doing any machine learning task, we need to import certain libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/ex1data1.txt', header = None) #read from dataset\nX = data.iloc[:,0] # read first column\ny = data.iloc[:,1] # read second column\nm = len(y) # number of training example\ndata.head() # view first few rows of the data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f5c6723b45a3a6755eedbf740c82a439df1b9c7"},"cell_type":"code","source":"#Plot Data\nplt.scatter(X, y)\nplt.xlabel('Population of City in 10,000s')\nplt.ylabel('Profit in $10,000s')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e52cf9a5f145efd2e1ef659feff0df31c24724fe"},"cell_type":"markdown","source":"# Adding the intercept term\n\nIn the following lines, we add another dimension to our data to accommodate the intercept term (the reason for doing this is explained in the videos). We also initialize the initial parameters theta to 0 and the learning rate alpha to 0.01."},{"metadata":{"trusted":true,"_uuid":"056f87400b640d1e93f2e3c303d310186b1b1442"},"cell_type":"code","source":"X = X[:,np.newaxis]\ny = y[:,np.newaxis]\ntheta = np.zeros([2,1])\niterations = 1500\nalpha = 0.01\nones = np.ones((m,1))\nX = np.hstack((ones, X)) # adding the intercept term","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4f3f92906db8ca9f2fe46ec913ee1dd1d56a29d"},"cell_type":"markdown","source":"np.newaxis is used if you want to convert 1-D array (shape: N elements) into row vector (shape: N rows, 1 column) or column vector (shape: 1 row, N columns). Here we are rearranging X and y into column vectors.\n\nNext we will be computing the cost and the gradient descent. The way to do this is very well explained by Andrew Ng in the video lectures. I am only providing the Python codes for the pseudo code which Andrew Ng uses in the lectures.\n\n# Computing the cost"},{"metadata":{"trusted":true,"_uuid":"0e67634f0d983d624d9056f5b2856f7747d1f78b"},"cell_type":"code","source":"def computeCost(X, y, theta):\n    temp = np.dot(X, theta) - y\n    return np.sum(np.power(temp, 2)) / (2*m)\nJ = computeCost(X, y, theta)\nprint(J)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb1fe308e1df1f0eec2ea777d1c959f779997b54"},"cell_type":"markdown","source":"You should expect to see a cost of 32.07.\n\n# Finding the optimal parameters using Gradient Descent"},{"metadata":{"trusted":true,"_uuid":"5aa155f34e8e19c6032228e9d483aa3a807fce58"},"cell_type":"code","source":"def gradientDescent(X, y, theta, alpha, iterations):\n    for _ in range(iterations):\n        temp = np.dot(X, theta) - y\n        temp = np.dot(X.T, temp)\n        theta = theta - (alpha/m) * temp\n    return theta\ntheta = gradientDescent(X, y, theta, alpha, iterations)\nprint(theta)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9882eec66c503730b1a11af2f4c6d9dc2cd0a894"},"cell_type":"markdown","source":"Expected theta values [-3.6303, 1.1664]\n\n\nWe now have the optimized value of theta . Use this value in the above cost function."},{"metadata":{"trusted":true,"_uuid":"70c85ed7a0ffe637e99d3f74970bf8656120e40b"},"cell_type":"code","source":"J = computeCost(X, y, theta)\nprint(J)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4106e1122f02707d38f4d5b7a8959c542f4dc34e"},"cell_type":"markdown","source":"It should give you a value of 4.483 which is much better than 32.07\n\n# Plot showing the best fit line"},{"metadata":{"trusted":true,"_uuid":"88a8dbc7d76dd60ddf1ef837c31a8ee88df79777"},"cell_type":"code","source":"plt.scatter(X[:,1], y)\nplt.xlabel('Population of City in 10,000s')\nplt.ylabel('Profit in $10,000s')\nplt.plot(X[:,1], np.dot(X, theta))\nplt.savefig('graph.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c28ebb06ae441413a1348aea2d95618b11ea07ff"},"cell_type":"markdown","source":"Lets extend the idea of linear regression to work with multiple independent variables.\n\n# Linear Regression with multiple variables\n\nIn this section, we will implement linear regression with multiple variables (also called Multivariate Linear Regression)."},{"metadata":{"_uuid":"545a8c5c22cdaba43bf92758844d9c9f630ab334"},"cell_type":"markdown","source":"*Problem context:\nSuppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices. Your job is to predict housing prices based on other variables.*"},{"metadata":{"_uuid":"ef53674161c497cb5a74001dc295964933acd732"},"cell_type":"markdown","source":"The file ex1data2.txt((available under week 2¡¯s assignment material)) contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house.\n\nYou already have the necessary infrastructure which we built in our previous section that can be easily applied to this section as well. Here we will just use the equations which we made in the above section."},{"metadata":{"trusted":true,"_uuid":"c764e509e5302e0d9f65355bfb633f81a813c629"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ndata = pd.read_csv('../input/ex1data2.txt', sep = ',', header = None)\nX = data.iloc[:,0:2] # read first two columns into X\ny = data.iloc[:,2] # read the third column into y\nm = len(y) # no. of training samples\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54a24a1741bcf81793a2c406215d414e89a379c2"},"cell_type":"markdown","source":"As can be seen above we are dealing with more than one independent variables here (but the concepts you have learnt in the previous section applies here as well).\n\n# Feature Normalization\n\nBy looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly.\n\nOur task here is to:\n\n* Subtract the mean value of each feature from the dataset.\n* After subtracting the mean, additionally scale (divide) the feature values by their respective ¡°standard deviations.¡±"},{"metadata":{"trusted":true,"_uuid":"b675acf7a27a2cb3498ccab3a38c2f080f04bb2f"},"cell_type":"code","source":"X = (X - np.mean(X))/np.std(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3b15aa9f8f03dd2e41e788f43b754efae8a894d"},"cell_type":"markdown","source":"# Adding the intercept term and initializing parameters\n\n(the below code is similar to what we did in the previous section)"},{"metadata":{"trusted":true,"_uuid":"4dba89dbd8ca5130b84d149eb7b00936f3ee067c"},"cell_type":"code","source":"ones = np.ones((m,1))\nX = np.hstack((ones, X))\nalpha = 0.01\nnum_iters = 400\ntheta = np.zeros((3,1))\ny = y[:,np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af679ff88048e665e2db38ca505696809ddcb679"},"cell_type":"markdown","source":"# Computing the cost"},{"metadata":{"trusted":true,"_uuid":"b9b82275be4b23974ac2ac44a51e7116aa2d9585"},"cell_type":"code","source":"def computeCostMulti(X, y, theta):\n    temp = np.dot(X, theta) - y\n    return np.sum(np.power(temp, 2)) / (2*m)\nJ = computeCostMulti(X, y, theta)\nprint(J)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed44884b195a335aa39246a4856b0a12ed45007b"},"cell_type":"markdown","source":"You should expect to see a cost of 65591548106.45744.\n\n# Finding the optimal parameters using Gradient Descent"},{"metadata":{"trusted":true,"_uuid":"b4ab66776e7952d3d2c32f56982f7fd25c1f6ab1"},"cell_type":"code","source":"def gradientDescentMulti(X, y, theta, alpha, iterations):\n    m = len(y)\n    for _ in range(iterations):\n        temp = np.dot(X, theta) - y\n        temp = np.dot(X.T, temp)\n        theta = theta - (alpha/m) * temp\n    return theta\ntheta = gradientDescentMulti(X, y, theta, alpha, num_iters)\nprint(theta)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a19a2ad69c69585c16e799ddbdc8be428579e894"},"cell_type":"markdown","source":"your optimal parameters will be [[334302.06399328],[ 99411.44947359], [3267.01285407]]\n\nWe now have the optimized value of theta . Use this value in the above cost function."},{"metadata":{"trusted":true,"_uuid":"8ba7540aac6446a036131fdeb957fd9bc0c50d06"},"cell_type":"code","source":"J = computeCostMulti(X, y, theta)\nprint(J)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed5f51e6c0106d02e759b6ab3f2541d66a66bb0b"},"cell_type":"markdown","source":"This should give you a value of 2105448288.6292474 which is much better than 65591548106.45744\n\nYou now have learnt how to perform Linear Regression with one or more independent variables. Well done!\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}