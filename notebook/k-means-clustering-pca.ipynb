{"cells":[{"metadata":{"_uuid":"db7b70fe66ffbab974b8ef6cab59a2bd3c717330"},"cell_type":"markdown","source":"## K-Means Clustering and PCA of Human Activity Recognition\n\n***Ruslan Klymentiev***\n\n**Date created: **July 21st, 2018"},{"metadata":{"_uuid":"5fccfee7b9af1771dd838e1c7ee7a726be3f1acc"},"cell_type":"markdown","source":"### Intro\n\nClustering was always a subject I tried to avoid (for no reason). In this project I will finally use my knowledge of clustering and PCA algorithms to explore the Human Activity Recognition dataset. \n\nI would love to point on resourses I have learned from:\n\n1. DataCamp Tutorial: [Python Machine Learning: Scikit-Learn Tutorial](https://www.datacamp.com/community/tutorials/machine-learning-python);\n\n2. DataCamp course: [Unsupervised Learning in Python](https://www.datacamp.com/courses/unsupervised-learning-in-python/);\n\n3. Cognitive Class course: [Machine Learning with Python](https://courses.cognitiveclass.ai/courses/course-v1:CognitiveClass+ML0101ENv3+2018/info);\n\n4. And of course [Prof. Google](http://google.com)!\n\n### Dataset info\n\nHuman Activity Recognition database built from the recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (*WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING*) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import random \nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import StandardScaler\nfrom IPython.display import display\nfrom sklearn.cluster import KMeans \nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.metrics import homogeneity_score, completeness_score, \\\nv_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n%matplotlib inline\n\nnp.random.seed(123)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"Data = pd.read_csv('../input/train.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d75d9a39ca8762b4094ba03884aa4891a53bfb6","collapsed":true},"cell_type":"code","source":"Data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c778c9d35c5a36800c8fe54b8584febf066d52cc","collapsed":true},"cell_type":"code","source":"print('Shape of the data set: ' + str(Data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ce7fbd3acc1b4a8126030208c93cbb28c8f7dec","collapsed":true},"cell_type":"code","source":"#save labels as string\nLabels = Data['activity']\nData = Data.drop(['rn', 'activity'], axis = 1)\nLabels_keys = Labels.unique().tolist()\nLabels = np.array(Labels)\nprint('Activity labels: ' + str(Labels_keys))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c37995ce0245b2daf2f4172a889dd24cbe3aa8f","collapsed":true},"cell_type":"code","source":"#check for missing values\nTemp = pd.DataFrame(Data.isnull().sum())\nTemp.columns = ['Sum']\nprint('Amount of rows with missing values: ' + str(len(Temp.index[Temp['Sum'] > 0])) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fbedddd526219ed7c20866c1908910c2eec9173","collapsed":true},"cell_type":"code","source":"#normalize the dataset\nscaler = StandardScaler()\nData = scaler.fit_transform(Data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdbd26ac5a9a1010601f69bfc9016a18a7754f33","collapsed":true},"cell_type":"code","source":"#check the optimal k value\nks = range(1, 10)\ninertias = []\n\nfor k in ks:\n    model = KMeans(n_clusters=k)\n    model.fit(Data)\n    inertias.append(model.inertia_)\n\nplt.figure(figsize=(8,5))\nplt.style.use('bmh')\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91cfe23cbfe44daebbd2f92523f511b90c6cb69e"},"cell_type":"markdown","source":"**Looks like the best value (\"elbow\" of the line) for k is 2 (two clusters).**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7188fdd938b8ac2d13a27532549fd39891a4fcfc"},"cell_type":"code","source":"def k_means(n_clust, data_frame, true_labels):\n    \"\"\"\n    Function k_means applies k-means clustering alrorithm on dataset and prints the crosstab of cluster and actual labels \n    and clustering performance parameters.\n    \n    Input:\n    n_clust - number of clusters (k value)\n    data_frame - dataset we want to cluster\n    true_labels - original labels\n    \n    Output:\n    1 - crosstab of cluster and actual labels\n    2 - performance table\n    \"\"\"\n    k_means = KMeans(n_clusters = n_clust, random_state=123, n_init=30)\n    k_means.fit(data_frame)\n    c_labels = k_means.labels_\n    df = pd.DataFrame({'clust_label': c_labels, 'orig_label': true_labels.tolist()})\n    ct = pd.crosstab(df['clust_label'], df['orig_label'])\n    y_clust = k_means.predict(data_frame)\n    display(ct)\n    print('% 9s' % 'inertia  homo    compl   v-meas   ARI     AMI     silhouette')\n    print('%i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n      %(k_means.inertia_,\n      homogeneity_score(true_labels, y_clust),\n      completeness_score(true_labels, y_clust),\n      v_measure_score(true_labels, y_clust),\n      adjusted_rand_score(true_labels, y_clust),\n      adjusted_mutual_info_score(true_labels, y_clust),\n      silhouette_score(data_frame, y_clust, metric='euclidean')))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1e22250d6fde4cad012ac66805ba112157fe593"},"cell_type":"markdown","source":"*More on clustering metrics can be found in [DataCamp Tutorial](https://www.datacamp.com/community/tutorials/machine-learning-python).*"},{"metadata":{"trusted":true,"_uuid":"4859477e2f03dff8379fd94ced4c538024a3a9da","collapsed":true},"cell_type":"code","source":"k_means(n_clust=2, data_frame=Data, true_labels=Labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf131bc7df0790288abdf2fdff3c24e0518825bf"},"cell_type":"markdown","source":"**It looks like algorithm found patterns for Moving and Not-Moving activity with high level of accuracy.**\n\n**Check how it will cluster by 6 clusters (original number of classes).**"},{"metadata":{"trusted":true,"_uuid":"77fc311b114a73666db3d1884fd8855494b9cf98","collapsed":true},"cell_type":"code","source":"k_means(n_clust=6, data_frame=Data, true_labels=Labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9ddf90d06a4eda0f9e97555cd39364d976c9d94"},"cell_type":"markdown","source":"**Doesn't look like good connection between clusters and original labels so I will stick with 2 clusters.**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"14fb8f60665b00277373839b7deca6a2bb783ae9"},"cell_type":"code","source":"#change labels into binary: 0 - not moving, 1 - moving\nLabels_binary = Labels.copy()\nfor i in range(len(Labels_binary)):\n    if (Labels_binary[i] == 'STANDING' or Labels_binary[i] == 'SITTING' or Labels_binary[i] == 'LAYING'):\n        Labels_binary[i] = 0\n    else:\n        Labels_binary[i] = 1\nLabels_binary = np.array(Labels_binary.astype(int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c5902af7a341c558ee3d04bddc8a82db89ba2d4","collapsed":true},"cell_type":"code","source":"k_means(n_clust=2, data_frame=Data, true_labels=Labels_binary)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fab904423a94794b73a9fe02e9279d281eeef1b"},"cell_type":"markdown","source":"### Principal component analysis (PCA)\n\n> Principal Component Analysis is a dimension-reduction tool that can be used to reduce a large set of variables to a small set that still contains most of the information in the large set.\n\n**2-cluster algorithm seems to fbe able to find patterns for moving/not-moving labels perfectly so far, but let's see if it can still be improved by dimension reduction. **"},{"metadata":{"trusted":true,"_uuid":"163528d7a251f7a95e5885aa6dd6e909755bcdd1","collapsed":true},"cell_type":"code","source":"#check for optimal number of features\npca = PCA(random_state=123)\npca.fit(Data)\nfeatures = range(pca.n_components_)\n\nplt.figure(figsize=(8,4))\nplt.bar(features[:15], pca.explained_variance_[:15], color='lightskyblue')\nplt.xlabel('PCA feature')\nplt.ylabel('Variance')\nplt.xticks(features[:15])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95185b3da6fe8962a6fb7e46517cc68024ec1593"},"cell_type":"markdown","source":"**1 feature seems to be best fit for our algorithm.**"},{"metadata":{"trusted":true,"_uuid":"bd1680834e4fd355bcfa9a60e389e72f05e56a26","collapsed":true},"cell_type":"code","source":"def pca_transform(n_comp):\n    pca = PCA(n_components=n_comp, random_state=123)\n    global Data_reduced\n    Data_reduced = pca.fit_transform(Data)\n    print('Shape of the new Data df: ' + str(Data_reduced.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"43f3100460829ed0f65a616b5dde28f2f6afd129"},"cell_type":"code","source":"# pca_transform(n_comp=3)\n# k_means(n_clust=2, data_frame=Data_reduced, true_labels=Labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32a1efa2e6a608615b84aa162e15d41de237c39e","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# colors = ['green', 'blue', 'orange', 'gray', 'pink', 'red']\n# fig = plt.figure(figsize=(12,8))\n# ax = fig.add_subplot(111, projection='3d')\n# for i in range(len(colors)):\n#     x = Data_reduced[:, 0][Labels == Labels_keys[i]]\n#     y = Data_reduced[:, 1][Labels == Labels_keys[i]]\n#     z = Data_reduced[:, 2][Labels == Labels_keys[i]]\n#     ax.scatter(xs=x, ys=y, zs=z, zdir='y', s=20, c=colors[i], alpha=0.2)\n\n# ax.set_xlabel('First Principal Component')\n# ax.set_ylabel('Second Principal Component')\n# ax.set_zlabel('Third Principal Component')\n# ax.set_title(\"PCA Scatter Plot\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1930c533868db12321e36543692211c739595933","collapsed":true},"cell_type":"code","source":"pca_transform(n_comp=1)\nk_means(n_clust=2, data_frame=Data_reduced, true_labels=Labels_binary)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d10035131986233b1be712972047543386f40b05"},"cell_type":"markdown","source":"**Inertia and Silhouette seems to be much better now after reduction. **\n\n**Just check clustering model for 2 components.**\n"},{"metadata":{"trusted":true,"_uuid":"117254747dc177e5f18a3077eab9f2b509e5a0af","collapsed":true},"cell_type":"code","source":"pca_transform(n_comp=2)\nk_means(n_clust=2, data_frame=Data_reduced, true_labels=Labels_binary)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab1886cbcd7f99f2b50d99171d6d9e752355f393"},"cell_type":"markdown","source":"**No improvements here.**\n\n**So far it seems like this was best I could do. Still learning clustering algorithms and I might come back to this project later.**\n\n**If you know any interesting dataset to practice clustering on (not Iris dataset, haha), please suggest!**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}