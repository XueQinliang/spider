{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"97d9399e-a2f6-b3df-f7c1-43ba57b9aee2"},"source":"# Try Different ML Methods in Python"},{"cell_type":"markdown","metadata":{"_cell_guid":"c5e1ba9c-6994-98df-3b4d-9aab9c7b95f9"},"source":"## Load Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f30cc32-b5a2-0599-e748-ada38bd27be1"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\ntrain = pd.read_csv('../input/train.csv')\ntest  = pd.read_csv('../input/test.csv')\ntrain.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fdd258ac-e7b9-9526-5483-f6f02590f40c"},"outputs":[],"source":"train.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"a3a465df-7814-3afe-2c58-77005a0ff0ab"},"source":"High dimension data always results in a good training score, yet the test score is not necessarily good as the overfiting  problem may occur. Therefore, before learning, we use the feature selection techniques built in sklearn package to select the useful features. The tutorial of feature selection can be found at the link below.\n\nhttp://scikit-learn.org/stable/modules/feature_selection.html"},{"cell_type":"markdown","metadata":{"_cell_guid":"eeedcb94-ff1a-c5a2-fbe8-d029ea1e9b1d"},"source":"## Feature Selection"},{"cell_type":"markdown","metadata":{"_cell_guid":"65bf65d1-f4d0-eb09-728d-22be273bcebd"},"source":"### Tree_based feature selection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"254124ca-6587-5be1-135a-b6d7582e661f"},"outputs":[],"source":"import sklearn\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfeatures = train.iloc[:,0:562]\nlabel = train['Activity']\nclf = ExtraTreesClassifier()\nclf = clf.fit(features, label)\nmodel = SelectFromModel(clf, prefit=True)\nNew_features = model.transform(features)\nprint(New_features.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"67c78259-68fc-1e4d-8d68-e0f88c9f355c"},"source":"### L1-based feature selection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"061269ff-b185-805d-b732-707589fdd30d"},"outputs":[],"source":"from sklearn.svm import LinearSVC\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(features, label)\nmodel_2 = SelectFromModel(lsvc, prefit=True)\nNew_features_2 = model_2.transform(features)\nprint(New_features_2.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0db77ede-5d41-3c3c-0b22-d7ccae2a0a30"},"source":"The L1-based feature selection will keep more features within the training set.  "},{"cell_type":"markdown","metadata":{"_cell_guid":"4ea6dfe0-faf1-8455-ef72-75c09ef97828"},"source":"## Fitting Classifiers"},{"cell_type":"markdown","metadata":{"_cell_guid":"4f4872b2-fede-69f0-8994-c5f8da64a2c6"},"source":"**Load Models**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c20dfa7c-b784-57a0-3d73-ba23bcda16f4"},"outputs":[],"source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nClassifiers = [DecisionTreeClassifier(),RandomForestClassifier(n_estimators=200),GradientBoostingClassifier(n_estimators=200)]"},{"cell_type":"markdown","metadata":{"_cell_guid":"d25e98eb-1360-9275-c89e-325636ef9fed"},"source":"### Without feature selection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb40d272-bac4-3a03-354c-1ccd1454f86d"},"outputs":[],"source":"from sklearn.metrics import accuracy_score\nimport timeit\ntest_features= test.iloc[:,0:562]\nTime_1=[]\nModel_1=[]\nOut_Accuracy_1=[]\nfor clf in Classifiers:\n    start_time = timeit.default_timer()\n    fit=clf.fit(features,label)\n    pred=fit.predict(test_features)\n    elapsed = timeit.default_timer() - start_time\n    Time_1.append(elapsed)\n    Model_1.append(clf.__class__.__name__)\n    Out_Accuracy_1.append(accuracy_score(test['Activity'],pred))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7134dc57-aecc-8039-78ab-418ebc73ce13"},"source":"### Tree-based feature selection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"34fb1f03-7ee5-dd19-2714-ff2915b6be9e"},"outputs":[],"source":"test_features= model.transform(test.iloc[:,0:562])\nTime_2=[]\nModel_2=[]\nOut_Accuracy_2=[]\nfor clf in Classifiers:\n    start_time = timeit.default_timer()\n    fit=clf.fit(New_features,label)\n    pred=fit.predict(test_features)\n    elapsed = timeit.default_timer() - start_time\n    Time_2.append(elapsed)\n    Model_2.append(clf.__class__.__name__)\n    Out_Accuracy_2.append(accuracy_score(test['Activity'],pred))"},{"cell_type":"markdown","metadata":{"_cell_guid":"2cefb8fa-6ed0-985e-17af-2f3adde95329"},"source":"### L1-Based feature selection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"038da192-7b99-5828-39ed-4ae0f8923167"},"outputs":[],"source":"test_features= model_2.transform(test.iloc[:,0:562])\nTime_3=[]\nModel_3=[]\nOut_Accuracy_3=[]\nfor clf in Classifiers:\n    start_time = timeit.default_timer()\n    fit=clf.fit(New_features_2,label)\n    pred=fit.predict(test_features)\n    elapsed = timeit.default_timer() - start_time\n    Time_3.append(elapsed)\n    Model_3.append(clf.__class__.__name__)\n    Out_Accuracy_3.append(accuracy_score(test['Activity'],pred))"},{"cell_type":"markdown","metadata":{"_cell_guid":"62d31faf-2990-f057-f70e-0b18f9bd3670"},"source":"## Evaluation"},{"cell_type":"markdown","metadata":{"_cell_guid":"dd20beff-0328-e882-0ee1-e16c559aaeea"},"source":"In the final chapter, we will evaluate the feature selections based on **running time and accuracy.** The running time is somehow determinant to the scala-bility of the model while the accuracy is gonna to tell us whether shrinking the dimension of the data may hugely jeopardize the performance of the model."},{"cell_type":"markdown","metadata":{"_cell_guid":"681b10af-a2c3-06e4-06a6-ccd4db76def8"},"source":"### Accuracy"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c1aab49-d865-2b89-4fa5-aa1f2bae13a7"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nind =  np.arange(3)   # the x locations for the groups\nwidth = 0.1       # the width of the bars\nfig, ax = plt.subplots()\nrects1 = ax.bar(ind, Out_Accuracy_1, width, color='r')\nrects2 = ax.bar(ind + width, Out_Accuracy_2, width, color='y')\nrects3 = ax.bar(ind + width + width ,Out_Accuracy_3, width, color='b')\nax.set_ylabel('Accuracy')\nax.set_title('Accuracy by Models and Selection Process')\nax.set_xticks(ind + width)\nax.set_xticklabels(Model_3,rotation=45)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"7a61c306-9394-8f84-38e1-d74eacfb4475"},"source":"**The legend may cover the main part of bar so I remove them in this plot. For the meanings of colors, you can refer to the next plot.**"},{"cell_type":"markdown","metadata":{"_cell_guid":"23391ae3-23e5-2431-1f3b-b09734f68607"},"source":"### Running Time"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c28c389-362c-a475-e2fc-0131cef58030"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nind =  np.arange(3)   # the x locations for the groups\nwidth = 0.1       # the width of the bars\nfig, ax = plt.subplots()\nrects1 = ax.bar(ind, Time_1, width, color='r')\nrects2 = ax.bar(ind + width, Time_2, width, color='y')\nrects3 = ax.bar(ind + width + width ,Time_3, width, color='b')\nax.set_ylabel('Running Time')\nax.set_title('Time by Models and Selection Process')\nax.set_xticks(ind + width)\nax.set_xticklabels(Model_3,rotation=45)\nax.legend((rects1[0], rects2[0],rects3[0]), ('No Selection', 'Tree_Based','L1_Based'))\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"3f6d4b3f-302a-43c2-2aec-c190edea8295"},"source":"## Conclusion"},{"cell_type":"markdown","metadata":{"_cell_guid":"c80c5bb1-1542-9bc9-c018-2ed48c04c46d"},"source":"1. The feature selection can hugely decrease the running time of complicated model, without obviously jeopardizing the performance of model.\n\n2. The overall accuracy of the model will not be necessarily compromised by shrinking the size of the data set. The main reason is that good feature selection may prevent over-fitting to some extents.  "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}