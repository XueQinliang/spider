{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"044de1ff-eb45-3678-2d7f-0236adfa8eda"},"source":"## Data loading and preparation ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4cf4c2e3-e5c7-114b-de1b-d51d258b756b"},"outputs":[],"source":"import numpy as np\nimport pandas as pd"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac551f92-9e8d-17bb-fd9b-280f2dcda413"},"outputs":[],"source":"train = pd.read_csv(\"../input/train.csv\", index_col=0)\ntest = pd.read_csv(\"../input/test.csv\", index_col=0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f55f0dd7-e418-235d-5223-939b4dec257e"},"source":"Just take a little look at the data. does this look good?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4b889e0-a242-e78b-16ce-a3bf5bc17fe2"},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67c9cbe5-6692-d41b-432f-73033e5518d0"},"outputs":[],"source":"# Really simple data preparation\ny_train = pd.get_dummies(train[[\"type\"]], prefix=\"\")\ntrain.drop(\"type\", inplace=True, axis=1)\n\ntrain_test = pd.concat([train, test], axis=0)\n\n# It looks like the color actually is just noise, and does not give any signal to the monster-class.\n# Comment one of these lines.\n#train_test = pd.get_dummies( train_test, columns=[\"color\"], drop_first=False)\ntrain_test.drop(\"color\", inplace=True, axis=1)\n\nX_train = train_test.iloc[:len(y_train)]\nX_test  = train_test.iloc[len(y_train):]\n\n# Clean up\ndel train_test\ndel train\ndel test"},{"cell_type":"markdown","metadata":{"_cell_guid":"ccf2723c-04f3-058d-5b36-66be7cb9ae38"},"source":"## A really simple neural network ##\nHere is an implementation of a really simple neural network. This is the kind of neural network you would expect in the late 1990's. There is no weight decay regularisation or dropout or anything fancy, so the only way to prevent overfitting is early stopping, and limiting the capacity by setting the number of hidden units. \n\nAlso note that there is only three layers: input, hidden and output. The output has softmax outputs and the hidden layer has sigmoid activation function. Please try other configurations if you like."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5481a261-0e8a-e819-30b5-b1162ebd4c00"},"outputs":[],"source":"## A dead simple neural network class in Python+Numpy. Plain SGD, and no regularization.\ndef sigmoid(X):\n    return 1.0 / ( 1.0 + np.exp(-X) )\n\ndef softmax(X):\n    _sum = np.exp(X).sum()\n    return np.exp(X) / _sum\n\nclass neuralnet(object):\n    def __init__(self, num_input, num_hidden, num_output):\n        self._W1 = (np.random.random_sample((num_input, num_hidden)) - 0.5).astype(np.float32)\n        self._b1 = np.zeros((1, num_hidden)).astype(np.float32)\n        self._W2 = (np.random.random_sample((num_hidden, num_output)) - 0.5).astype(np.float32)\n        self._b2 = np.zeros((1, num_output)).astype(np.float32)\n\n    def forward(self,X):\n        net1 = np.matmul( X, self._W1 ) + self._b1\n        y = sigmoid(net1)\n        net2 = np.matmul( y, self._W2 ) + self._b2\n        z = softmax(net2)\n        return z,y\n\n    def backpropagation(self, X, target, eta):\n        z, y = self.forward(X)\n        d2 = (z - target)\n        d1 = y*(1.0-y) * np.matmul(d2, self._W2.T)\n        # The updates are done within this method. This more or less implies\n        # utpdates with Stochastic Gradient Decent. Let's fix that later.\n        # TODO: Support for full batch and mini-batches etc.\n        self._W2 -= eta * np.matmul(y.T,d2)\n        self._W1 -= eta * np.matmul(X.reshape((-1,1)),d1)\n        self._b2 -= eta * d2\n        self._b1 -= eta * d1"},{"cell_type":"markdown","metadata":{"_cell_guid":"484140bb-17d9-e406-82c4-a4b9847cd3fa"},"source":"*Who you gonna call?*\n... The neural network!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c199cb3b-82ee-6d1e-c7f4-5569caefa3cf"},"outputs":[],"source":"# Some hyper-parameters to tune.\nnum_hidden = 8\nn_epochs   = 1500\neta        = 0.01\n# Create the net.\nnn = neuralnet( X_train.shape[1], num_hidden, y_train.shape[1])"},{"cell_type":"markdown","metadata":{"_cell_guid":"188084f7-4692-0fab-3665-b7242fc75d61"},"source":"We train in a simple loop, pure Stochastic Gradient Decent."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db546b9a-0333-186c-56ec-3556e20f7b5c"},"outputs":[],"source":"# (EDIT: It's much faster to convert the dataframes to numpy arrays and then iterate)\nX = np.array(X_train, dtype=np.float32)\nY = np.array(y_train, dtype=np.float32)\nfor epoch in range(n_epochs):\n    for monster, target in zip(X,Y):\n        nn.backpropagation( monster, target, eta)"},{"cell_type":"markdown","metadata":{"_cell_guid":"63274c72-52c4-9be6-14ab-683959e111c7"},"source":"*We came, we saw, we kicked its ass!*\n\nLet's make a submission:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"afab5c80-ba04-2e2a-b877-b20d1d51745f"},"outputs":[],"source":"with open('submission-{}-hidden.csv'.format(num_hidden), 'w') as f:\n    f.write(\"id,type\\n\")\n    for index, monster in X_test.iterrows():\n        probs = nn.forward( np.array(monster, dtype=np.float32))[0]\n        f.write(\"{},{}\\n\".format(index, y_train.columns.values[np.argmax(probs)][1:]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"18cd4f6d-5c99-ccf6-f911-2ddaf00b9f36"},"source":"TODO: Local CV and parameter tuning."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}