{"cells":[{"metadata":{"_uuid":"bca4f119454517cdb29b807c0c84222a73d039fd"},"cell_type":"markdown","source":"#### Flower Classification\n\nList of classes of Flowers given in the dataset-\n1. Daisy\n2. Rose\n3. Dandelion\n4. Tulip\n5. Sunflower\n\nIdea here is to fine-tune a pretrained model (**Resnet34**) using the FastAI Library to get the best possible (close to SOTA) result."},{"metadata":{"_uuid":"cbde60370f2c5faeee7526f2185baa969df9eaa6"},"cell_type":"markdown","source":"### Necessary Library Imports\n\nA directory containing pretrained **Resnet34** model was also required and was available on Kaggle as a public dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pathlib\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport gc\nprint(os.listdir(\"../input\"))\n\nfrom sklearn.metrics import confusion_matrix\nfrom fastai.imports import *\nfrom fastai.transforms import *\nfrom fastai.conv_learner import *\nfrom fastai.model import *\nfrom fastai.dataset import *\nfrom fastai.sgdr import *\nfrom fastai.plots import *\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d6e727904e0aa80b8abf97dc330b2e83a114a62"},"cell_type":"markdown","source":"The following are the two helper modules required to put Resnet34 weights in the apt directory for PyTorch to use directly. These were taken from Anshul Rai's [kernel here](https://www.kaggle.com/anshulrai/using-fastai-in-kaggle-kernel)."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"810b4f2a44734538bc359b0a874ba1c46242f71f"},"cell_type":"code","source":"cache_dir = os.path.expanduser(os.path.join('~', '.torch'))\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models')\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"630f2b45d2dbe3a947a0b9cd5fbd225b6c7718ff","collapsed":true},"cell_type":"code","source":"!cp ../input/resnet34/resnet34.pth /tmp/.torch/models/resnet34-333f7ec4.pth","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"PATH = '../input/flowers-train-valid-split/flowers_split/flowers_split/'\nsz=224","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69b3f71d0bd89e84d39e84556555cb02294918cc","scrolled":false},"cell_type":"code","source":"os.listdir(f'{PATH}valid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e51d616a4a8e7334b28a4416a2d7eaef75cd5906"},"cell_type":"markdown","source":"### Check status of GPU Availability"},{"metadata":{"trusted":true,"_uuid":"ab745de935895f7fe7bd913bf530d6de69c1c6fb","scrolled":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06d92c11bdcc5d4d4b5dbe55c70f5a9809195efc","collapsed":true},"cell_type":"code","source":"sample = os.listdir(f'{PATH}valid/daisy')[:5]\n#sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb91ee3e023cab80e8e5fd985536fbb9613651a8"},"cell_type":"code","source":"img = plt.imread(f'{PATH}valid/daisy/{sample[0]}')\nplt.imshow(img);\ndel sample","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e27497dce755a1ee6e5054c2afb333de45d3171a"},"cell_type":"markdown","source":"### Image Dimensions\n\nHere, we have got ourselves a standard **3 Channel** image so our pretrained models should work fine with added tricks of Data Augmentation."},{"metadata":{"trusted":true,"_uuid":"e797539081b5acb7f8d97f2b1546078733b83dad","collapsed":true},"cell_type":"code","source":"#img.shape\ndel img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bb7de499465a3e12fad33977f633adff844285c"},"cell_type":"markdown","source":"## Baseline Model (88% Accuracy)\n\n- Resnet34 Architecture\n- Precomputed Activations\n- No Data Augmentation"},{"metadata":{"trusted":true,"_uuid":"c83d556d88e2debd37a327fabc1f3b906904e7da"},"cell_type":"code","source":"arch = resnet34\ndata = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch,224))\ndata.path = pathlib.Path('.')  ## IMPORTANT for PyTORCH to create tmp directory which won't be otherwise allowed on Kaggle Kernel directory structure\nlearn = ConvLearner.pretrained(arch, data, precompute=False) #Precompute=True causes the Commit & run operation to fail\nlearn.fit(0.01, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be05ef68d80ccf3e84947e8cbe2bc11335ac517b"},"cell_type":"code","source":"gc.collect()\ndata.classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"054522e2fdaa849c5b6127e39f323027ed83b81b","collapsed":true},"cell_type":"code","source":"log_preds = learn.predict()\nlog_preds.shape\npreds = np.argmax(log_preds, axis=1)\nprobs = np.exp(log_preds[:,1]) ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"544c26c8811bb987bf2429ef08451d5fd08e1e33"},"cell_type":"code","source":"def rand_by_mask(mask): return np.random.choice(np.where(mask)[0], 4, replace=False)\ndef rand_by_correct(is_correct): return rand_by_mask((preds == data.val_y)==is_correct)\n\ndef plot_val_with_title(idxs, title):\n    imgs = np.stack([data.val_ds[x][0] for x in idxs])\n    title_probs = [probs[x] for x in idxs]\n    print(title)\n    return plots(data.val_ds.denorm(imgs), rows=1, titles=title_probs)\n\ndef plots(ims, figsize=(12,6), rows=1, titles=None):\n    f = plt.figure(figsize=figsize)\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, len(ims)//rows, i+1)\n        sp.axis('Off')\n        if titles is not None: sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i])\n\ndef load_img_id(ds, idx): return np.array(PIL.Image.open(PATH+ds.fnames[idx]))\n\ndef plot_val_with_title(idxs, title):\n    imgs = [load_img_id(data.val_ds,x) for x in idxs]\n    title_probs = [probs[x] for x in idxs]\n    print(title)\n    return plots(imgs, rows=1, titles=title_probs, figsize=(16,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3b36cd9adc3030309eec871b4b96086e5be339c"},"cell_type":"code","source":"plot_val_with_title(rand_by_correct(True), \"Correctly classified\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"c1c82ac9c211a2ef87207adb9c877a67365c6344"},"cell_type":"code","source":"def most_by_mask(mask, mult):\n    idxs = np.where(mask)[0]\n    return idxs[np.argsort(mult * probs[idxs])[:4]]\n\ndef most_by_correct(y, is_correct): \n    mult = -1 if (y==1)==is_correct else 1\n    return most_by_mask(((preds == data.val_y)==is_correct) & (data.val_y == y), mult)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20a357e9eb3ad29a50a23b04b290df615fd342ee"},"cell_type":"code","source":"plot_val_with_title(most_by_correct(2, True), \"Most correct Roses\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"119251f038f0d1cb50905740468138914850aeb4"},"cell_type":"markdown","source":"## Finding a Learning Rate\n\nUsing the `lr_find` (learning rate finder) from the FastAI library to get an optimum learning rate."},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true,"_uuid":"c132ac1b440ac9f918ebbf58c15cb114314d0fbf"},"cell_type":"code","source":"lrf=learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f4c11934002151990ca81218867d89c338f54c6"},"cell_type":"markdown","source":"### Learning Rate ~ 0.01"},{"metadata":{"trusted":true,"_uuid":"2357df4c678ee626a6de08d4f2ed77c261302720"},"cell_type":"code","source":"learn.sched.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3222baa146b41d40be104ef1d82efc0bc802c46f"},"cell_type":"code","source":"del data\ndel learn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8007ca245f3ca05f326942d744449aac279c331"},"cell_type":"code","source":"tfms = tfms_from_model(resnet34, sz, aug_tfms=transforms_side_on, max_zoom=1.1)\ndata = ImageClassifierData.from_paths(PATH, tfms=tfms)\ndata.path = pathlib.Path(\".\")\nlearn = ConvLearner.pretrained(arch, data, precompute=False)\nlearn.fit(1e-2, 1)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2a4bab192ce5f5cee2b20a951f0de9627f4c3f9"},"cell_type":"markdown","source":"## Best Model (93% Accuracy)\n\n- **No precomputed activations.**\n- **Unfreezing all layers.**\n- **Use of SGDR with varying Learning Rates for each set of layers.**"},{"metadata":{"trusted":true,"_uuid":"7926c13d733abfb007e092da460cd7291ff48d86"},"cell_type":"code","source":"learn.precompute = False\nlearn.unfreeze()\nlr=np.array([1e-4,1e-3,1e-2])\nlearn.fit(lr, 3, cycle_len=1, cycle_mult=2)\n#learn.sched.plot_lr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18d7e7d0b01d2d2fdd0db6845a71b90e57e9f207"},"cell_type":"code","source":"log_preds,y = learn.TTA()\nprobs = np.mean(np.exp(log_preds),0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59b6fd838724543e202ad7fdc5ea493f775005c7"},"cell_type":"markdown","source":"# Create a confusion matrix to visualize class-wise results!"},{"metadata":{"trusted":true,"_uuid":"5cd7512c27f97dc960f88e28eb4c89b587854458"},"cell_type":"code","source":"accuracy_np(probs, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e22f8b0829b110601a7377ae76de4627370b354","collapsed":true},"cell_type":"code","source":"#plt.figure(figsize=(15,15))\npreds = np.argmax(probs, axis=1)\nprobs = probs[:,1]\ncm = confusion_matrix(y, preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2a01c4be50d90ac4b36c4209906d7f08347fa1b"},"cell_type":"markdown","source":"## Confusion Matrix (Dev Set)"},{"metadata":{"trusted":true,"_uuid":"e5625305c22c13225895518efe6ba70f224cae37"},"cell_type":"code","source":"plot_confusion_matrix(cm, data.classes, figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"967d38e5064a772d85c14707dedb932a83ade130"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}