{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler # Used for scaling of data\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import metrics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom keras import backend as K\nfrom keras.wrappers.scikit_learn import KerasRegressor","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# Read in train data\ndf_train = pd.read_csv('../input/train.csv', index_col=0)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"cb0db41d4fc18a4992301ae08bf61fad4f489f56"},"cell_type":"code","source":"df_train.head()","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f073ac12db41eb616c7d3116537783d890286afd","collapsed":true},"cell_type":"markdown","source":"# Prepare data\n    Investigate what data that has a linear or some kind of relation to the sale price\n    Drop the unimportant features or less unimportant features\n    Drop features which has many NaN values"},{"metadata":{"trusted":true,"_uuid":"f41e1f2f85b2beda54623e15555c4aebca349522"},"cell_type":"code","source":"#descriptive statistics summary\ndf_train['SalePrice'].describe()","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cfcee21e591d2e426d3ac9b3d78e96564bfbb87"},"cell_type":"code","source":"#histogram\nsns.distplot(df_train['SalePrice']);","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d96bb824f0a8b9994a9c312e0032e7fddb2c964"},"cell_type":"code","source":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"3e04a673e79b42e251535b523cf4fdd375c948e4"},"cell_type":"markdown","source":"    - Skewness means the top of the iceberg is not in the middle but rather towards left or right.\n    - Kurtosis describe if the gaussian distrubution is very small and narrow or very wide"},{"metadata":{"_uuid":"b037115d4df4b5a57ab16204e15d954173ffefa0"},"cell_type":"markdown","source":"Use a heatmap to see which features have strongest correlation with house price"},{"metadata":{"trusted":true,"_uuid":"e1b24a67d4ba435393dfe8a94bac1b46bb5b2653"},"cell_type":"code","source":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"69c5af34c8ab83091cfbc91ab77bf3145901b6a6"},"cell_type":"markdown","source":"Here we can detect multicollinearity for example basement area and the area of the first floor so these hold more or less the same kind of data. The same goes for garage variables, for example if you have a big garage you also have more cars in it.\n\nSome variables are also important for the SalePrice with the biggest one being OverallQual\n\nLet's plot top 10 most important for correlating with SalePrice"},{"metadata":{"trusted":true,"_uuid":"ef685a7c0285e63735037ff7aff16b58b7aff810"},"cell_type":"code","source":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"eb0f81a1f65090603bc258fffed8177ab193a5fe"},"cell_type":"markdown","source":"From this plot we can draw the conclusion that:\n    - OverallQual is important\n    - GrLivArea is also important\n    - TotalBsmtSF is important\n    - GarageCars and GarageArea are two important features but we drop GarageArea since it is more or less the same information as GarageCars\n    - TotalBsmtSF and 1stFlrSF are also more or less the same so we drop 1StFlrSF\n    - TotRmsAbvGrd and GrLivArea are also strongly correlated to let's drop TotRmsAbvGrd\n \n Let's scatterplot these important features."},{"metadata":{"trusted":true,"_uuid":"20308da96c6fe321ad8eaa36936b1f4f310e246b"},"cell_type":"code","source":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"ac7fec49425bb709149872260a913ca04eda50e8"},"cell_type":"markdown","source":"The basement area and total living area seems to have similarities their saleprice plot looks almost identical, let's drop basement area.\n\nMaybe also remove year built data since this data can be tricky to use."},{"metadata":{"_uuid":"394c7fab3b8a90176635ef240466257c45b0b549"},"cell_type":"markdown","source":"Let's have a  look at the missing data.\n\nLet's display a % of the data that is missing from some columns."},{"metadata":{"trusted":true,"_uuid":"a5f4b941925a23b99e781dc5ce921aa789cbc9a7"},"cell_type":"code","source":"#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"e81bfe548fff5b1a48f1591d25a84f55e6b8b830"},"cell_type":"markdown","source":"Some of theese features are of interest for us and they don't show a massive shortage of data so lets create mean data for those values."},{"metadata":{"trusted":true,"_uuid":"a16226a47ed6d2fec9a6e69d8f7d982e4589e1e1","collapsed":true},"cell_type":"code","source":"df_train = df_train.fillna(df_train.mean())","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"5d2f7b9502cb2cccc0f34642a042a227f703fddd"},"cell_type":"markdown","source":"Now let's remove outliers for example data that doesn't match what we expect like an insane price for a house\n\nTo do this we standardize the data so that the mean is 0 and a standard deviation of 1. "},{"metadata":{"trusted":true,"_uuid":"5fd1ebd08dc93d5b68d519ac7d2b695b212612f1"},"cell_type":"code","source":"#standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"91fc3e063f9fc40b36aee72864be793171f62f32"},"cell_type":"markdown","source":"    -Values that are similar to each other stay close to 0\n    -Values that are a bit odd get high values such as the 7 values."},{"metadata":{"trusted":true,"_uuid":"a9ce9e94821aaa869a66174feff34e4fb7b0a96f"},"cell_type":"code","source":"#bivariate analysis saleprice/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"565e61c2e6aabfacbebfea5b7de0b68d2d8d151d"},"cell_type":"markdown","source":"What has been revealed:\n\n* The two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.\n* The two observations in the top of the plot are those 7 something observations that we said we should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them."},{"metadata":{"_uuid":"8028a479de52da9a0240437ad642f456234688d2"},"cell_type":"markdown","source":"# Prepare data\nRight now I think we have an idea of what kind of data we are interested in and what data we don't think are useful for us. Let's build a pipeline for removing the data."},{"metadata":{"_uuid":"a59af02c285e253ac8fb2b54e43f0598480babf2"},"cell_type":"markdown","source":"Let's reload the data so we can have a fresh start!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"58901f4a6c2a09dec4ad37fd4324e53f9af86572"},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"1d03fb5b6cdd841a9fe0983595b2f379e24b2755"},"cell_type":"markdown","source":"Let's not log the data since a neural network is quite good at working with non-linear data. I also tested and verified that the model didn't perform better or worse if I logged the data before hand."},{"metadata":{"trusted":true,"_uuid":"6ce8bbf540debd24e8fd5a63c53c28a8a7e1b5d2","collapsed":true},"cell_type":"code","source":"cols = ['SalePrice','OverallQual', 'GrLivArea', 'GarageCars', 'FullBath', 'YearBuilt']\ndf_train = df_train[cols]\n# Create dummy values\ndf_train = pd.get_dummies(df_train)\n#filling NA's with the mean of the column:\ndf_train = df_train.fillna(df_train.mean())\n# Always standard scale the data before using NN\nscale = StandardScaler()\nX_train = df_train[['OverallQual', 'GrLivArea', 'GarageCars', 'FullBath', 'YearBuilt']]\nX_train = scale.fit_transform(X_train)\n# Y is just the 'SalePrice' column\ny = df_train['SalePrice'].values\nseed = 7\nnp.random.seed(seed)\n# split into 67% for train and 33% for test\nX_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size=0.33, random_state=seed)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30d990717c54ee2a96c9687bd4e6a82a21abd560","collapsed":true},"cell_type":"code","source":"def create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))\n    model.add(Dense(30, activation='relu'))\n    model.add(Dense(40, activation='relu'))\n    model.add(Dense(1))\n    # Compile model\n    model.compile(optimizer ='adam', loss = 'mean_squared_error', \n              metrics =[metrics.mae])\n    return model","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a77c30ae22162e72bba30c1f8616fbb35a5e0dc","scrolled":true},"cell_type":"code","source":"model = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"30008da456af620fb89a59aeaac613f1c7d5e9bf","collapsed":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"944e699c04d97aaa7264f0fab5fa8eb3612dcb89"},"cell_type":"markdown","source":"Let's investigate how well this model did!"},{"metadata":{"trusted":true,"_uuid":"d642df9bf8c35e3b3f3a8be8e73dc5f271aa4429","scrolled":false,"collapsed":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['mean_absolute_error'])\nplt.plot(history.history['val_mean_absolute_error'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3c6af154d715d6ecef8a54a6e9485470e7c2385"},"cell_type":"markdown","source":"This result is not very good and gives us a mean absolute error just above 20000 dollars. I beleive this model performs bad due to the fact that we have a quite small data-set becuase a neural network performs the best when having a big dataset. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"656ac4cee8aba9da1488ad0d24e7dfdba16cf3bd"},"cell_type":"code","source":"df_test = pd.read_csv('../input/test.csv')\ncols = ['OverallQual', 'GrLivArea', 'GarageCars', 'FullBath', 'YearBuilt']\nid_col = df_test['Id'].values.tolist()\ndf_test['GrLivArea'] = np.log1p(df_test['GrLivArea'])\ndf_test = pd.get_dummies(df_test)\ndf_test = df_test.fillna(df_test.mean())\nX_test = df_test[cols].values\n# Always standard scale the data before using NN\nscale = StandardScaler()\nX_test = scale.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b0e3651e7b54c5bb1b0c2fee80b74ce0e3ba4b0c"},"cell_type":"code","source":"prediction = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46afd7b911c25323c947d27e4fa480e2c44f2c13","scrolled":true,"collapsed":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Id'] = id_col\nsubmission['SalePrice'] = prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c59f0688b4adb73a2af19f570fe48d7e622c2f96"},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"492d851b6a2070bbcf088dc68f9560b4d56b85e2"},"cell_type":"markdown","source":"**Sources of information**\n\n[Comprehensive data exploration with Python\n](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)\n\n*Can recommend this notebook it is a fun and informative read*"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"44cba4f83556ab5e88ad1a5e1588b7f924e995be"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}