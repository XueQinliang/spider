{"cells":[{"metadata":{"_uuid":"c458c47dfa32f5d11deeed3666c6f69a153f3a21"},"cell_type":"markdown","source":"# Intro\n\nIn this micro-challenge by Kaggle, I solve the blackjack optimal strategy using Deep Reinforcement Learning. Even though this might be like cracking a nut with a sledgehammer it is neverthless interesting to see if it works. Therefore, I slightly adjust the Blackjack simulator kindly provided by the Kaggle Team to be suitable for DQN learning. Finally, the simulator will test my program by playing 1 Mio. hands of blackjack. You'll see how frequently my program won."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Blackjack Rules [Kaggle]\n\nWe'll use a slightly simplified version of blackjack (aka twenty-one). In this version, there is one player (who you'll control) and a dealer. Play proceeds as follows:\n\n- The player is dealt two face-up cards. The dealer is dealt one face-up card.\n- The player may ask to be dealt another card ('hit') as many times as they wish. If the sum of their cards exceeds 21, they lose the round immediately.\n- The dealer then deals additional cards to himself until either:\n    - The sum of the dealer's cards exceeds 21, in which case the player wins the round, or\n    - The sum of the dealer's cards is greater than or equal to 17. If the player's total is greater than the dealer's, the player wins. Otherwise, the dealer wins (even in case of a tie).\n\nWhen calculating the sum of cards, Jack, Queen, and King count for 10. Aces can count as 1 or 11 (when referring to a player's \"total\" above, we mean the largest total that can be made without exceeding 21. So e.g. A+8 = 19, A+8+8 = 17)\n\n"},{"metadata":{"_uuid":"e1e724d9bea28388c810b3b54634826d9c61743f"},"cell_type":"markdown","source":"# The Blackjack Simulator\n\nI use the simulator environment provided by Kaggle for this challenge:"},{"metadata":{"trusted":true,"_uuid":"504ecbb21fd8da2960c97faba7110830ca7b09a0","collapsed":true},"cell_type":"code","source":"# SETUP. You don't need to worry for now about what this code does or how it works. \n# If you're curious about the code, it's available under an open source license at https://github.com/Kaggle/learntools/\nfrom learntools.core import binder; binder.bind(globals())\nfrom learntools.python.ex3 import q7 as blackjack\nprint('Setup complete.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37baba27c4769a1d2a40492e6466c82fe8b3ddc3"},"cell_type":"markdown","source":"It simulates games between my player agent and their own dealer agent by calling the function `should_hit`"},{"metadata":{"_uuid":"59680b96d77e4b5039fd57a25c83239472587ecd"},"cell_type":"markdown","source":"# The Blackjack Player\nKaggle suggests a simple strategy as an example: Always stay after the first round."},{"metadata":{"trusted":true,"_uuid":"7e8a457161558a269aaaadaa1acb8f711fa5a08f","collapsed":true},"cell_type":"code","source":"def should_hit(player_total, dealer_total , player_aces):\n    \"\"\"Return True if the player should hit (request another card) given the current game\n    state, or False if the player should stay. player_aces is the number of aces the player has.\n    \"\"\"\n    return False\nblackjack.simulate(n_games=1000000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"265433cd001797b246aabac05dce1bb36d06ad4e"},"cell_type":"markdown","source":"Another strategy proposed by [Chris Mattler](https://www.kaggle.com/cmattler) achieves better results than the naive one above."},{"metadata":{"trusted":true,"_uuid":"e8c2c9a32840b7b7f26afd8a09152bca1f32eef5","collapsed":true},"cell_type":"code","source":"def should_hit(player_total, dealer_total , player_aces):\n    \"\"\"Return True if the player should hit (request another card) given the current game\n    state, or False if the player should stay. player_aces is the number of aces the player has.\n    \"\"\"\n    if  player_total <= 11:\n        return True\n    elif player_total == 12 and (dealer_total < 4 or dealer_total > 6):\n        return True\n    elif player_total <= 16 and (dealer_total > 6):\n        return True\n    elif player_total == 17 and (dealer_total == 1):\n        return True\n    return False\nblackjack.simulate(n_games=1000000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b08506ce521363862c8cbba51d74394599c7b23"},"cell_type":"markdown","source":"Finally [Kevin Mader](https://www.kaggle.com/kmader/july-24-micro-challenge?utm_medium=social&utm_source=linkedin.com&utm_campaign=micro%20challenge%20july%2024) achieves 42.2% success rate using a Decision-Tree approach. Let's see what DQN can achieve ..."},{"metadata":{"_uuid":"af2a491582cf71693467fd9fe3d8f4de48f8e7a8"},"cell_type":"markdown","source":"# My Turn\n\nI write my own `should_hit` function using Deep Reinforcement Learning. Let's first define the DQN agent class."},{"metadata":{"trusted":true,"_uuid":"95c08278ac9b6bef8e399a00b0661a1101190d5f","collapsed":true},"cell_type":"code","source":"# Implementation of DQN largely based on the code from https://keon.io/deep-q-learning/.\nimport random\nimport numpy as np\nimport pandas as pd\nfrom collections import deque\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras import backend as K\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size,is_eval=False,target_updateC = 10,\n                 gamma=0.95,epsilon_min=0.01,epsilon_decay=0.995):\n        # Game hyperparams\n        self.state_size = state_size\n        self.action_size = action_size\n        \n        # Reinforcement learning hyperparams\n        self.gamma = gamma    # discount rate\n        self.epsilon = 1.0  # initial exploration rate\n        self.epsilon_min = epsilon_min # base exploration rate to keep forever\n        self.epsilon_decay = epsilon_decay # exploration rate decay after each experienced replay\n        self.memory = deque(maxlen=2000) # Max Number of frames to remember\n        self.is_eval = is_eval\n        self.target_updateC = target_updateC # Threshold for updating target model\n        self.C = 0 # Counting replay calls target model upate\n        \n        # Neural network hyperparams\n        self.learning_rate = 0.001\n        self.model = self._build_model()\n        self.target_model = self._build_model()\n        self.update_target_model()\n    \n    def _huber_loss(self, target, prediction):\n        # Error cliping between -1 and 1\n        error = prediction - target\n        return K.mean(K.sqrt(1+K.square(error))-1, axis=-1)\n                \n    def _build_model(self):\n        # Neural Net for Deep-Q learning Model\n        model = Sequential()\n        model.add(Dense(32, input_dim=self.state_size,activation='relu'))\n        model.add(BatchNormalization())\n        model.add(Dense(32, activation='relu'))\n        model.add(BatchNormalization())\n        model.add(Dense(self.action_size, activation='linear'))\n        model.compile(loss=\"mse\",#self._huber_loss,\n                      optimizer=Adam(lr=self.learning_rate))\n        return model\n    \n    def update_target_model(self):\n        # copy weights from model to target_model\n        self.target_model.set_weights(self.model.get_weights())\n    \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n        \n    def act(self,state):\n        if not self.is_eval and np.random.rand() <= self.epsilon:\n        # The agent acts randomly\n            return random.randrange(self.action_size)\n        \n        # Predict the reward value based on the given state\n        act_values = self.model.predict(state)\n        # Pick the action based on the predicted reward\n        return np.argmax(act_values[0])\n\n    def replay(self, batch_size):\n        # Count number of replay calls/ gradient updates\n        self.C += 1\n        # Every C threshold update target model.\n        if self.C > self.target_updateC:\n            self.C = 0\n            self.update_target_model()\n            \n        # Experienced replay based on past memory and observations\n        # Randomly sample batch from past experiences\n        minibatch = random.sample(self.memory, batch_size)\n        # Create training data\n        states = []\n        targets = []\n        for state, action, reward, next_state, done in minibatch:\n            # Predict Q for unchoosen action.\n            target = self.model.predict(state)\n            # if the game has finished\n            if done:\n                target[0][action] = reward\n            else:\n                # use separate network for generating the discounted future reward\n                t = self.target_model.predict(next_state)[0]\n                target[0][action] = reward + self.gamma * np.amax(t)\n            # Aggregate training data   \n            states.append(state)\n            targets.append(target)\n        \n        # Reshape to numpy array of with dim (batchsize,.)\n        states=np.vstack(states)\n        targets=np.vstack(targets)\n       \n        # Retrain the network with full batch\n        self.model.fit(states, targets, epochs=5, verbose=0)\n\n        # After every experienced replay decrease the exploration rate a little\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def load(self, name):\n        self.model.load_weights(name)\n\n    def save(self, name):\n        self.model.save_weights(name)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59c69212c539fc658522dba40d11dc58715181ba"},"cell_type":"markdown","source":"Next, I edit the [Blackjack simulation environment](https://github.com/Kaggle/learntools/blob/master/learntools/python/blackjack.py) provided by Kaggle to be suitable for DQN learning. More specifically, I re-write the original simulation code a little bit to observe intermediate states and rewards during each BlackJack game."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"65b6dbb466d597c06a318c9362d08b843bb1d887"},"cell_type":"code","source":"# Using the Blackjack simulation environment provided by Kaggle\nfrom learntools.python.blackjack import BlackJack\n\nclass BlackJackEnv:\n    def __init__(self):\n        self.player_cards = []\n        self.dealer_cards = []\n    \n    @property\n    def player_total(self):\n        return BlackJack.card_total(self.player_cards)\n    @property\n    def dealer_total(self):\n        return BlackJack.card_total(self.dealer_cards)\n    \n    def _getState(self):\n        # State is player_total, dealer_card_val, player_aces \n        state = [self.player_total,\n                 self.dealer_total,\n                 self.player_cards.count('A')] \n        return(state)\n    \n    def reset(self):\n        # Begin game by dealing cards\n        p1, p2 = BlackJack.deal(), BlackJack.deal()\n        self.player_cards = [p1, p2]\n        d1 = BlackJack.deal()\n        self.dealer_cards = [d1]\n        return(self._getState())\n    \n    def _play(self):\n        # If player stays in game: reward == 1; else -1\n        c = BlackJack.deal()\n        self.player_cards.append(c)\n        if self.player_total > 21:\n            #Player busts! Dealer wins.\n            return self._getState(),-10,True\n        else:\n            #Player still in the game\n            return self._getState(),1,False\n        \n    def step(self,action):\n        # action: 1 == hit, 0 == stay\n        # Function returns: next_state, reward, done\n        if action==1:\n            # Play next card\n            return(self._play())\n        else:\n            # Dealers turn\n            while True:\n                c = BlackJack.deal()\n                self.dealer_cards.append(c)\n                if self.dealer_total > 21:\n                    #'Dealer busts! Player wins.\n                    return (self._getState(),1,True)\n                # Stand on 17\n                elif self.dealer_total >= 17:\n                    #'Dealer stands.\n                    if self.dealer_total >= self.player_total:\n                        #Dealer wins--> Reward is -10\n                        return (self._getState(),-10,True)\n                    else:\n                        #Player wins.\n                        return (self._getState(),1,True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"414829d8165507c08b72ea3cb2579a32bb2d3b6b"},"cell_type":"markdown","source":"Finally, I define the [DQN algorithm](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) which allows to simulate/play many episodes of BlackJack games during which the DQN agent learns from experience the best BlackJack strategy."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"be96ff06d37f943d7c9b783bd58ec16df31b0d17"},"cell_type":"code","source":"def playBlackJack(episodes,replay_batch_size,train=True,\n                 is_eval=False,name=\"BlackJack-dqn\"):\n    # episodes = a number of games we want the agent to play\n    # replay_batch_size = how many randomly selected experiences are used to train agent\n    \n    # initialize the agent\n    state_size = 3 # player_total, dealer_card_val, player_aces \n    action_size = 2 #hit or stay\n    env = BlackJackEnv()\n    agent = DQNAgent(state_size, action_size,is_eval=is_eval)\n    if is_eval:\n        agent.load(name+\".h5\")\n    done = False\n    # Init empty scores list\n    scores=[]\n    # Iterate the game\n    for e in range(episodes):\n        # reset state in the beginning of each game and return initial state\n        state = env.reset()\n        state = np.reshape(state, [1, state_size])\n        done=False\n        while not done:\n            # Decide action\n            action = agent.act(state)\n            # Advance the game to the next frame based on the action.\n            # Reward is 1 for every card survived\n            next_state, reward, done = env.step(action)\n            next_state = np.reshape(next_state, [1, state_size])\n            # Remember the previous state, action, reward, and done\n            agent.remember(state, action, reward, next_state, done)\n            # make next_state the new current state .\n            state = next_state\n            # done becomes True when the game ends ex) The agent drops the pole\n            # train the agent with the experience every x games\n            if (len(agent.memory) > replay_batch_size) and train and e%10 == 0:\n                agent.replay(replay_batch_size)\n        # Save  scores of all games\n        scores.append(reward)\n        \n        # Every now and then save the model and print current game performance\n        if e%100 == 0:\n            print(\"episode: {}/{}, score: {}, exploration: {:0.2f}\"\n                  .format(e+1, episodes, reward ,agent.epsilon))\n            agent.save(name+\".h5\")\n    #Final model saved & close\n    agent.save(name+\".h5\")\n    return(scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db66da15a82508f92cb6f68e70248e1a5f3253c9"},"cell_type":"markdown","source":"# Let's learn & evaluate my agent"},{"metadata":{"trusted":true,"_uuid":"dc17162b7295150a0a70011b9590ae3a3de8e380","collapsed":true},"cell_type":"code","source":"scores = playBlackJack(5000,64,train=True,\n                 is_eval=False,name=\"BlackJack-dqn\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0b3d6b16ef645633e03a2106441527b28bc8d32"},"cell_type":"markdown","source":"![](http://)Let's see how the DQN agent is doing using the original simulation environment from Kaggle ..."},{"metadata":{"trusted":true,"_uuid":"4f46c2dc095209845982a5827be50f5725329f51","collapsed":true},"cell_type":"code","source":"# Game params\nstate_size=3\naction_size=2\n# Init agent\nagent = DQNAgent(state_size, action_size,is_eval=True)\nagent.load(\"BlackJack-dqn.h5\")\n    \ndef should_hit(player_total, dealer_card_val, player_aces):\n    \"\"\"Return True if the player should hit (request another card) given the current game\n    state, or False if the player should stay. player_aces is the number of aces the player has.\n    \"\"\"\n    # Reshape state\n    state = np.array([player_total,dealer_card_val, player_aces])\n    state = np.reshape(state, [1, state_size])\n    # Decide action\n    action = agent.act(state)\n    return action==1\n\nblackjack.simulate(n_games=1000000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e9fc80b4b7a596680f67eb131f3ca75e3eade69"},"cell_type":"markdown","source":"Nice, the DQN agent achieves the same level of performance as other suggestions after playing 5,000 games of Blackjack."},{"metadata":{"_uuid":"0ef8c5091995e13270b7e35e83c8d8ab87868120"},"cell_type":"markdown","source":"# Discuss Your Results\n\nHow high can you get your win rate? We have a [discussion thread](https://www.kaggle.com/learn-forum/58735#latest-348767) to discuss your results. Or if you think you've done well, reply to our [Challenge tweet](https://twitter.com/kaggle) to let us know."},{"metadata":{"_uuid":"29af50cbbf2e28ddc6c308f4ea62faac4992ebd0"},"cell_type":"markdown","source":"---\nThis exercise is from the **[Python Course](https://www.kaggle.com/Learn/python)** on Kaggle Learn.\n\nCheck out **[Kaggle Learn](https://www.kaggle.com/Learn)**  for more instruction and fun exercises."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}