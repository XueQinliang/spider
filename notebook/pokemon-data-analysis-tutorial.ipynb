{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2ba9f5271cd3595ed8e4d750a00fd5e9c056c645"},"cell_type":"markdown","source":"**Pokemon** is a global icon to children and adults everywhere. It is a TV series that has expanded into video games, card games, movies, merchandise and everything inbetween. The motivation behind this analysis is to further understand the dynamics of the pokemon universe through data. Part of this notebook is being built off the process used in  [Rachel Tatman's kernal]( https://www.kaggle.com/rtatman/which-pokemon-win-the-most/notebook). She has done some fanstatic work and does an excellent job explaing her thought process. I am interested in improving my python skills, so I will replicate her work from R to python and develop further analysis. I will be looking at missing values and getting some simple calcualtions in. I will then visualize some results using Matplotlib and Seaborn. I will conclude the venture with some machine learning and PCA to try and predict the win percentage of the pokemon. \n\nThe structure is as follows: \n## Table of Contents\n> 1. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n    * 1.1. [Missing-Values](#Missing-Values)\n    * 1.2 [Visualizing the Data](#Visualizing-the-Data)\n        * 1.2.1 [Seaborn Pairplot](#Seaborn-Pairplot)\n        * 1.2.2 [Correlation Table](#Correlation-Table)\n    * 1.3 [Exploratory Data Analysis Conclusion](#Exploratory-Data-Analysis-Conclusion)\n> 2. [Machine Learning](#Machine-Learning)\n    * 2.1 [Select Data](#Select-Data)\n    * 2.2 [Preprocessing Data](#Preprocessing-Data)\n    * 2.3 [Feature Scaling](#Feature-Scaling)  \n    * 2.4 [Regression Models](#Regression-Models)  \n        * 2.4.1 [Multiple Linear Regression](#Multiple-Linear-Regression)\n        * 2.4.2 [Polynomial Regression](#Polynomial-Regression)\n        * 2.4.3 [SVM](#SVM)\n        * 2.4.4 [Regression Tree](#Regression-Tree)\n        * 2.4.5 [Rangom Forest](#Random-Forest)\n        * 2.4.6 [XGBoost](#XGBoost)\n    * 2.5 [Principle Component Analysis](#Principle-Component-Analysis)\n    * 2.6 [Validation](#Validation)\n> 3. [Conclusion](#Conclusion)\n\nData analysis is a powerful tool. However, simply creating visualizations and preditions is not enough to drive decisions in the business world. I will do my best to explain each figure and calculation and how it could be useful to a business in the Pokemon Universe. This is what I am thinking. Let's say there is a company called \"**Team Rocket**\" who makes millions of dollars off of pokemon battles (I do not condone animal violence in any way, shape or form and no Pokemon were injured during this analysis). As a data consultant, **Team Rocket** gives you this data set and ask you to come up with some useful insight on how to improve their business. This can be difficult because there in not much direction given for the analysis. \n\nLet's break down the task. \n>  1. Understand the business: Team Rocket funds their illegal activities through winning pokemon battles and tournaments. Thus, we will want our analysis to be directed towards finding the best pokemon. \n>  2. Start shallow and dive deeper: As we explore the data, I think its best practice to start the investigation at a higher lever then work our way down and traverse the data. \n>  3. Drive Business Decisions: Once out analysis is complete, relate it back to the business and make suggestions for areas Team Rocket could improve in.\n"},{"metadata":{"_uuid":"73135d97e417bcb74530fe2ee26ea5f9896104cc"},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nWe begin by importing the proper libraries, files and taking a look at what we're dealing with, "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as plt # data visualization\n#import matplotlib.pyplot as plt\nimport seaborn as sns #data visualization\nimport random \n\nrandom.seed(1)\n# Import the dataset\npokemon = pd.read_csv(\"../input/pokemon.csv\")\n# rename the column with a pound sign/hashtag as \"number\" its name\n# The reason for this is when  we try and access this column later it will comment out the code\npokemon = pokemon.rename(index=str, columns={\"#\": \"Number\"})\ncombat = pd.read_csv(\"../input/combats.csv\")\npokemon.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4777b3d793b564c7254468212a3d82723773be5b","_kg_hide-input":true,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"combat.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"_uuid":"79814ce49657c89db9ea4d0c296e3f3bfb51604c","collapsed":true},"cell_type":"code","source":"print(\"Dimenstions of Pokemon: \" + str(pokemon.shape))\nprint(\"Dimenstions of Combat: \" + str(combat.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a437e1d88f9d104b892d9f016647e0dede9fbb23"},"cell_type":"markdown","source":"## Missing Values"},{"metadata":{"trusted":true,"_uuid":"9962f36083e93323fc8cb2fcae447b003b6d8226","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"pokemon.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43eab81fb73cb039b46623c1a3daddefb8c2e7d1","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"combat.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89e1920b6237f182ce59b82f54c77d2f0297a3fb"},"cell_type":"markdown","source":"Some quick notes on my initial thought and comments about the data. \n>    1. Generation is which season of the show the pokemon come out. Think of it as a similar concept to \"baby boomers\" and \"millennials\"\n>    2. Type 2 there is are NA values. Some Pokemon have secondary classification. For example, one pokemon might be able to breath fire and fly. So its Type 1 would be 'fire' but it would also can fly so its Type 2 would be 'flying'.  Some poemon don't have a secondary type, thus the NA value. There are 386 pokemon without a Type 2. \n>    3. There is one name that is missing. We will want to explore that instance, maybe we can fix it.\n>    4. There are 800 different pokemon in the dataset and 50k battles recorded\n\n**Let's begin by finding the missing pokemon!**"},{"metadata":{"trusted":true,"_uuid":"cd5e121f02c2556bd65e988962f2060724b2cb24","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"print(\"This pokemon is before the missing Pokemon: \" + pokemon['Name'][61])\nprint(pokemon[pokemon['Name'].isnull()])\nprint(\"This pokemon is after the missing Pokemon: \" + pokemon['Name'][63])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93dd315ad1a758b9a7b6fa77b85362e20d3e9460"},"cell_type":"markdown","source":"Assuming that the indexes follow the same numbering system as the National Pokedex Number, we can look up the missing Pokemon that falls between the two by looking him up. I used this [link](https://bulbapedia.bulbagarden.net/wiki/List_of_Pok%C3%A9mon_by_National_Pok%C3%A9dex_number). **Primeape** is the missing Pokemon. Update his name in the database.\n"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"714eb5524ad949bcb1c49c54a812662749cd023d","collapsed":true},"cell_type":"code","source":"pokemon['Name'][62] = \"Primeape\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a1355f586b30c85a24beace915259bb83cedb6c"},"cell_type":"markdown","source":"From the combats dataset we will calculate the win percentage of each pokemon. Hopefully, this can help us lean more about which pokemon win. Then we can dive deeper to try and determine which stats contribute most to winning or which type of pokemon win the most. We can continually increase the granularity of our analysis to derive better understanding. We then look at the shape of the data frames and find a very interesting anomaly."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"# calculate the win % of each pokemon \n# add the calculation to the pokemon dataset \ntotal_Wins = combat.Winner.value_counts()\n# get the number of wins for each pokemon\nnumberOfWins = combat.groupby('Winner').count()\n\n#both methods produce the same results\ncountByFirst = combat.groupby('Second_pokemon').count()\ncountBySecond = combat.groupby('First_pokemon').count()\nprint(\"Looking at the dimensions of our dataframes\")\nprint(\"Count by first winner shape: \" + str(countByFirst.shape))\nprint(\"Count by second winner shape: \" + str(countBySecond.shape))\nprint(\"Total Wins shape : \" + str(total_Wins.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70428d6d6114d14e103ae15d4b3b91ee0e91eaa7"},"cell_type":"markdown","source":"We can see that the number of dimensions is different in the total wins. This can only mean there is one pokemon that was unable to win during it's fights. Lets find the pokemon that did not win a single fight."},{"metadata":{"trusted":true,"_uuid":"8fbf782a3ca47a09bdc9e6837811fb3307aad435","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"find_losing_pokemon= np.setdiff1d(countByFirst.index.values, numberOfWins.index.values)-1 #offset because the index and number are off by one\nlosing_pokemon = pokemon.iloc[find_losing_pokemon[0],]\nprint(losing_pokemon)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38f9821207048ae383e9b9af58488a5305c34d7c"},"cell_type":"markdown","source":"Poor **Shuckle**  :( It appears that he has very strong defense but is very weak in all other catergories. I suppose the saying \"the best offense is a good defense\" doesnt apply in the Pokemon universe. \n\nAdd Shuckle to the data so that the array lengths are the same. This will make merging the two datasets easier. In addition, Schuckle was a pokemon that lost all of his battles. Could there be pokemon that didnt battle at all? What I do now is caculate the win percentage of each pokemon and add that feature to the data frame. I think this might be useful down the road. Lets say a new pokemon is discovered, based on its stats can we predict how much it will win? But before we do that, let's do some more exploratory data analysis! \n\nThe process of creating the \"Win Percentage\" column is known as **feature engineering**. It is the process of using given data and combining it in come combination to create new variable (or features) to be used in the analysis and is an important part of machine learning. Below, we develope the new feature, add it to our data set then we merge the two datasets. From there, we notice that there are some pokemon that dont have any recoreded battles. \n"},{"metadata":{"trusted":true,"_uuid":"c78f6a29c1d4ed923ccf9d5dd337891e3608d812","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"numberOfWins = numberOfWins.sort_index()\nnumberOfWins['Total Fights'] = countByFirst.Winner + countBySecond.Winner\nnumberOfWins['Win Percentage']= numberOfWins.First_pokemon/numberOfWins['Total Fights']\n\n# merge the winning dataset and the original pokemon dataset\nresults2 = pd.merge(pokemon, numberOfWins, right_index = True, left_on='Number')\nresults3 = pd.merge(pokemon, numberOfWins, left_on='Number', right_index = True, how='left')\n\n# We can look at the difference between the two datasets to see which pokemon never recorded a fight\n#missing_Pokemon = np.setdiff1d(pokemon.index.values, results3.index.values)\n#subset the dataframe where pokemon win percent is NaN\nresults3[results3['Win Percentage'].isnull()]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d949b5cfaea70daab7069cb7db880cce1e0e972"},"cell_type":"markdown","source":"Looks like our inital hunch was correct! There were some Pokemon that did not battle. Later on we will use some machine learning techniques to predict their liklihood of winning. "},{"metadata":{"_uuid":"f71ced18e49fc3190a9aae78c323e1670b5ec28e"},"cell_type":"markdown","source":"## Find the top 10 Pokemon with the worse win percentage"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"329b79869d9bbed1c7baa81ba29416dd0374013e","collapsed":true},"cell_type":"code","source":"results3[np.isfinite(results3['Win Percentage'])].sort_values(by = ['Win Percentage']).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12501f2e4fd43d2133b592fa5f12f5c6d410b12c"},"cell_type":"markdown","source":"## Find the top 10 Pokemon with the best win percentage"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ff705477864920e62d89f6dd8ec6507532034a74","collapsed":true},"cell_type":"code","source":"results3[np.isfinite(results3['Win Percentage'])].sort_values(by = ['Win Percentage'], ascending = False ).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d9c0eec80c7f7dba2835870b4b244a4c2cef432"},"cell_type":"markdown","source":"There is a clear difference between Poekmon that win and lose. The most obvious reason to me are the vast differences in their stats. From a quick glance it appears that attack and speed are significantly higher in Pokemon that win. I also would like to point out two other oberservations. There are several winning pokemon with the Prefix \"**Mega**\". I am not sure the difference between **Mega Aerodactyl** and regular  **Aerodactyl** but I'm confident that plays a role. Perhaps asking my 7 year old cousin about the difference could help.\n\nWhich actually brings up a good point. As you drive into the data, there might be anolomlies that dont make sense but you have a \"gut\" feeling that there might be a correlation to the outcome. This is why understanding the buisness and where the data is coming from is so very crutial to helping the business out. In this case of \"Mega\" vs \"non-Mega\", maybe finding \"Mega\" pokemon in the wild is much more difficult and cost a lot more resources (ie: Time, pokeball quality, more potions and stronger pokemon to catch it). For these reasons, even though the Mega Aerodactyl wins more often the business cost might outweight the potential business revenue. \n\n## Basic statistics of the data "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"73ab2938f57be18d6b74f4e5a53b10ea29287a2d","collapsed":true},"cell_type":"code","source":"results3.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b365c14a169714b800f8686e0dad32d94712964c"},"cell_type":"markdown","source":"The above table displays the basic statistics of each variable like the mean, min, max, ect. The reason I find this helpful is it gives a better understanding of the range of values we are working with and the average of these values. If we make some predictions and the data is well outside the range we are working with, then we can make a reasonable assumption that our model is not working correctly. In addition, some machine learning models we might want to standardize or normalize the data.\n\n## Visualizing the Data\n\nAs I stated in the begining, I think data analyst often get too excited (not that there is anything wrong with that) about the project and they begin their exploration with too much detail. I belive we shoud, start very basic and take one step at a time. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"440f639d4a055260b70391069be1635b1d5c2805","collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nsns.set_color_codes(\"pastel\")\nax = sns.countplot(x=\"Type 1\", hue=\"Legendary\", data=results3)\n#f.set_xticklabels(rotation=30)\nplt.xticks(rotation= 90)\nplt.xlabel('Type 1')\nplt.ylabel('Total ')\nplt.title(\"Total Pokemon by Type 1\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"8c7ccafb77c1958fadc65ac04f031eb15be942a9","collapsed":true},"cell_type":"code","source":"ax = sns.countplot(x=\"Type 2\", hue=\"Legendary\", data=results3)\nplt.xticks(rotation= 90)\nplt.xlabel('Type 2')\nplt.ylabel('Total ')\nplt.title(\"Total Pokemon by Type 2\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8600e042fcc930342e2e0d6dd0b0f50898d3eaae"},"cell_type":"markdown","source":"We can see that water, normal, bug and grass are the most common type 1 and flying, ground and poison are the most common type 2. Even though it doesnt tell me the exact population (ie. there are 1 million grass type 1 pokemon in the world), what it doesnt tell me is which attributes are most apparent. How this can be usefull is possibly predicting the liklihood of battling a particular type of pokemon and having a counter type to it. For example, a water type pokemon. Since we know water pokemon are weak agaisnt grass or electric, it might be a good idea to keep one of those types in our battle squad. \n\nLets aggregate our win percentage by type 1."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"decc14e92fd61aea3cb9a0b60dca292535768414","collapsed":true},"cell_type":"code","source":"results3.groupby('Type 1').agg({\"Win Percentage\": \"mean\"}).sort_values(by = \"Win Percentage\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"712c2a57f3a6cc3432d16028236d565275c6edc0"},"cell_type":"markdown","source":"Flying, dragon and electric type pokemon have a significant win percentage over the other types. My hypothesis is that there are many pokemons whos attacks are ineffective while the other pokemon is flying, thus giving the other pokemon a significant advantage. \n\n**We can further break down the data by looking at type by generation, legendary by generation, stats by type, stats by generation, stats by lengendary  and so on**. \n\nI broke up the data into a smaller subset for this section. I defined the indepenent variables to be [ 'HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed'] and the dependent variable to be ['Win Percentage']. In this section, I will be exploreing these relationships as well as some other visualizations that will better explain the trends in the Pokeverse. I keep 'Type 1' in the data because later on I want to see how the relationships break down by type. \n"},{"metadata":{"_uuid":"f011c22355ecf6399186c57bbe54511687715972"},"cell_type":"markdown","source":"## Seaborn Pairplot"},{"metadata":{"trusted":true,"_uuid":"a88cbdca84a9377f24c62559406389e02ef5010e","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"#sns.distplot(results3[\"Win Percentage\"].dropna(), bins=20)\ncol = ['Type 1','HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', 'Win Percentage']\n#results3.loc[:,'HP':'Speed'].corr()\nsns.pairplot(results3.loc[:,col].dropna())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"787ffeee0359d79ce6edcd7d67429bd6c7a44c33"},"cell_type":"markdown","source":"### Seaborn PairGrid"},{"metadata":{"trusted":true,"_uuid":"30a4bfe7b50f512059c8e53062265539cb3decdb","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ng = sns.PairGrid(results3.loc[:,col], diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(sns.regplot)\ng.map_diag(sns.kdeplot, lw=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"616e874a2d6e741bc14e85a5c2e6be3d63cffebe"},"cell_type":"markdown","source":"\nWow this figure has a lot going on! But thats okay, we only need to look at it peice by piece do derive some meaningful understanding.\n\nI like the PairGrid better because it provides more customizable options compared to the Pairplot. As you can tell I can have two different kinds of graphs on the upper and lower halves of the grid. In the PairGrid above we see 3 types of graphs:\n\n> 1. Regression scatter plot in the upper right\n> 2. Density Plot down the diagonal\n> 3. Bivariate Denisty plot with contour levels \n\nIn the bottow row, we see 6 empty graphs. The bivariate desity plots cannot plot due to the different scale of values. Win Percentage values range [0,1] while the independent vairables (HP, Attack,...) range from [0,200+].\n\nThe diagonal shows the density plots. For most of the independent variables look realtively normal distribution with a right skew. This means a majority of the pokemon stats are on average higher than the median value for the set. However, the density for the win percentage is different. Comparing the density plot with the frequency plot in the pairplot for 'Win Percentage' we see a more uniformly distribution of the rate at which pokemon win with a slight decrease of the frequency at higher levels. \n\nThe upper right section is the most easy to understand and probably the most useful for our analysis. These are regression plots with a line of best fit. Esentially the slope of the line y=mx+b, where m is the slope is the correlation value between the two variables. Thus we would expect to see a simialr pattern if we were to build a heat map or a correlation table. What I am most interested in is the relationship between each independent variable and the dependent variable (Win Percentage). The greater the slope, m, the more correlated the values are in determining the liklihood of winning. Just by 'eye balling' it (not the most mathmematically correct method), but it appears that **speed** and **attack** have the largest relationship to winning. Lets take a closer look into these two plots given below. \n\nWe begin by replicating the regplot to that we can see it better. To dive deeper, lets take a look at if there any trends when we further break down the data into 'Type 1'.\n\n## Correlation Table"},{"metadata":{"trusted":true,"_uuid":"1ddad6cf90ee8cf7eacd68b5959f70818393f3d0","_kg_hide-input":true,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"results3.loc[:,col].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a81d703d4fa04c4c13c14dac492f8fe4d6f3666","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# https://datascience.stackexchange.com/questions/10459/calculation-and-visualization-of-correlation-matrix-with-pandas\n#This function was taken from the link above \ndef correlation_matrix(df):\n    from matplotlib import pyplot as plt\n    from matplotlib import cm as cm\n\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    cmap = cm.get_cmap('jet', 50)\n    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n    ax1.grid(True)\n    plt.title('Pokemon Feature Correlation')\n    labels=['Type 1','HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', 'Win %']\n    ax1.set_xticklabels(labels,fontsize=7)\n    ax1.set_yticklabels(labels,fontsize=7)\n    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n    fig.colorbar(cax, ticks=[0.00,.05,.10,.15,.20,.25,.30,.35,.40,.45,.50,.55,.60,.65,.70,.75,.8,.85,.90,.95,1])\n    plt.show()\n\ncorrelation_matrix(results3.loc[:,col])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1427b1055bc6f4a4e3fcbc6e5344c7f48bd4ae51"},"cell_type":"markdown","source":"The table and the correlation figure provide the exact same information. The figure with colors is more pleasant to look at and when discussing results with** Team Rocket** executives, I would used the \"Pokemon Feature Correlation\" plot. The diagonal of the plot all the correlations are 1.0, which is perfectly positively correlated. This is becase the diagonal compares each feature to itself. Also, if we were to fold the matrix in half down the diagonal, it would be perfectly semmetical. The top half above the diagonal provides the same information as the lower half. While it might be interesting to look at how the independent features are correlated, I think for our business problem we investigate the features correlated to \"win %\". Looking at the features with the highest correlation to winning which are speed and atatck. "},{"metadata":{"trusted":true,"_uuid":"189a066e693c1a79ef54d24331721f3c34b4f7ce","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n#f, (ax1, ax2) = plt.subplots(1,2)\nsns.regplot(x=\"Speed\", y=\"Win Percentage\", data=results3, logistic=True).set_title(\"Speed vs Win Percentage\")\nsns.lmplot(x=\"Speed\", y=\"Win Percentage\", data=results3, hue = 'Type 1',  logistic=True)#.set_title(\"Speed vs Win Percentage\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"040258c7c14ef6e5fe47a00935b82786dba893ec","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"ax = sns.regplot(x=\"Attack\", y=\"Win Percentage\", data=results3).set_title(\"Attack vs Win Percentage\")\nsns.lmplot(x=\"Attack\", y=\"Win Percentage\", data=results3, hue = 'Type 1',fit_reg =False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71e7ba1e6faca5f0f53881f0d7f5e433c27b55db"},"cell_type":"markdown","source":"Why did we plot the same figure twice? The top plot with one solid color was to look at the relationship between the independent and dependent variable as a whole. Breaking down the data further, it was subset by 'type 1' in hopes that a new pattern would appear. I was thinking maybe we would see dragon type pokemon clustered at the higher end of win percentage. But the message I want to send is to think of new ways to continue to break down the data and dive deeper. \n\n# Exploratory Data Analysis Conclusions\n\nThis is where we communicate all the insight we have developed into a concise manner. Remeber, its not enought to simply state the results. Try and relate it back to the **Team Rocket** business model. \n\n1. **Water, normal, bug and grass are the most common type 1 and flying, ground and poison are the most common type 2. **\n    * Team Rocket should have pokemon in their battle squads to counter these types. [Pokemon Type Weaknesses](https://pokemondb.net/type)\n2. **The Pokemon type that win the most are flying, dragon, electric and dark. The Pokemon type that win the least are fairy, rock, steel, poison.**\n    * While this seems stright forward given that we have looked at the data, it might not always be apparent to those who have not. Communicating to Team Rocket, \"Hey, these are the kinds of Pokemon (flying, dragon, electric and dark) you should be spending your resources on because they win. If you have these pokemon (fairy, rock, steel, poison) you should avoid wasting resources and release any you have into the wild so you can reduce your overhead cost. \n3. **Speed and Attack!!**\n    * Defense doesn't win championships in the Pokeverse. We need powerful attacks and quickness. If we look back at the top 10 most winning pokemon, all have speeds over 100+ and attacks over 100+ (except for Greninja's attack) \n\nAs data consultants, it might not be enough to only work with this dataset. It our job to ask \"what else is missing?\" or \"what further analysis can we do given more data?\" \n\n### What else are we missing and further analysis\n   * I'm not sure if this data is similar to the video game but maybe pokemon level (ie 1-100) plays a role in this. A level 1 Pikachu would lose to a level 50 Blastoise, regardless of the fact Pikachu has the type advantage. \n   * Can we get the data of pokemon battle squads from previous tournaments. We can start to analyze what kinds of pokemon are used at the competative level.\n   * Geographic location of where to find the different Pokemon would be a tremendous help into advising Team Rocket. \n   * We know from the TV show that Pokemon have their own personalities, some pokemon are stubborn, don't work well with others or listen to their masters. Getting textual review data about what people think about each pokemon could be helpful in understanding how Team Rocket could train them. \n\n# Machine Learning\n\nWhat is a Kaggle kernel without throwing in some machine learning :)\n\nMachine learning is broken up into two categories, **supervised learning and unsupervised learing**. For this dataset, we want to build a model that will predict the liklihood of a pokemon winning a battle. To do this we will be testing several suprvised learning algortithms. There are two sub categories of supervised learning and those are **regression and classification**. In this kernal, we will be using regression models such as **Multiple Linear Regression, Polynomial Regression, SVM,  Decision Tree Regression, Random Forest, XGBoost**. We wil then try to reduce the number of features being used and run the model again. Lastly, we will utilize an unsupervised learning method called **Principle Component Analysis** which transforms the data space and reduces the dimensionality of the problem. The three approaches will be comapred. \n\nThe process for machine learning can be broken into several steps. \n>    1. Select data\n>    2. Preprocess data\n>    3. Transform Data\n>    4. Fit data to model/predict\n>    5. Validate model\n>    6. Parameter tuning\n\nI combine steps 4 and 5 together since step 5 is relatively short. I may eventually work on step 6, parameter tuning however this is a much more in depth and complex subject. \n \n ## Select Data"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"5c32355b4b474074ac39e1c9d8f2e524eed284aa"},"cell_type":"code","source":"dataset = results2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"365c86f02e29b60aca1db9bfc0f8e1a462d658a2"},"cell_type":"markdown","source":"## Preprocessing  Data\n\nThe data is broken into a testing and training to prevent against over fitting.  I chose to do 80% to 20%, There is no correct ratio to split, it depends on the size of the data you are working with. In this stage we can also encode any categorical variables we may have. In our case, this would be 'Type 1', 'Type 2', 'Generation' and 'Lengendary'. I do realize the type of Pokemon is very important to the outcome of the battle, for simplicity purposes I plan on leaving that out for now and may revisit this issue in the future. "},{"metadata":{"trusted":true,"_uuid":"ae98e4f23a316db04e4a709adbe5c4bc1f8c489b","_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"#remove rows with NA values because it will cause errors when fitting to the model\ndataset.dropna(axis=0, how='any')\n# Splitting the dataset into the Training set and Test set\nX = dataset.iloc[:, 5:11].values\ny = dataset.iloc[:, 15].values\n\n# Encoding categorical data (if there is some)\n# In this case it could be pokemon type\n#'''from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n#labelencoder = LabelEncoder()\n#X[:, 3] = labelencoder.fit_transform(X[:, 3])\n#onehotencoder = OneHotEncoder(categorical_features = [3])\n#X = onehotencoder.fit_transform(X).toarray()'''\n\n# Avoiding the Dummy Variable Trap\n#X = X[:, 1:]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03a2d82e2e96283f10f552b9fe283a9f479af234"},"cell_type":"markdown","source":"## Feature Scaling\n\nFrom there the data is standardized. It can also be normalized or transformed using PCA (which we will use later). Standardizing is important because it brings all the variables withing the same range. If variable 'A'  ranges [1-1million] and variable 'B' ranges [0-1], variable 'A' will completly dominate 'B'. 'B' would essentially become negligible. Thus, we scale 'A' to capture the variances of each vairable.  For this data I did not use feature scaling. "},{"metadata":{"trusted":true,"_uuid":"9ce14d92c6bb22326e09b2ebe2d91234f4c814bd","_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# Feature Scaling\n#from sklearn.preprocessing import StandardScaler\n#sc = StandardScaler()\n#X_train = sc.fit_transform(X_train)\n#X_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db67e623727a3efecb7039a2d248f2e13fd52d39"},"cell_type":"markdown","source":"# Regression Models\n\nI put each regression model into a function so that I don't have to rewrite the code when I change the dimensionality of the data. You will notice that the code looks pretty much identical for each model. This is because I am a template that works very well for easy implementation! Feel free to use this structure! The only difference is the library we are using and when we initialize the regressor object. \n\n## Multiple Linear Regression"},{"metadata":{"trusted":true,"_uuid":"66694109a4c23aa87572f1321d8093d005fe9c44","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"def ml_linearreg(X_train, X_test, y_train, y_test):\n    # Fitting Multiple Linear Regression to the Training set\n    from sklearn.linear_model import LinearRegression\n    regressor = LinearRegression()\n    regressor.fit(X_train, y_train)\n    print(regressor.score(X_train, y_train))\n    # Predicting the Test set results\n    y_pred = regressor.predict(X_test)\n\n    # Validating the results\n    from sklearn.metrics import mean_absolute_error\n    from math import sqrt\n    mae = mean_absolute_error(y_test, y_pred)\n    #print(\"Mean Absolute Error: \" + str(mae))\n    return mae\n\nml_linearreg(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a51335a449bc0b54f5f99d1c88e521c04068747"},"cell_type":"markdown","source":"## Polynomial Regression"},{"metadata":{"_uuid":"b96e89357667ed08876d172e8ca4f83c9cd4c6f7"},"cell_type":"markdown","source":"## SVM"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"_uuid":"9ed928d76fff5de0f9cba416ee211f1eccad797f","collapsed":true},"cell_type":"code","source":"# Feature Scaling\n#from sklearn.preprocessing import StandardScaler\n#sc_X = StandardScaler()\n#sc_y = StandardScaler()\n#X = sc_X.fit_transform(X)\n#y = sc_y.fit_transform(y)\ndef ml_svm(X_train, X_test, y_train, y_test):\n    # Fitting SVR to the dataset\n    from sklearn.svm import SVR\n    regressor = SVR(kernel = 'linear')\n    regressor.fit(X_train, y_train)\n    print(regressor.score(X_train, y_train))\n\n    #Predict Output\n    y_pred= regressor.predict(X_test)\n    #y_pred = sc_y.inverse_transform(y_pred)\n\n    from sklearn.metrics import mean_absolute_error\n    from math import sqrt\n    mae = mean_absolute_error(y_test, y_pred)\n    #print(\"Mean Absolute Error: \" + str(mae))\n    return mae\n\nml_svm(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88e3ca98cc4433c1da03aea176dab0cb8bf2e4ae"},"cell_type":"markdown","source":"## Decision Tree Regression"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"_uuid":"49da35e8cf1c73823bdcd63eb698830071a594de","collapsed":true},"cell_type":"code","source":"#feature scaling not needed\n\ndef ml_decisiontree(X_train, X_test, y_train, y_test):\n    # Fitting Decision Tree Regression to the dataset\n    from sklearn.tree import DecisionTreeRegressor\n    regressor = DecisionTreeRegressor(random_state = 0)\n    regressor.fit(X_train, y_train)\n    print(regressor.score(X_train, y_train))\n\n    # Predicting a new result\n    y_pred = regressor.predict(X_test)\n\n    from sklearn.metrics import mean_absolute_error\n    from math import sqrt\n    mae = mean_absolute_error(y_test, y_pred)\n    #print(\"Mean Absolute Error: \" + str(mae))\n    return mae\n\nml_decisiontree(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa5f06d6feb434aa953042ba1caf255cba7c4f94"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"258ae70495120f43be17ddb7cb3b487952bf90b3","collapsed":true},"cell_type":"code","source":"# no feature scaling needed\ndef ml_randomforest(X_train, X_test, y_train, y_test):\n    # Fitting Random Forest Regression to the dataset\n    from sklearn.ensemble import RandomForestRegressor\n    regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n    regressor.fit(X_train, y_train)\n    print(regressor.score(X_train, y_train))\n\n    # Predicting a new result\n    y_pred = regressor.predict(X_test)\n\n    from sklearn.metrics import mean_absolute_error\n    from math import sqrt\n    mae = mean_absolute_error(y_test, y_pred)\n    #print(\"Mean Absolute Error: \" + str(mae))\n    return mae\n\nml_randomforest(X_train, X_test, y_train, y_test)\n    \n# Visualising the Random Forest Regression results (higher resolution)\n#X_grid = np.arange(min(X), max(X), 0.01)\n#X_grid = X_grid.reshape((len(X_grid), 1))\n#plt.scatter(X, y, color = 'red')\n#plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\n#plt.title('Truth or Bluff (Random Forest Regression)')\n#plt.xlabel('Position level')\n#plt.ylabel('Salary')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e56b34db0451bb6b48ad38911a683980ece4b351"},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true,"_uuid":"3548b2f7276a7d9a0ea1e2ecd14249c51755116e","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"\ndef ml_xgboost(X_train, X_test, y_train, y_test):\n    import xgboost\n    # fitting XGBoost to training set\n    xgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75, colsample_bytree=1, max_depth=7)\n    xgb.fit(X_train,y_train)\n    print(xgb.score(X_train, y_train))\n    # Prediction\n    y_pred = xgb.predict(X_test)\n    #print(explained_variance_score(y_pred ,y_test))\n    from sklearn.metrics import mean_absolute_error\n    from math import sqrt\n    mae = mean_absolute_error(y_test, y_pred)\n    #print(\"Mean Absolute Error: \" + str(mae))\n    return mae\n\nml_xgboost(X_train, X_test, y_train, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e007f2b3366fc76011805c74c0fb92e23bf71669","collapsed":true},"cell_type":"code","source":"#store all the ML results in an array\nall_stats = [ml_linearreg(X_train, X_test, y_train, y_test), ml_svm(X_train, X_test, y_train, y_test), ml_decisiontree(X_train, X_test, y_train, y_test), ml_randomforest(X_train, X_test, y_train, y_test), ml_xgboost(X_train, X_test, y_train, y_test)]\n#all_stats","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"846966cf0080ec6c3abceccc9d37001788a5e301"},"cell_type":"markdown","source":"# Principle Component Analysis \n\nPrinciple component analysis (PCA) is a dimensionality reduction technique. It uses linear algebra to tranform the data into a new space of principle components. Each principle component explains some variance of the dataset. The goal of this technique is to reduce the amount of features we are using for our model and simplify. The principle components consist of component loadings. The loadings are the correlation coefficients between the variables and factors."},{"metadata":{"trusted":true,"_uuid":"8ce5cbac6f4af54534ef8b062aa67f6eec74f96b","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"#PCA\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Applying PCA\nfrom sklearn.decomposition import PCA\n#pca = PCA(n_components = None)\npca = PCA(n_components = 3)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n# Provides a vector of the variance explained by each component\nexplained_variance = pca.explained_variance_ratio_\nprint(\"This is the variance explained by the principle components\")\nprint(explained_variance)\n\n#loadings vectors\n#print(pca.components_.T * np.sqrt(pca.explained_variance_))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"34baf6044b8b5f7b271dbd1f583a8d258ba1bf6f"},"cell_type":"markdown","source":"One you look at how much variance each independent variable provide, decide how many components you want for the model. The more components the more variance the model will have.  Re-run the code but change n_components from 2 to the desired number. \n\nRun the machine learning algorithms using the data after PCA. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"dba9538d66b4b6651df8b88cc51fc3d98cadf5cb","collapsed":true},"cell_type":"code","source":"# run PCA transformed data on ML algos\nPCA = [ml_linearreg(X_train, X_test, y_train, y_test), ml_svm(X_train, X_test, y_train, y_test), ml_decisiontree(X_train, X_test, y_train, y_test), ml_randomforest(X_train, X_test, y_train, y_test), ml_xgboost(X_train, X_test, y_train, y_test)]\n#PCA\n#ml_linearreg(X_train, X_test, y_train, y_test)\n#ml_svm(X_train, X_test, y_train, y_test)\n#ml_decisiontree(X_train, X_test, y_train, y_test)\n#ml_randomforest(X_train, X_test, y_train, y_test)\n#ml_xgboost(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"57ec9c45fecff12fe602a23698b1e032ee7790e8","collapsed":true},"cell_type":"code","source":"# reduce the features to only speed and attack. \ndataset = results2\ndataset.dropna(axis=0, how='any')\n# Splitting the dataset into the Training set and Test set\nX = dataset.loc[:, ['Attack','Speed']].values\ny = dataset.loc[:, ['Win Percentage']].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n#ml_linearreg(X_train, X_test, y_train, y_test)\n#ml_svm(X_train, X_test, y_train, y_test)\n#ml_decisiontree(X_train, X_test, y_train, y_test)\n#ml_randomforest(X_train, X_test, y_train, y_test)\n#ml_xgboost(X_train, X_test, y_train, y_test)\n\nreduced_stats = [ml_linearreg(X_train, X_test, y_train, y_test), ml_svm(X_train, X_test, y_train, y_test), ml_decisiontree(X_train, X_test, y_train, y_test), ml_randomforest(X_train, X_test, y_train, y_test), ml_xgboost(X_train, X_test, y_train, y_test)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8ea5fd6048aa92cdafb2d34683a2af63a307880"},"cell_type":"markdown","source":"## Validation \n\nSo far we have fited our data to various regression models and predicted the likilhood of a pokemon winning a battle. The question still remains, how do we know how accurate our predictions are? Well, there are many metrics that the Sklearn library provides us. If you have looked through the code you will see that I used mean absolute error. This gives the average amount our predictions were off. Other metrics are as follows. \n\n1. Mean Absolute Error\n2. Mean Squared Error\n3. Root Mean Squared Error\n4. R Squared\n5. Explained Variance Score\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"8e661e24c4503dee41ffc8da0c22304613a6bd8a","collapsed":true},"cell_type":"code","source":"#compare results from the 3 trials \nml_results = pd.DataFrame({'All Factors': all_stats, 'Reduced Factors': reduced_stats, 'PCA': PCA})\nml_results.rename(index = {0:'Linear',1:'SVM', 2:'Decision Tree', 3:'Random Forest', 4:'XGBoost'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b2849a2f432a83edfdc23bf1f2c7c169dd24c65"},"cell_type":"markdown","source":"The above table provides a comparision of the mean absolute error for each machine learning model and the different dimensionality approaches we took.  **The model with the least error was the XGBoost with reduced factors**. This was the case where we only used attack and speed to predict the win percentage. All models performed significantly worse under PCA. I did not expect these results. It could be because of a combiantion of the parameters I am using for each of the models. I definetly didnt put very much effort into trying to optomize the results. To correct the issue, we would need to run a **grid search with k-folds cross validation*. "},{"metadata":{"_uuid":"2fa951686cb7c00d9088f2385eb30903b4fc2d0e"},"cell_type":"markdown","source":"# Conclusion\n\nThis kernel and analysis are incomplete. My plans are as follows\n\nvisualization:\n        add several more seaborn bar plots, pie charts ect \n        \nMachine Learning: I want to add these machine learning approaches as a simple implementation and practice. \n        SVM, Regression Tree, Random Forrest, XGboot\n        improve the PCA section by looking at the loadings of the principle components \n        \n       validation: add the other validation metrics\n   \n  Final thoughts, relate back to our business example\n  \n  I will continue to edit the grammar and update my commentary of graphs throughout this process\n  \n Thank you for taking the time to read my kernel! If you found it useful or helpful, please give it an upvote. I appreciate any and all comments, questions or suggestions and I will do my best to respond in a timely manner. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}