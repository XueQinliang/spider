{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f5ad892d-d6a8-945a-015d-b0a16aff3bd4"},"source":"<....Work in progress...>\n\nThank you for opening this script!\n\nI have made all efforts to document each and every step involved in the prediction process so that this notebook acts as a good starting point for new Kagglers and new machine learning enthusiasts.\n\nPlease **upvote** this kernel so that it reaches the top of the chart and is easily locatable by new users. Your comments on how we can improve this kernel is welcome. Thanks.\n***\n## Layout of the document\nThe prediction process is divided into two notebooks.\n\nPart 1 : Covers data statistics, data visualization, and feature selection : https://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-on-feature-selection\n\nThis notebook : Covers prediction using various algorithms \n***\n## Data statistics\n* Shape\n* Datatypes\n* Description\n* Skew\n* Class distribution\n\n## Data Interaction\n* Correlation\n* Scatter plot\n\n## Data Visualization\n* Box and density plots\n* Grouping of one hot encoded attributes\n\n## Data Cleaning\n* Remove unnecessary columns\n\n## Data Preparation\n* Original\n* Delete rows or impute values in case of missing\n* StandardScaler\n* MinMaxScaler\n* Normalizer\n\n## Feature selection\n* ExtraTreesClassifier\n* GradientBoostingClassifier\n* RandomForestClassifier\n* XGBClassifier\n* RFE\n* SelectPercentile\n* PCA\n* PCA + SelectPercentile\n* Feature Engineering\n\n## Evaluation, prediction, and analysis\n* LDA (Linear algo)\n* LR (Linear algo)\n* KNN (Non-linear algo)\n* CART (Non-linear algo)\n* Naive Bayes (Non-linear algo)\n* SVC (Non-linear algo)\n* Bagged Decision Trees (Bagging)\n* Random Forest (Bagging)\n* Extra Trees (Bagging)\n* AdaBoost (Boosting)\n* Stochastic Gradient Boosting (Boosting)\n* Voting Classifier (Voting)\n* MLP (Deep Learning)\n* XGBoost\n\n***"},{"cell_type":"markdown","metadata":{"_cell_guid":"b2c56171-517f-e38b-d72a-bf619947081b"},"source":"## Load raw data:\n\nInformation about all the attributes can be found here:\n\nhttps://www.kaggle.com/c/forest-cover-type-prediction/data\n\nLearning: \nWe need to predict the 'Cover_Type' based on the other attributes. Hence, this is a classification problem where the target could belong to any of the seven classes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"06ffea42-305e-1805-6380-b3d5ed0283f8"},"outputs":[],"source":"# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read raw data from the file\n\nimport pandas #provides data structures to quickly analyze data\n#Since this code runs on Kaggle server, train data can be accessed directly in the 'input' folder\ndataset = pandas.read_csv(\"../input/train.csv\") \n\n#Drop the first column 'Id' since it just has serial numbers. Not useful in the prediction process.\ndataset = dataset.iloc[:,1:]"},{"cell_type":"markdown","metadata":{"_cell_guid":"2c949a64-97fe-5d39-a4d2-f2eefb3ab73b"},"source":"## Data Cleaning\n* Remove unnecessary columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"852062ac-990c-03d8-2af9-86a5318a3f63"},"outputs":[],"source":"#Removal list initialize\nrem = []\n\n#Add constant columns as they don't help in prediction process\nfor c in dataset.columns:\n    if dataset[c].std() == 0: #standard deviation is zero\n        rem.append(c)\n\n#drop the columns        \ndataset.drop(rem,axis=1,inplace=True)\n\nprint(rem)\n\n#Following columns are dropped"},{"cell_type":"markdown","metadata":{"_cell_guid":"b08bf340-b519-acbb-4907-2893e60dbe93"},"source":"## Data Preparation\n* Original\n* Delete rows or impute values in case of missing\n* StandardScaler\n* MinMaxScaler\n* Normalizer"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8bd9157c-5c93-8488-9047-e8eeedb97ecd"},"outputs":[],"source":"#get the number of rows and columns\nr, c = dataset.shape\n\n#get the list of columns\ncols = dataset.columns\n#create an array which has indexes of columns\ni_cols = []\nfor i in range(0,c-1):\n    i_cols.append(i)\n#array of importance rank of all features  \nranks = []\n\n#Extract only the values\narray = dataset.values\n\n#Y is the target column, X has the rest\nX_orig = array[:,0:(c-1)]\nY = array[:,(c-1)]\n\n#Validation chunk size\nval_size = 0.1\n\n#Use a common seed in all experiments so that same chunk is used for validation\nseed = 0\n\n#Split the data into chunks\nfrom sklearn import cross_validation\nX_train, X_val, Y_train, Y_val = cross_validation.train_test_split(X_orig, Y, test_size=val_size, random_state=seed)\n\n#Import libraries for data transformations\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\n\n#All features\nX_all = []\n#Additionally we will make a list of subsets\nX_all_add =[]\n\n#columns to be dropped\nrem_cols = []\n#indexes of columns to be dropped\ni_rem = []\n\n#Add this version of X to the list \nX_all.append(['Orig','All', X_train,X_val,1.0,cols[:c-1],rem_cols,ranks,i_cols,i_rem])\n\n#point where categorical data begins\nsize=10\n\nimport numpy\n\n#Standardized\n#Apply transform only for non-categorical data\nX_temp = StandardScaler().fit_transform(X_train[:,0:size])\nX_val_temp = StandardScaler().fit_transform(X_val[:,0:size])\n#Concatenate non-categorical data and categorical\nX_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\nX_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n#Add this version of X to the list \nX_all.append(['StdSca','All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n\n#MinMax\n#Apply transform only for non-categorical data\nX_temp = MinMaxScaler().fit_transform(X_train[:,0:size])\nX_val_temp = MinMaxScaler().fit_transform(X_val[:,0:size])\n#Concatenate non-categorical data and categorical\nX_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\nX_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n#Add this version of X to the list \nX_all.append(['MinMax', 'All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n\n#Normalize\n#Apply transform only for non-categorical data\nX_temp = Normalizer().fit_transform(X_train[:,0:size])\nX_val_temp = Normalizer().fit_transform(X_val[:,0:size])\n#Concatenate non-categorical data and categorical\nX_con = numpy.concatenate((X_temp,X_train[:,size:]),axis=1)\nX_val_con = numpy.concatenate((X_val_temp,X_val[:,size:]),axis=1)\n#Add this version of X to the list \nX_all.append(['Norm', 'All', X_con,X_val_con,1.0,cols,rem_cols,ranks,i_cols,i_rem])\n\n#Impute\n#Imputer is not used as no data is missing\n\n#List of transformations\ntrans_list = []\n\nfor trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n    trans_list.append(trans)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b1b587c5-638e-fa8f-5558-deeb9ba903fa"},"source":"## Feature Selection\nUsing the rankings produced in :\nhttps://www.kaggle.com/sharmasanthosh/forest-cover-type-prediction/exploratory-study-on-feature-selection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"442389b8-bfbe-ec8b-746f-d5222b0f93f7"},"outputs":[],"source":"#Select top 75%,50%,25%\nratio_list = [0.75,0.50,0.25]\n\n#Median of rankings for each column\nunsorted_rank = [0,8,11,4,5,2,5,7.5,9.5,3,8,28.5,14.5,2,35,19.5,12,14,37,25.5,50,44,9,28,20.5,19.5,40,38,20,38,43,35,44,22,24,33,49,42,46,47,27.5,19,31.5,23,28,42,30.5,46,40,12,13,18]\n\n#List of feature selection models\nfeat = []\n\n#Add Median to the list \nn = 'Median'\nfor val in ratio_list:\n    feat.append([n,val])   \n\nfor trans,s, X, X_val, d, cols, rem_cols, ra, i_cols, i_rem in X_all:\n    #Create subsets of feature list based on ranking and ratio_list\n    for name, v in feat:\n        #Combine importance and index of the column in the array joined\n        joined = []\n        for i, pred in enumerate(unsorted_rank):\n            joined.append([i,cols[i],pred])\n        #Sort in descending order    \n        joined_sorted = sorted(joined, key=lambda x: x[2])\n        #Starting point of the columns to be dropped\n        rem_start = int((v*(c-1)))\n        #List of names of columns selected\n        cols_list = []\n        #Indexes of columns selected\n        i_cols_list = []\n        #Ranking of all the columns\n        rank_list =[]\n        #List of columns not selected\n        rem_list = []\n        #Indexes of columns not selected\n        i_rem_list = []\n        #Split the array. Store selected columns in cols_list and removed in rem_list\n        for j, (i, col, x) in enumerate(list(joined_sorted)):\n            #Store the rank\n            rank_list.append([i,j])\n            #Store selected columns in cols_list and indexes in i_cols_list\n            if(j < rem_start):\n                cols_list.append(col)\n                i_cols_list.append(i)\n            #Store not selected columns in rem_list and indexes in i_rem_list    \n            else:\n                rem_list.append(col)\n                i_rem_list.append(i)    \n        #Sort the rank_list and store only the ranks. Drop the index \n        #Append model name, array, columns selected and columns to be removed to the additional list        \n        X_all_add.append([trans,name,X,X_val,v,cols_list,rem_list,[x[1] for x in sorted(rank_list,key=lambda x:x[0])],i_cols_list,i_rem_list])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef7fb5a7-c7be-6615-6441-6a2974b2cf41"},"outputs":[],"source":"#Import plotting library    \nimport matplotlib.pyplot as plt    \n\n#Dictionary to store the accuracies for all combinations \nacc = {}\n\n#List of combinations\ncomb = []\n\n#Append name of transformation to trans_list\nfor trans in trans_list:\n    acc[trans]=[]"},{"cell_type":"markdown","metadata":{"_cell_guid":"bbe1f53e-df03-ae2e-9ac9-1e0e9a0fc448"},"source":"## Evaluation, prediction, and analysis\n* LDA (Linear algo)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c8b2d88-2666-f3a9-098e-dccca8babc33"},"outputs":[],"source":"#Evaluation of various combinations of LinearDiscriminatAnalysis using all the views\n\n#Import the library\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n#Set the base model\nmodel = LinearDiscriminantAnalysis()\nalgo = \"LDA\"\n\n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n#Accuracy of the model using all features\nfor trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n    model.fit(X[:,i_cols_list],Y_train)\n    result = model.score(X_val[:,i_cols_list], Y_val)\n    acc[trans].append(result)\n    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n    #print(result)\ncomb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n        \n#Accuracy of the model using a subset of features    \nfor trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n    model.fit(X[:,i_cols_list],Y_train)\n    result = model.score(X_val[:,i_cols_list], Y_val)\n    acc[trans].append(result)\n    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n    #print(result)\nfor v in ratio_list:\n    comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is 65%. Occurs when all features are used and without any transformation!\n#Performance of MinMax and Normalizer is very poor"},{"cell_type":"markdown","metadata":{"_cell_guid":"39d77f0c-4d50-91ae-33ad-cf4efae8e87d"},"source":"## Evaluation, prediction, and analysis\n* LR (Linear algo)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03f35c5c-643f-f08b-e417-675a9319f5d2"},"outputs":[],"source":"#Evaluation of various combinations of LogisticRegression using all the views\n\n#Import the library\nfrom sklearn.linear_model import LogisticRegression\n\nC_list = [100]\n\nfor C in C_list:\n    #Set the base model\n    model = LogisticRegression(n_jobs=-1,random_state=seed,C=C)\n   \n    algo = \"LR\"\n\n    ##Set figure size\n    #plt.rc(\"figure\", figsize=(25, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"All\",1.0))\n\n    #Accuracy of the model using a subset of features    \n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    for v in ratio_list:\n        comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n      \n#Best estimated performance is close to 67% with LR when C=100 and all attributes are considered and with standardized data\n#Performance improves will increasing value of C\n#Performance of Normalizer and MinMax Scaler is poor in general"},{"cell_type":"markdown","metadata":{"_cell_guid":"53c0cc77-90eb-ea8e-e4e6-46fbbb59b411"},"source":"## Evaluation, prediction, and analysis\n* KNN (Non-linear algo)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af974c32-efad-df50-3921-47f5dca39214"},"outputs":[],"source":"#Evaluation of various combinations of KNN Classifier using all the views\n\n#Import the library\nfrom sklearn.neighbors import KNeighborsClassifier\n\nn_list = [1]\n\nfor n_neighbors in n_list:\n    #Set the base model\n    model = KNeighborsClassifier(n_jobs=-1,n_neighbors=n_neighbors)\n   \n    algo = \"KNN\"\n\n    ##Set figure size\n    #plt.rc(\"figure\", figsize=(25, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_neighbors,\"All\",1.0))\n\n    #Accuracy of the model using a subset of features    \n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    for v in ratio_list:\n        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_neighbors,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n \n#Best estimated performance is close to 86% when n_neighbors=1 and normalizer is used"},{"cell_type":"markdown","metadata":{"_cell_guid":"5aa2fc20-122f-db7a-3f9a-98a8aac9fb87"},"source":"## Evaluation, prediction, and analysis\n* Naive Bayes"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21cb14f7-1d2e-cd10-63b8-472ebc4f5ac9"},"outputs":[],"source":"#Evaluation of various combinations of Naive Bayes using all the views\n\n#Import the library\nfrom sklearn.naive_bayes import GaussianNB\n\n#Set the base model\nmodel = GaussianNB()\nalgo = \"NB\"\n\n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n#Accuracy of the model using all features\nfor trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n    model.fit(X[:,i_cols_list],Y_train)\n    result = model.score(X_val[:,i_cols_list], Y_val)\n    acc[trans].append(result)\n    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n    #print(result)\ncomb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n        \n#Accuracy of the model using a subset of features    \nfor trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n    model.fit(X[:,i_cols_list],Y_train)\n    result = model.score(X_val[:,i_cols_list], Y_val)\n    acc[trans].append(result)\n    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n    #print(result)\nfor v in ratio_list:\n    comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is close to 64%. Original with 50% subset outperfoms all transformations of NB"},{"cell_type":"markdown","metadata":{"_cell_guid":"e7d94b6d-6b7e-f5c2-23a0-1961b49cef2d"},"source":"## Evaluation, prediction, and analysis\n* CART (Non-linear algo)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7716c4f2-0075-6535-e244-5711c38c5dff"},"outputs":[],"source":"#Evaluation of various combinations of CART using all the views\n\n#Import the library\nfrom sklearn.tree import DecisionTreeClassifier\n\nd_list = [13]\n\nfor max_depth in d_list:\n    #Set the base model\n    model = DecisionTreeClassifier(random_state=seed,max_depth=max_depth)\n   \n    algo = \"CART\"\n\n    #Set figure size\n    plt.rc(\"figure\", figsize=(15, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"All\",1.0))\n\n    #Accuracy of the model using a subset of features    \n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    for v in ratio_list:\n        comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n    \n#Best estimated performance is close to 79% when max_depth=13 and for Original"},{"cell_type":"markdown","metadata":{"_cell_guid":"db295e4c-47b9-3341-2c96-41395d2be871"},"source":"## Evaluation, prediction, and analysis\n* SVM (Non-linear algo)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2e1b555-209f-5917-8760-5507a29b98a8"},"outputs":[],"source":"#Evaluation of various combinations of SVM using all the views\n\n#Import the library\nfrom sklearn.svm import SVC\n\nc_list = [10]\n\nfor C in c_list:\n    #Set the base model\n    model = SVC(random_state=seed,C=C)\n\n    algo = \"SVM\"\n\n    #Set figure size\n    #plt.rc(\"figure\", figsize=(15, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"All\",1.0))\n\n    ##Accuracy of the model using a subset of features    \n    #for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n    #    model.fit(X[:,i_cols_list],Y_train)\n    #    result = model.score(X_val[:,i_cols_list], Y_val)\n    #    acc[trans].append(result)\n    #    print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n    #    print(result)\n    #for v in ratio_list:\n    #    comb.append(\"%s with C=%s+%s of %s\" % (algo,C,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Training time is very high compared to other algos\n#Performance is very poor for original. Shows the importance of data transformation\n#Best estimated performance is close to 77% when C=10 and for StandardScaler with 0.25 subset"},{"cell_type":"markdown","metadata":{"_cell_guid":"8007309e-ffd5-6aac-4e2b-46d9783a0c6e"},"source":"## Evaluation, prediction, and analysis\n* Bagged Decision Trees (Bagging)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e128756-f838-f8b0-9671-bfffcc42ce96"},"outputs":[],"source":"#Evaluation of various combinations of Bagged Decision Trees using all the views\n\n#Import the library\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Base estimator\nbase_estimator = DecisionTreeClassifier(random_state=seed,max_depth=13)\n\nn_list = [100]\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = BaggingClassifier(n_jobs=-1,base_estimator=base_estimator, n_estimators=n_estimators, random_state=seed)\n   \n    algo = \"Bag\"\n\n    #Set figure size\n    plt.rc(\"figure\", figsize=(20, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n\n    #Accuracy of the model using a subset of features    \n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    for v in ratio_list:\n        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is close to 82% when n_estimators is 100 for Original"},{"cell_type":"markdown","metadata":{"_cell_guid":"f2d0e657-57d5-d773-77df-be31fba0e74a"},"source":"## Evaluation, prediction, and analysis\n* Random Forest (Bagging)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4c28025-9b76-f634-f33b-1045a4a3dae9"},"outputs":[],"source":"#Evaluation of various combinations of Random Forest using all the views\n\n#Import the library\nfrom sklearn.ensemble import RandomForestClassifier\n\nn_list = [100]\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = RandomForestClassifier(n_jobs=-1,n_estimators=n_estimators, random_state=seed)\n   \n    algo = \"RF\"\n\n    #Set figure size\n    plt.rc(\"figure\", figsize=(20, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n\n    #Accuracy of the model using a subset of features    \n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    for v in ratio_list:\n        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is close to 85% when n_estimators is 100"},{"cell_type":"markdown","metadata":{"_cell_guid":"01de07bd-ff0d-1d7f-8be0-278088394a05"},"source":"## Evaluation, prediction, and analysis\n* Extra Trees (Bagging)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08928963-6c7a-1df4-40aa-ee650995ae6d"},"outputs":[],"source":"#Evaluation of various combinations of Extra Trees using all the views\n\n#Import the library\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nn_list = [100]\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = ExtraTreesClassifier(n_jobs=-1,n_estimators=n_estimators, random_state=seed)\n   \n    algo = \"ET\"\n\n    #Set figure size\n    plt.rc(\"figure\", figsize=(20, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n\n    #Accuracy of the model using a subset of features    \n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    for v in ratio_list:\n        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is close to 88% when n_estimators is 100 , StdScaler with 0.75"},{"cell_type":"markdown","metadata":{"_cell_guid":"417e7f27-53fb-658c-466b-e05be2f29b98"},"source":"## Evaluation, prediction, and analysis\n* AdaBoost (Boosting)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86125f08-0f3b-bc79-4953-2a961a66beef"},"outputs":[],"source":"#Evaluation of various combinations of AdaBoost ensemble using all the views\n\n#Import the library\nfrom sklearn.ensemble import AdaBoostClassifier\n\nn_list = [100]\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = AdaBoostClassifier(n_estimators=n_estimators, random_state=seed)\n   \n    algo = \"Ada\"\n\n    #Set figure size\n    plt.rc(\"figure\", figsize=(20, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n\n    #Accuracy of the model using a subset of features    \n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    for v in ratio_list:\n        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is close to 38% when n_estimators is 100"},{"cell_type":"markdown","metadata":{"_cell_guid":"a35afac7-15fb-dc01-3588-8cc3f31bb781"},"source":"## Evaluation, prediction, and analysis\n* Gradient Boosting (Boosting)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8468b0bb-59e9-b5f9-9b64-ed6c8dc7b21d"},"outputs":[],"source":"#Evaluation of various combinations of Stochastic Gradient Boosting using all the views\n\n#Import the library\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nd_list = [9]\n\nfor max_depth in d_list:\n    #Set the base model\n    model = GradientBoostingClassifier(max_depth=max_depth, random_state=seed)\n   \n    algo = \"SGB\"\n\n    #Set figure size\n    plt.rc(\"figure\", figsize=(20, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"All\",1.0))\n\n    ##Accuracy of the model using a subset of features    \n    #for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n    #    model.fit(X[:,i_cols_list],Y_train)\n    #    result = model.score(X_val[:,i_cols_list], Y_val)\n    #    acc[trans].append(result)\n    #    #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n    #    #print(result)\n    #for v in ratio_list:\n    #    comb.append(\"%s with d=%s+%s of %s\" % (algo,max_depth,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n\n#training time is too high\n#Best estimated performance is close to 86% when depth is 7"},{"cell_type":"markdown","metadata":{"_cell_guid":"4bba0620-cedf-41e4-b9cb-99ab46a198c7"},"source":"## Evaluation, prediction, and analysis\n* Voting Classifier (Voting)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ce798b5-dc06-bb2b-47d1-4973e60f31e7"},"outputs":[],"source":"#Evaluation of various combinations of Voting Classifier using all the views\n\n#Import the library\nfrom sklearn.ensemble import VotingClassifier\n\nlist_estimators =[]\n\nestimators = []\nmodel1 = ExtraTreesClassifier(n_jobs=-1,n_estimators=100, random_state=seed)\nestimators.append(('et', model1))\nmodel2 = RandomForestClassifier(n_jobs=-1,n_estimators=100, random_state=seed)\nestimators.append(('rf', model2))\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nbase_estimator = DecisionTreeClassifier(random_state=seed,max_depth=13)\nmodel3 = BaggingClassifier(n_jobs=-1,base_estimator=base_estimator, n_estimators=100, random_state=seed)\nestimators.append(('bag', model3))\n\nlist_estimators.append(['Voting',estimators])\n\nfor name, estimators in list_estimators:\n    #Set the base model\n    model = VotingClassifier(estimators=estimators, n_jobs=-1)\n   \n    algo = name\n\n    #Set figure size\n    plt.rc(\"figure\", figsize=(20, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    comb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n\n    #Accuracy of the model using a subset of features    \n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    for v in ratio_list:\n        comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is close to 86%"},{"cell_type":"markdown","metadata":{"_cell_guid":"974030fd-cc78-a2b1-24e0-6bbf87e3fa70"},"source":"## Evaluation, prediction, and analysis\n* XGBoost"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe04235b-c66c-22a1-9bae-a4124d2183be"},"outputs":[],"source":"#Evaluation of various combinations of XG Boost using all the views\n\n#Import the library\nfrom xgboost import XGBClassifier\n\nn_list = [300]\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = XGBClassifier(n_estimators=n_estimators, seed=seed,subsample=0.25)\n   \n    algo = \"XGB\"\n\n    #Set figure size\n    plt.rc(\"figure\", figsize=(20, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"All\",1.0))\n\n    #Accuracy of the model using a subset of features    \n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n        model.fit(X[:,i_cols_list],Y_train)\n        result = model.score(X_val[:,i_cols_list], Y_val)\n        acc[trans].append(result)\n        #print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n        #print(result)\n    for v in ratio_list:\n        comb.append(\"%s with n=%s+%s of %s\" % (algo,n_estimators,\"Subset\",v))\n    \n##Plot the accuracies of all combinations\n#fig, ax = plt.subplots()\n##Plot each transformation\n#for trans in trans_list:\n#        plt.plot(acc[trans])\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Display the plot\n#plt.legend(trans_list,loc='best')    \n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is close to 80% when n_estimators is 300, sub_sample=0.25 , subset=0.75"},{"cell_type":"markdown","metadata":{"_cell_guid":"70b659c5-87ae-2e3a-62f5-2668239ce4b8"},"source":"## Evaluation, prediction, and analysis\n* Multi-layer perceptrons (Deep learning)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a48bf097-92a5-4f2f-585c-7c9d8267faf8"},"outputs":[],"source":"#Evaluation of baseline model of MLP using all the views\n\n#Import libraries for deep learning\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n#Import libraries for encoding\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\n\n#no. of output classes\ny = 7\n\n#random state\nnumpy.random.seed(seed)\n\n# one hot encode class values\nencoder = LabelEncoder()\nY_train_en = encoder.fit_transform(Y_train)\nY_train_hot = np_utils.to_categorical(Y_train_en,y) \nY_val_en = encoder.fit_transform(Y_val)\nY_val_hot = np_utils.to_categorical(Y_val_en,y) \n\n\n# define baseline model\ndef baseline(v):\n     # create model\n     model = Sequential()\n     model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n     model.add(Dense(y, init='normal', activation='sigmoid'))\n     # Compile model\n     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n     return model\n\n# define smaller model\ndef smaller(v):\n # create model\n model = Sequential()\n model.add(Dense(v*(c-1)/2, input_dim=v*(c-1), init='normal', activation='relu'))\n model.add(Dense(y, init='normal', activation='sigmoid'))\n # Compile model\n model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n return model\n\n# define deeper model\ndef deeper(v):\n # create model\n model = Sequential()\n model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n model.add(Dense(v*(c-1)/2, init='normal', activation='relu'))\n model.add(Dense(y, init='normal', activation='sigmoid'))\n # Compile model\n model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n return model\n\n# Optimize using dropout and decay\nfrom keras.optimizers import SGD\nfrom keras.layers import Dropout\nfrom keras.constraints import maxnorm\n\ndef dropout(v):\n    #create model\n    model = Sequential()\n    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu',W_constraint=maxnorm(3)))\n    model.add(Dropout(0.2))\n    model.add(Dense(v*(c-1)/2, init='normal', activation='relu', W_constraint=maxnorm(3)))\n    model.add(Dropout(0.2))\n    model.add(Dense(y, init='normal', activation='sigmoid'))\n    # Compile model\n    sgd = SGD(lr=0.1,momentum=0.9,decay=0.0,nesterov=False)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    return model\n\n# define decay model\ndef decay(v):\n    # create model\n    model = Sequential()\n    model.add(Dense(v*(c-1), input_dim=v*(c-1), init='normal', activation='relu'))\n    model.add(Dense(y, init='normal', activation='sigmoid'))\n    # Compile model\n    sgd = SGD(lr=0.1,momentum=0.8,decay=0.01,nesterov=False)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n    return model\n    \nest_list = [('MLP',baseline),('smaller',smaller),('deeper',deeper),('dropout',dropout),('decay',decay)]\n\nfor name, est in est_list:\n \n    algo = name\n\n    #Set figure size\n    plt.rc(\"figure\", figsize=(20, 10))\n\n    #Accuracy of the model using all features\n    for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all:\n        model = KerasClassifier(build_fn=est, v=v, nb_epoch=10, verbose=0)\n        model.fit(X[:,i_cols_list],Y_train_hot)\n        result = model.score(X_val[:,i_cols_list], Y_val_hot)\n        acc[trans].append(result)\n    #    print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n    #    print(result)\n    comb.append(\"%s+%s of %s\" % (algo,\"All\",1.0))\n\n    ##Accuracy of the model using a subset of features    \n    #for trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n    #    model = KerasClassifier(build_fn=est, v=v, nb_epoch=10, verbose=0)\n    #    model.fit(X[:,i_cols_list],Y_train_hot)\n    #    result = model.score(X_val[:,i_cols_list], Y_val_hot)\n    #    acc[trans].append(result)\n    #    print(trans+\"+\"+name+\"+%d\" % (v*(c-1)))\n    #    print(result)\n    #for v in ratio_list:\n    #    comb.append(\"%s+%s of %s\" % (algo,\"Subset\",v))\n\n#Plot the accuracies of all combinations\nfig, ax = plt.subplots()\n#Plot each transformation\nfor trans in trans_list:\n        plt.plot(acc[trans])\n#Set the tick names to names of combinations\nax.set_xticks(range(len(comb)))\nax.set_xticklabels(comb,rotation='vertical')\n#Display the plot\nplt.legend(trans_list,loc='best')    \n#Plot the accuracy for all combinations\nplt.show()    \n\n# Best estimated performance is 71% \n# Performance is poor is general. Data transformations make a huge difference."},{"cell_type":"markdown","metadata":{"_cell_guid":"81915fe8-ff04-8b87-7c53-33f7ebf85127"},"source":"##Make Predictions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c0a79ee-461f-1f28-c47f-7453892cd3ad"},"outputs":[],"source":"# Make predictions using Extra Tress Classifier + 0.5 subset as it gave the best estimated performance\n\nn_estimators = 100\n\n#Obtain the list of indexes for the required model\nindexes = []\nfor trans,name,X,X_val,v,cols_list,rem_list,rank_list,i_cols_list,i_rem_list in X_all_add:\n    if v == 0.5:\n        if trans == 'Orig':\n            indexes = i_cols_list\n            break\n\n#Best model definition\nbest_model = ExtraTreesClassifier(n_jobs=-1,n_estimators=n_estimators)\nbest_model.fit(X_orig[:,indexes],Y)\n\n#Read test dataset\ndataset_test = pandas.read_csv(\"../input/test.csv\")\n#Drop unnecessary columns\nID = dataset_test['Id']\ndataset_test.drop('Id',axis=1,inplace=True)\ndataset_test.drop(rem,axis=1,inplace=True)\nX_test = dataset_test.values\n\n#Make predictions using the best model\npredictions = best_model.predict(X_test[:,indexes])\n# Write submissions to output file in the correct format\nwith open(\"submission.csv\", \"w\") as subfile:\n    subfile.write(\"Id,Cover_Type\\n\")\n    for i, pred in enumerate(list(predictions)):\n        subfile.write(\"%s,%s\\n\"%(ID[i],pred))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}