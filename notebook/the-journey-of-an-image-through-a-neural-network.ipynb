{"cells":[{"metadata":{"_uuid":"3ee4847b5cbaadefa1bd153a5f25fe011534d5cf"},"cell_type":"markdown","source":"# The Journey of an Image Through a Neural Network\n\n***\n**Update 21/7/18:** Added visualisation function using UMAP instead of TSNE, which is much faster. Documentation for UMAP can be found here: https://github.com/lmcinnes/umap\n***\n\nThis is my first time playing with convolutional neural networks, tensorflow and keras, and the beauty of these packages is how quickly you can produce a network that gives very good results, especially on something like the MNIST dataset. However, they hide almost all the details of what they're doing in the background, and I wanted to get an idea of what's really happening behind the scenes.\n\nI spent quite some time trying different approaches to look in to what the networks I created were doing, and the few I thought were most useful are shown in this notebook. The main three are plotting the confusion matrix, visualising the network's ability to separate image classes after each layer using TSNE, and showing how the representation of the image changes after each network layer. The notebook starts with the [class I have written](#classcode) with various functions to produce these network visualisations. But it has several hundred lines of code and to begin with I'd suggest you jump straight to [where I begin putting the class in action](#simplenetwork). For me the aim of this notebook is to understand the networks, rather than the details of the code or the accuracy they achieve.\n\nIf you do want to look at the class code I have added a lot of documentation/comments to it, so hopefully it's understandable. I've also tried to make the class flexible enough to work with different networks, or even different datasets (though I haven't tested that - in particular it's likely non-numeric image classes would give errors). It would be great if people fork the notebook and use the class themselves. Even better if you publicly post what you've done with it and let me know so I can have a look! Also, if you have suggestions for things I could add I'd be interested to hear about those too.\n\n***\n**NOTE:** Descriptions in the text below may not exactly match the images you see, as I have not fixed random seeds so the output can be slightly different each time.\n***\n\n**Sections of this notebook:**\n* [Modules I have used](#modules)\n* [NetworkVisualiser Class](#classcode)\n* [Visualising a simple network with one dense layer](#simplenetwork)\n* [Visualising a network with one convolutional layer](#oneconvol)\n* [Visualising a network with multiple convolutional layers](#multiconvol)\n\n**Links that may be useful:**\n* [Kaggle Deep Learning Track](https://www.kaggle.com/learn/deep-learning): The neural networks I define generally use the same packages and have the same type of infrastructure from these courses, and these work very well on the MNIST dataset.\n* [A Guide to TF Layers: Building a Convolutional Neural Network](https://www.tensorflow.org/tutorials/layers): A guide in the TensorFlow documentation using the MNIST data. It doesn't use keras as I do here, but it gave me some ideas about combinations of layers I could try, including MaxPool2D layers.\n* [Introduction to CNN Keras - 0.997 (top 6%)](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6) by [Yassine Ghouzam](https://www.kaggle.com/yassineghouzam): An excellent and popular kernel that shows how you can get very high accuracies using similar networks to the ones I define here, with a few extra tips and tricks to improve the results.\n* [sklearn documentation on manifold learning](http://scikit-learn.org/stable/modules/manifold.html), which includes the TSNE technique I use here.\n* [sklearn documentation on component analysis](http://scikit-learn.org/stable/modules/decomposition.html)."},{"metadata":{"_uuid":"7a8c35aa80ea381ae24873762b597d5ac126fd16"},"cell_type":"markdown","source":"<a id='modules'></a>\n## Module Imports"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c870394bbf9668de26864c2088f84db3a498e38d"},"cell_type":"code","source":"# general data and misc libraries\nimport pandas as pd\nimport numpy as np\nfrom math import ceil\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# tensorflow/keras for cnn training\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n\n# sklearn component analysis and utilities\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\n\n# umap dimensionality reduction\n#https://github.com/lmcinnes/umap\nimport umap","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39eb9cc34f25fe9d4dc2f671ca08654015dbe5ad"},"cell_type":"markdown","source":"<a id='classcode'></a>\n## NetworkVisualiser class\n\nClick the button on the right to see the class code --------->"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"96d9f01942d5e14287b2bc8fd4f5450431c2c27e","_kg_hide-input":true},"cell_type":"code","source":"class NetworkVisualiser:\n    '''Utility to visualise the progression of an image through the layers of a convolutional neural network. \n    Networks are created with tensorflow.python.keras. \n    \n    layers: A list of tensorflow.python.keras.layers from which the neural network is created.\n    The layers are added to a Sequential model, and the last (output) layer should be a Dense layer\n    with num_classes nodes. It is highly recommended to set the name parameter of each layer\n    to a short meaningful value to aid the interpretation of the visualisations created.\n    \n    data_file: Path to file containing image data. Each image must be represented by a flattened\n    row of pixel values in the file.\n        \n    label_col: Column in data_file to use for image class labels.\n    \n    num_classes: Number of different image classes contained in the data.\n    \n    img_rows: Height of each image in pixels.\n    \n    img_cols: Width of each image in pixels.'''\n    \n    def __init__(self, layers, data_file='../input/train.csv',label_col='label',\n                 num_classes=10,img_rows=28,img_cols=28):\n        \n        # definition of network infrastructure\n        self.layers = layers\n        # no. layers in defined network\n        self.n_layers = len(layers)\n\n        # no. of classes to identify in images\n        self.num_classes = num_classes\n        # height of images in pixels \n        self.img_rows = img_rows\n        # width of images in pixels\n        self.img_cols = img_cols\n        \n        # load data file\n        self.load_data(data_file, label_col)\n                \n    def load_data(self, data_file, label_col):\n        '''Load data from file, separate images and class labels,\n        reshape the images, and create class vectors. Called automatically\n        during class initialisation.\n        \n        data_file: Path to file containing image data. Each image must be\n        represented by a flattened row of pixel values in the file.\n        \n        label_col: Data column to use for image class labels.'''\n        \n        print('Loading data...')\n        df = pd.read_csv(data_file)\n        print('Shape of data file:',df.shape)\n\n        # get data excluding label column\n        X = df.drop(label_col,axis=1)\n       \n        # reconstruct images from flattened rows\n        X = X.values.reshape(len(X),self.img_rows,self.img_cols,1)\n        \n        # normalise X to lie between 0 and 1\n        X = X/X.max()\n       \n        self.X = X\n        print('Shape of network input:',X.shape)\n\n        # extract true label of each image\n        self.labels = df[label_col].values\n        \n        # convert labels in to dummy vectors\n        y = keras.utils.to_categorical(self.labels, self.num_classes)\n        self.y = y\n        print('Shape of label vectors:',y.shape)\n        print('First label vector:',y[0])\n        \n    def show_images(self, images_per_class=10):\n        '''Display example images from each class.\n        \n        images_per_class: defines how many images will be displayed\n        for each class.'''\n        plt.figure(figsize=(images_per_class,self.num_classes))\n\n        for i in range(self.num_classes):\n            # select images in class i\n            tmp = self.X[self.y[:,i]==1]\n\n            # display 1st 10 images in class i\n            for j in range(10):\n                plt.subplot(self.num_classes,10,(10*i)+(j+1))\n                plt.imshow(tmp[j][:,:,0])\n\n                # use the same colour range for each image\n                plt.clim(0,1)\n                # don't show axes\n                plt.axis('off')\n                \n    def fit(self,\n            loss=keras.losses.categorical_crossentropy,\n            optimizer='adam', metrics=['accuracy'],\n            epochs=3, batch_size=100, validation_split=0.2):\n        '''Creates a network using the layers defined in self.layers,\n        fits the network, and then calls self.set_layer_outputs.\n        \n        Arguments are passed to tensorflow.python.keras.models.Sequential.compile\n        and tensorflow.python.keras.models.Sequential.fit.'''\n        \n        # Buld model\n        self.model = Sequential()\n\n        # add each defined layer to the model\n        for layer in self.layers:\n            self.model.add(layer)\n            \n        # get layer names\n        self.layer_names = [self.model.layers[i].name for i in range(self.n_layers)]\n\n        # set model optimisation parameters\n        self.model.compile(loss=loss,optimizer=optimizer,\n                           metrics=metrics)\n\n        # Fit model\n        self.model.fit(self.X, self.y,\n                  batch_size=batch_size,\n                  epochs=epochs,\n                  validation_split = validation_split)\n        \n        # calculate output at each network layer\n        self.set_layer_outputs()\n\n    def set_layer_outputs(self): \n        '''Calculates the representation of each image in each layer of the network.'''\n        \n        print('Calculating output at each layer...')\n        # maximum no. filters in any convolutional layer, used to define figure sizes later\n        # initialise at 9 (min fig size), overwrite in loop if a layer has more than 9\n        self.max_filters = 9\n\n        self.layer_out = []\n        prev_layer_out = None\n\n        for key in self.layer_names:\n            # create a model consisting of only the current layer\n            layer_model = Sequential()\n            layer_model.add(self.model.get_layer(key))\n\n            # if this is the first layer, predict using the input images\n            if prev_layer_out is None:\n                curr_layer_out = layer_model.predict(self.X)\n                #self.layer_out.append(layer_model.predict(self.X))\n            # otherwise, predict using output of previous layer\n            else:\n                curr_layer_out = layer_model.predict(prev_layer_out)\n\n            self.layer_out.append(curr_layer_out)\n            \n            # check if this is a convolution layer with more filters\n            # than the current maximum.\n            if curr_layer_out.ndim==4:\n                n_filters = curr_layer_out.shape[-1]\n                if n_filters>self.max_filters:\n                    self.max_filters = n_filters\n\n            prev_layer_out = curr_layer_out\n\n            print(key, 'layer has shape',curr_layer_out.shape)\n            \n        # probability network assigns to correct label for each image\n        # (self.layer_out[-1] is output layer)\n        self.true_prob = pd.Series([self.layer_out[-1][i,self.labels[i]] for i in range(len(self.labels))])\n\n        \n    def show_conf_matrix(self):\n        '''Calculate and display the network confusion matrix.'''\n        # calculate confusion matrix\n        cm=confusion_matrix(self.labels,\n                            self.layer_out[-1].argmax(axis=1))\n\n        # convert to percentages\n        cm=100*cm/cm.sum(axis=1)\n\n        # display\n        plt.figure(figsize=(10,8))\n        sns.heatmap(cm,annot=True,square=True,cmap=plt.rcParams['image.cmap'],fmt='.1f',\n                    vmax=2*max([np.triu(cm,k=1).max(),np.tril(cm,k=-1).max()]))\n        # vmax: used to limit range of colour bar to highlight incorrect predictions,\n        # rather than letting diagonal dominate. Current limit is double the maximum\n        # incorrect prediction prediction for any number (2*max in upper or lower triangle).\n        plt.ylabel('TRUE')\n        plt.xlabel('PREDICTED')\n        \n    def umap_layers(self,img_ids):\n        '''Use umap to produce a 2D representation of the segmentation\n        of image classes in each layer of the network. \n        \n        img_ids: Indices of images to use to calculate and display results.\n        Choosing ~1000 images typically\n        gives good results.'''\n        \n        # umap to see distinction between digits at different layers\n        print('Calculating UMAP representations...')\n\n        # make figure\n        rows = ceil(np.sqrt(self.n_layers+1))\n        cols = ceil((self.n_layers+1)/rows)\n        plt.figure(figsize=(5*cols,4*rows))\n\n        # visualise structure of input \n        i=1\n        print('Input...',end='')\n        # flatten data\n        pipe_in = self.X[img_ids].reshape([len(img_ids),-1])\n        # transform data\n        pipe_out = umap.UMAP().fit_transform(pipe_in)\n        # plot data, with points coloured by class\n        plt.subplot(rows,cols,i)\n        plt.scatter(pipe_out[:,0],pipe_out[:,1],c=self.labels[img_ids],cmap='tab10',s=2)\n        plt.xticks([]), plt.yticks([])\n        plt.colorbar()\n        plt.title('Input')\n        i+=1\n\n        # visualise structure of network layers\n        for id_layer in range(self.n_layers):\n            key = self.layer_names[id_layer]\n            out = self.layer_out[id_layer]\n            print(key+'...',end='')\n            \n            pipe_in = out[img_ids].reshape([len(img_ids),-1])\n            pipe_out = umap.UMAP().fit_transform(pipe_in)\n\n            plt.subplot(rows,cols,i)\n            plt.scatter(pipe_out[:,0],pipe_out[:,1],c=self.labels[img_ids],cmap='tab10',s=2)\n            plt.xticks([]), plt.yticks([])\n            plt.colorbar()\n            plt.title(key)\n            i+=1\n        print('Done!')\n\n    def tsne_layers(self,img_ids):\n        '''Use sklearn.manifold.TSNE to produce a 2D representation of the segmentation\n        of image classes in each layer of the network. \n        \n        As TSNE is computationally expensive and impractical to use for internal layers\n        that may have thousands of features, the top self.num_classes components in the\n        output of each layer are first extracted using sklearn.decomposition import PCA.\n        TSNE is then run on the PCA components.\n        \n        img_ids: Indices of images to use to calculate and display the TSNE results.\n        Strongly recommended not to use all images, which will likely fail to compute\n        and in any case would produce messy plots. Choosing ~1000 images typically\n        gives good results.'''\n        \n        # t-sne to see distinction between digits at different layers\n        print('Calculating TSNE representations...')\n        # TSNE too computationally expensive to run on data with many features.\n        # First use PCA to extract the first num_classes components, then follow with TSNE.\n        pipe = Pipeline([('pca',PCA(n_components=self.num_classes)),('tsne',TSNE())])\n\n        # make figure\n        rows = ceil(np.sqrt(self.n_layers+1))\n        cols = ceil((self.n_layers+1)/rows)\n        plt.figure(figsize=(5*cols,4*rows))\n\n        # visualise structure of input \n        i=1\n        print('Input...',end='')\n        # flatten data\n        pipe_in = self.X[img_ids].reshape([len(img_ids),-1])\n        # transform data\n        pipe_out = pipe.fit_transform(pipe_in)\n        # plot data, with points coloured by class\n        plt.subplot(rows,cols,i)\n        plt.scatter(pipe_out[:,0],pipe_out[:,1],c=self.labels[img_ids],cmap='tab10',s=2)\n        plt.xticks([]), plt.yticks([])\n        plt.colorbar()\n        plt.title('Input')\n        i+=1\n\n        # visualise structure of network layers\n        for id_layer in range(self.n_layers):\n            key = self.layer_names[id_layer]\n            out = self.layer_out[id_layer]\n            print(key+'...',end='')\n            \n            pipe_in = out[img_ids].reshape([len(img_ids),-1])\n            pipe_out = pipe.fit_transform(pipe_in)\n\n            plt.subplot(rows,cols,i)\n            plt.scatter(pipe_out[:,0],pipe_out[:,1],c=self.labels[img_ids],cmap='tab10',s=2)\n            plt.xticks([]), plt.yticks([])\n            plt.colorbar()\n            plt.title(key)\n            i+=1\n        print('Done!')\n    \n        \n    def visualise_network(self,img_id):\n        '''Visualise one image in all layers of the neural network.\n        \n        img_id: The index of the image to be displayed.'''\n        \n        # no. rows in figure (including extra row for input layer)\n        nrows = self.n_layers+1\n        # no. additional columns for text labels\n        txtwidth = max([int(self.max_filters/12),2])\n        ncols = self.max_filters+txtwidth\n        # text box/font style\n        props = dict(boxstyle='round', facecolor='w')\n        fontsize = 1.3*ncols\n        # create figure\n        plt.figure(figsize=(2*ncols,2*nrows))\n\n        ####################\n        # input\n        ####################\n\n        # title\n        row = 1\n        ax = plt.subplot2grid((nrows, ncols), (0, 0), colspan=txtwidth)\n        plt.plot([0,0],[1,1])\n        ax.text(0, 1, 'Input', verticalalignment='center',fontsize=fontsize,bbox=props)\n        plt.axis('off')\n\n        # show input image\n        plt.subplot2grid((nrows, ncols), (0, int((ncols-txtwidth)/2 + 0.5*txtwidth)), colspan=txtwidth)\n        plt.imshow(self.X[img_id][:,:,0])\n        plt.clim(0,1)\n        plt.axis('off')\n\n        # true label of input image\n        ax = plt.subplot2grid((nrows, ncols), (0, txtwidth), colspan=txtwidth)\n        plt.plot([0,0],[1,1])\n        ax.text(0,1,\n                'True label: '+str(self.y[img_id,:].argmax()),\n                verticalalignment='center',horizontalalignment='center',fontsize=fontsize,bbox=props)\n        plt.axis('off')\n\n        # predicted label for input image\n        ax = plt.subplot2grid((nrows, ncols), (0, ncols-txtwidth-1), colspan=txtwidth)\n        plt.plot([0,0],[1,1])\n        ax.text(0,1,\n                'Predicted: '+\n                str(self.layer_out[-1][img_id,:].argmax())+\n                ' ({:.1f}% probability)'.format(100*self.layer_out[-1][img_id,:].max()),\n                verticalalignment='center',horizontalalignment='center',\n                fontsize=fontsize,bbox=props)\n        plt.axis('off')\n\n        #######################\n        # remaining layers\n        #######################\n\n        for id_layer in range(self.n_layers):\n            key = self.layer_names[id_layer]\n            out = self.layer_out[id_layer]\n\n            row += 1\n\n            # layer title\n            ax = plt.subplot2grid((nrows, ncols), (row-1, 0), colspan=txtwidth)\n            plt.plot([0,0],[1,1])\n            ax.text(0, 1, key, verticalalignment='center',fontsize=fontsize,bbox=props)\n            plt.axis('off')\n\n            # annotated heatmap for output layer\n            if id_layer is self.n_layers-1:\n                plt.subplot2grid((nrows, ncols), (row-1, txtwidth), colspan=ncols-txtwidth)\n                sns.heatmap(out[img_id,:].reshape([1,-1])*100,\n                            annot=True,annot_kws={\"size\": fontsize},fmt='.1f',\n                            cmap=plt.rcParams['image.cmap'],cbar=False)\n                plt.yticks([])\n                plt.xticks(fontsize=fontsize)\n\n            # plot image representation for convolutional layers (4 dimensions: image, row, column, filter)\n            elif out.ndim==4:\n                # no. filters in layer\n                n_filters = out.shape[-1]\n                # no. subplots each filter spans in this layer\n                nsub_per_filt = self.max_filters/n_filters\n                # plot each filter\n                for i in range(n_filters):\n                    plt.subplot2grid((nrows, ncols), (row-1, txtwidth+int(i*nsub_per_filt)), colspan=int(nsub_per_filt))\n                    plt.imshow(out[img_id,:,:,i])\n                    plt.axis('off')\n\n            # 1d image plot for flattened layers (e.g. dense)\n            else:\n                n_filters = out.shape[-1]\n\n                plt.subplot2grid((nrows, ncols), (row-1, txtwidth), colspan=ncols-txtwidth)\n                plt.imshow(out[img_id,:].reshape(1,n_filters),aspect=max([n_filters/20,1]))\n                plt.axis('off')\n\n    def visualise_classbest(self):\n        '''Visualise the image in each class that the network predicts most accurately,\n        i.e. the image assigned the highest probability matching the true image label in each class.'''\n        \n        for i in range(self.num_classes):\n            self.visualise_network(self.true_prob[self.labels==i].sort_values(ascending=False).index[0])   \n\n            \n    def visualise_classworst(self):\n        '''Visualise the image in each class that the network predicts least accurately,\n        i.e. the image assigned the lowest probability matching the true image label in each class.'''\n        \n        for i in range(self.num_classes):\n            self.visualise_network(self.true_prob[self.labels==i].sort_values().index[0])   \n\n    def visualise_best(self, n_plots=5):\n        '''Visualise the images that are most accurately predicted by the model\n        (irrespective of class), i.e. the images with the largest probability\n        assigned to the true image label.\n        \n        n_plots: Number of images to visualise.'''\n        wrong_pred = self.true_prob.sort_values(ascending=False).index[:n_plots]\n\n        for img_id in wrong_pred:\n            self.visualise_network(img_id)\n\n            \n    def visualise_worst(self,n_plots=5):\n        '''Visualise the images that are least accurately predicted by the model\n        (irrespective of class), i.e. the images with the lowest probability\n        assigned to the true image label.\n        \n        n_plots: Number of images to visualise.'''\n        \n        wrong_pred = self.true_prob.sort_values().index[:n_plots]\n\n        for img_id in wrong_pred:\n            self.visualise_network(img_id)\n\n    def visualise_unsure(self,n_plots=5):\n        '''Visualise the images that the network is least confident about which\n        class the image should be assigned to, i.e. the images which have the\n        smallest probabilities assigned to the predicted labels.\n        \n        n_plots: Number of images to visualise.'''\n\n        # visualise predictions network is least sure about - smallest max probability\n        worst_pred = self.layer_out[-1].max(axis=1).argsort()[:n_plots]\n\n        for img_id in worst_pred:\n            self.visualise_network(img_id)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b0994d605b7f61ff767db3490133423dbdef940"},"cell_type":"markdown","source":"<a id='simplenetwork'></a>\n## Starting Simple: A one layer network\n\n### Building the Network\n\nIn the MNIST dataset we have 28 x 28 pixel (785 pixels per image total) greyscale images of digits. Our goal is to predict which single digit is shown in each image using a neural network and I will start with the simplest one possible, a network with no hidden layers. \n\nIn this network, the image is flattened in to a vector (with shape 1 x 785) and this vector is passed directly to the network's output layer, which is a dense layer with ten nodes (one for each digit). A \"dense\" layer is one in which all the outputs from the previous layer contribute to the input of each node in the current layer. In our simple network this means the output node for each digit calculates a weighted sum of all 785 pixel values, plus a further (image independent) bias term. The network therefore has 786 x 10 = 7860 parameters in total. Our predicted digit is the one whose output node has the largest value, and I use the \"softmax\" function so the sum of all node values is equal to 1.\n\nThis network looks something like this:\n\n![Image](https://raw.githubusercontent.com/jackroberts89/kaggle-digit-recognizer/master/simplenetwork.png)\n\nThis basic infrastructure in practice will work more or less like logistic regression, and doesn't take advantage of the strengths of neural networks (let alone convoluted neural networks). But it's a good place to start and gives a baseline to compare with more complex networks.\n\nMy `NetworkVisualiser` class requires the network infrastructure to be given as a list of keras layers, in this case one `Flatten` layer and the `Dense` output layer, defined as below. Note that I make use of the `name` argument for each layer - these strings will be used as plot titles by the `NetworkVisualiser` class."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c7c7a09147dce67cf9c6f2514639cab27666ab33"},"cell_type":"code","source":"num_classes = 10\n\nlayers = [\n          Flatten(name='Flatten'),\n          Dense(num_classes, activation='softmax',\n                          name='Output')\n        ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbd018a4e4c9717f8d0f4867172807728025ea22"},"cell_type":"markdown","source":"As I've setup the `NetworkVisualiser` class with default values appropriate for the MNIST dataset, all I need to specify when initialising the `NetworkVisualiser` object is the list of layers. In the background the class loads the `train.csv` datafile, and it then prints some basic information about the data as well as converting the image labels (true digit values) in to 1 x 10 vectors as required for the tensorflow/keras model."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a9d0c7440563e24ecf0700c07786e0b5a4255d9b"},"cell_type":"code","source":"nvis = NetworkVisualiser(layers)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b98b50a7b8bdfda4a2707b375fd6e6575c8f5d1f"},"cell_type":"markdown","source":"With the `nvis` object created, let's make use of the `NetworkVisualiser.show_images()` function to see what some of the MNIST images look like:"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c39696978a3f2bbd3117e73b4535ca6550e4d274"},"cell_type":"code","source":"# set default colour map to use\nplt.rcParams['image.cmap'] = 'Reds'\n\nnvis.show_images()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dc71b61bf0375f1c0cc7c985c04adb55f548044"},"cell_type":"markdown","source":"The function (by default) displays the first ten images for each digit. It's already possible to see areas where our networks, particularly this first simple one, might struggle, for example:\n* Some digits are written in multiple styles, such as 2, 4 and 7.\n* Some people's handwriting is really bad! E.g. in the row of twos one looks more like a fly than a two, and another looks more like a snake. One of the eights appears to have had its head chopped off, and one of the sixes seems to be asleep."},{"metadata":{"_uuid":"73a922fe16ed6be43aafd95cb490b1207a8778a2"},"cell_type":"markdown","source":"### Fitting the Network\n\nOk let's get to work and fit the network by calling `nvis.fit()`. Again sensible default values to use for MNIST are defined in the class so I call it without arguments (but I could, for example, specify `epochs=10` if I wanted to train the network for longer than the default of 3). As well as optimising all the model parameters (using the `tf.keras` library) the `NetworkVisualiser.fit()` performs an additional step - it calculates and saves the representation of each image in each layer of the network. These values are then used when creating visualisations later."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d32f24790435fc7d152a182059438af4d3bff2ec"},"cell_type":"code","source":"nvis.fit()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1be5dfa91e17250d275c97819c8dad3a3516461"},"cell_type":"markdown","source":"Somewhat surprisingly, this network already achieves better than 91% accuracy on the validation set!"},{"metadata":{"_uuid":"003662a54b94254781dd02f79bd8e4c6e771acad"},"cell_type":"markdown","source":"### Confusion Matrix\n\nTo look in to the performance of the network in a bit more detail let's look at its confusion matrix using `nvis.show_conf_matrix()`, which uses `sklearn` to calculate the matrix and `seaborn` to make the visualisation. The confusion matrix shows how often each digit was predicted correctly by the network, and which digitis they were mistaken for when a digit was incorrectly predicted. In the plot below the annotated values are percentages."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1c601ef38c576eb189ba57b0c9d389cb30bdc4c4"},"cell_type":"code","source":"nvis.show_conf_matrix()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a131b41b54a14645d9dae56a6a361eb570ab0dca"},"cell_type":"markdown","source":"The network is most accurate at predicting 1 (98% accuracy) and least accurate at predicting 5 (86% accuracy). In general the network seems to do a better job of predicting digits with the simplest shapes, and struggle with more complex shapes. 5 is often mistaken with 8 or 3 which have similar features (the 'S' shape within 8, or the curved bottom and flat top of 3), for example."},{"metadata":{"_uuid":"bfb7d341965bd247b5fa85491595ed08e7474415"},"cell_type":"markdown","source":"### TSNE or UMAP Visualisations\n\nTSNE is a dimensionality reduction technique, the output of which can be used to visualise structure within multi-dimensional data. Our input data has 785 dimensions (pixels), and the more complex networks will create even more, but we can use TSNE to reduce that to 2 dimensions and produce some interesting plots.\n\nThis can be done using the `NetworkVisualiser.tsne_layers` function. As TSNE is computationally intensive (and in any case a plot with 40000+ points gets messy) the function is run on a defined subset of the images - here I run it on the first 1000 images by passing in `range(1000)`. The function also uses principal component analysis (PCA) prior to TSNE to further reduce computation time.\n\nAlternatively UMAP, a newer dimensionality reduction technique, can be used instead with the function `NetworkVisualiser.umap_layers`. The `umap_layers` function doesn't use the PCA pre-processing step, but it still much quicker than `tmap_layers`.\n\nThe output of the function is a plot representing the network's ability to separate the images in to different classes after each layer:"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d0130701189529ce8ca6115f845b4756890b9b81"},"cell_type":"code","source":"nvis.umap_layers(range(1000))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c61c13bd63f669d938d108cad54643d84b003f28"},"cell_type":"markdown","source":"Even before the images have been passed through the network (the \"Input\" plot above) the different digits tend to be clustered together in groups, although there is quite some overlap between them (particularly between 4, 7 and 9, for example). The \"flatten\" layer doesn't change the ability of the network to distinguish digits, it only reshapes the data. The plot looks different as UMAP (or TSNE) is not guaranteed to give the same output each time, but if you look closely you will see the \"flatten\" plot is more or less a rotation of the \"input\" plot.\n\nIn the output layer the clustering is much improved - the digits are grouped more tightly and most of the groups are well separated from each other. But some points are assigned to the wrong group and there are still groups that overlap, for example 4 with 9, and 3 with 5 (these were also highlighted as issues in the confusion matrix above)."},{"metadata":{"_uuid":"e196edadf95d8fd5e4013523effb69af140f8f0d"},"cell_type":"markdown","source":"### The Journey of an Image Through the Network\n\nThe confusion matrix and UMAP plots give an idea of how the network is performing globally, but what is actually happening to each image as it steps through the network? The `NetworkVisualiser` class has functions to visualise how an image changes in each layer. \n\nLet's start with `NetworkVisualiser.visualise_classbest()`, which displays each digit in the training set that the network most confidently predicts correctly. Each row of each figure corresponds to a layer of the network, as follows:\n\n* **Input row:** Displays the original label, the correct label for that image and the predicted label (along with the probability with which the network predicted that digit).\n\n* **Flatten row:** Shows a representation of the image after it has been reshaped from 28 x 28 in to a 1 x 785 vector. Darker shades of red correspond to larger pixel values, as in the original image.\n\n* **Output row:** The probability that the image belongs to each digit class, according to the network."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"214e6119a3330cb3b41a6197f63d9224049972d5"},"cell_type":"code","source":"nvis.visualise_classbest()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b012d34fed2b5b6d66c1a85b3607ab91428f179"},"cell_type":"markdown","source":"With this simple network the visualisations are maybe not so interesting, but they get better later! The best predicted images are all clear examples of the most common handwritten form of each digit. I found it interesting to see how certain digits look in flattened form - for example the vertical line of a \"1\" produces many thin, evenly spaced bands in the flattened vector, whereas horizontal lines like the top of \"7\" produce fewer but thicker bands of non-zero values."},{"metadata":{"_uuid":"ce6e30e111c4afaf0aa79cb941e33070d4a6597c"},"cell_type":"markdown","source":"### Images the Network gets really wrong\n\nThe `NetworkVisualiser.visualise_classworst()` function displays the worst network predictions for each digit - the images where the network assigned close to zero probability to the correct digit."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"b1a9c2543dce54275edf4cfc4a72744c26ae3b56"},"cell_type":"code","source":"nvis.visualise_classworst()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0413d9bbd423ca39ece064a35394955551fa794d"},"cell_type":"markdown","source":"In some cases it's clear why the network struggles. The example of 4 above would probably most likely be assigned as a 9 by a human, the network sees a 0, and the 4 is only really visible once you know the true label. Although the 1 appears to be well drawn, the network is probably used to the simpler style of one and mistakes it for a 2. The same probably applies for the 7. For others, such as the 8, it's probably a case of our simple network just not being up to the job."},{"metadata":{"_uuid":"d688665b911557f541b31d316ae69d8c24943eb3"},"cell_type":"markdown","source":"### Images that confuse the network\n\nFinally, the `NetworkVisualiser.visualise_unsure()` function displays the five (by default) images that the network was least sure about which digit to assign them to. Some of these are going to be difficult for any network (or person), like the 7 below which appears to be an upside down mirror image. Others, like the 5s, appear to be quite clear but the simple network struggles with them."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"21adb677a8812a15f8a224095fc4ef3dfaa2c852"},"cell_type":"code","source":"nvis.visualise_unsure()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"912f302441c1bb1dfb409bb18ce70feac0703651"},"cell_type":"markdown","source":"<a id='oneconvol'></a>\n## Adding a Convolutional Layer to the Network\n\nLet's convert our little network into a convolutional neural network by adding a `Conv2D` layer. Our new network looks something like this:\n\n![Image](https://raw.githubusercontent.com/jackroberts89/kaggle-digit-recognizer/master/oneconvolutional.png)\n\nI've added a layer of 12 `Conv2D` filters, each with a 5 x 5 kernel. The 25 values in each kernel grid are parameters the model needs to fit, so this layer adds 25 x 12 = 300 parameters plus 12 bias terms (1 per filter). The kernels are multiplied element-wise with each 5 x 5 pixel region in the input image and then summed ([here](https://www.saama.com/blog/different-kinds-convolutional-filters/) is an explanation of this concept). The output of the convolutional layer is 12 new 'convoluted' images, each of which may highlight different features, such as horizontal or vertical lines, depending on the values in its corresponding kernel.\n\nThe convoluted images are slightly smaller than the input image, 24 x 24 instead of 28 x 28, and flattening the 12 convoluted images gives a 1 x 6912 vector (24 x 24 x 12 = 6912). Each of the final output nodes therefore has 6912 weights and one bias term, giving 69130 parameters. This network has almost 10 times more parameters than the first example."},{"metadata":{"_uuid":"a3ee4208c4c98d3ae857d5f7f79716139113fc28"},"cell_type":"markdown","source":"### Fitting the network\n\nLet's go ahead and create the network in the `NetworkVisualiser` class, adding the `Conv2D` layer with 12 filters to our `layers` list. The functions are the same as before, so I'll jump straight in to calling `nvis.fit()` and `nvis.show_conf_matrix()` to see some first results:"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fa42d36319c19430e6d9350bced2ad6d4c49b98a"},"cell_type":"code","source":"img_rows=28\nimg_cols=28\n\nlayers = [\n          Conv2D(12, kernel_size=(5, 5), \n                      activation='relu',name='Conv',\n                      input_shape=(img_rows, img_cols, 1)),\n          Flatten(name='Flatten'),\n          Dense(num_classes, activation='softmax',\n                          name='Output')\n        ]\n\nnvis = NetworkVisualiser(layers)\n\nnvis.fit()\nnvis.show_conf_matrix()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b911b97897482526830f606263b1dbd1e78782b8"},"cell_type":"markdown","source":"The accuracy on the validation set (`val_acc` after Epoch 3 above) is now better than 97%, a big improvement over the 91% we got with the simple one layer network. The ability of the network to recognise a 5 (as seen in the confusion matrix) was 86% but now has risen all the way to 99%. The digit the network struggles with most is now 9, which is often confused for a 7."},{"metadata":{"_uuid":"10c336a4f99885cd2290bb469cb9d5103f34e96d"},"cell_type":"markdown","source":"### UMAP Visualisation\n\nRunning `nvis.umap_layers()` with the same arguments as before, we see that in the output layer all the digits are well separated into there own clusters, whereas in the simpler network some of the clusters were overlapping. Now there are just a few images assigned to the wrong image cluster. It's more difficult to see any improvement immediately after the convolutional layer, but if you look closely it does look like the clusters are starting to separate away from each other."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cf90bb44ecbef713bd5de82b83c28541b061967f"},"cell_type":"code","source":"nvis.umap_layers(range(1000))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91fb7a7397cb315a42dc46cbdc02791c19db8f6c"},"cell_type":"markdown","source":"### Visualising the Layers of the Network\n\nNow we get to the main purpose of this exercise for me - what are the convolutional layers really doing? The output of running `nvis.visualise_classbest()` now has an extra row, `Conv`, which shows what the image looks like after being processed through each of the twelve 5x5 filters.\n\nEach filter picks out different shapes in the input image. For example, in the visualisation of a 3 below some of the filters clearly pick out the three horizontal arms of the digit, and others pick out the near vertical line of the right-hand side of the 3 or more complex shapes."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"74c01fe358246a20b19cb9889f2e68af8d454b86"},"cell_type":"code","source":"nvis.visualise_classbest()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bea44e505f6ca0f6ee2a49e8619114df63c239fc"},"cell_type":"markdown","source":"### Images the network predicts most incorrectly\n\nIt's just as interesting, and sometimes more insightful, to look at the images the network struggles with, so let's use `nvis.visualise_classworst()` to look at the examples where the network completely fails to identify the correct digit. In some cases the output of the convolutional layer makes it clear why the network makes mistakes. For example, the 'squarely' drawn 6 below is predicted to be a 5 or an 8. Many of the convoluted images indeed look more like a 5 or an 8 than they do a 6. Alternatively, the example of 8 below is badly drawn and has a 'filled in', flat top, which looks like a 5 in many of the convoluted layers and tricks the network."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"2b2107cd278a067b44981bcea5ca368d4d0c7c17"},"cell_type":"code","source":"nvis.visualise_classworst()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"380174ffff5c86b1ff5f99a77d525867cf1b850c"},"cell_type":"markdown","source":"### Images that confuse the network\n\nFinally, let's look at the five images for which the network is most unsure about which digit to predict (using `nvis.visualise_unsure()`. The majority of these images share something in common - the digits are rotated/drawn with a slant. This is something that could be improved by using `keras.preprocessing.image.ImageDataGenerator`, which can scale and rotate the input images whilst training the network to make it more robust to these sorts of issues. But I haven't done that here."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"dc76f01fab6d3d3032cb2c5e6ab98eb25e85b03d"},"cell_type":"code","source":"nvis.visualise_unsure()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6198e28433e8b52137872e4cba1a41076c41d1a0"},"cell_type":"markdown","source":"<a id='multiconvol'></a>\n## Getting Serious: Multiple Convolutional and Dense Layers!\n\nThe last example I'll show is a network with multiple convolutional and dense layers, the kind of network that should be able to achieve good scores on the leaderboard if trained carefully. I'll also introduce a new type of layer, `MaxPool2D`, which is used to downsample an image (see [here](https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networks)). \n\nThe network layers are:\n* **Conv1:** 10 convolutional filters with a 5x5 kernel.\n* **Conv2:** 15 convolutional filters with a 5x5x10 kernel.\n* **Conv3:** 20 convolutional filters with a 3x3x15 kernel.\n* **Conv4:** 25 convolutional filters with a 3x3x20 kernel.\n* **Pool1:** MaxPool2D with size 2x2 and stride 2, which halves the width and height of the image.\n* **Flatten:** Flatten the downsampled, convoluted images into a row vector.\n* **Dense1:** Dense layer with 50 nodes.\n* **Output:** Dense output layer with 10 nodes.\n\nNote that, with multiple convolutional layers, the output images from all filters in the previous layer are passed to each filter in the next layer. So if the first convolutional layer has 10 filters, the kernel in the next layer has a 'depth' of 10 (the kernels are 3-dimensional, with the length of the 3rd dimension being the no. of filters in the previous layer).\n\nTesting my diagram drawing skills to the limit, this network looks something like this:\n\n![Image](https://raw.githubusercontent.com/jackroberts89/kaggle-digit-recognizer/master/multiconvolutional.png)\n\nThe image size in each layer, and the number of parameters that need to be fitted, are:\n\n* **Conv1:** 250 weights + 10 bias. Image size: 24x24\n* **Conv2:** 3750 weights + 15 bias. Image size: 20x20\n* **Conv3:** 2700 weights + 20 bias. Image size: 18x18\n* **Conv4:** 4500 weights + 25 bias. Image size: 16x16\n* **Pool1:** 0 parameters. Image size: 8x8.\n* **Flatten:** 0 parameters. Image size: 1x1600.\n* **Dense1:** 80000 weights + 50 bias. Image size: 1x50.\n* **Output:** 500 weights + 10 bias. Image size: 1x10.\n\nOverall, the number of parameters to fit increases from around 70000 to 90000 compared to the previous (one convolutional layer) model. Including the `Pool2D` layer in particular stops the number of parameters from increasing too much."},{"metadata":{"_uuid":"4b42206da76d04fa1f2fec9cac7dc10b93ab057c"},"cell_type":"markdown","source":"### Fitting the network\n\nLet's define the new list of layers, create the `NetworkVisualiser` object, fit the network, and display the confusion matrix as before. This network takes a lot longer to train, about 2:30 minutes per epoch on my (underpowered) laptop."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fdf2f189febb88e342b616be5956be91ef1bac13"},"cell_type":"code","source":"# Define layers to include in the network\n# recommend giving each layer a meaningful name\n# output should always be dense with num_classes nodes\nlayers = [\n          Conv2D(10, kernel_size=(5, 5), \n                          activation='relu',name='Conv1',\n                          input_shape=(img_rows, img_cols, 1)),\n          Conv2D(15, kernel_size=(5, 5), \n                          activation='relu',name='Conv2'),\n          Conv2D(20, kernel_size=(3, 3), \n                          activation='relu',name='Conv3'),  \n          Conv2D(25, kernel_size=(3, 3), \n                          activation='relu',name='Conv4'),    \n          MaxPool2D(pool_size=(2, 2), strides=2,\n                          name='Pool1'),\n          Flatten(name='Flatten'),\n          Dense(50, activation='relu',name='Dense1'),   \n          Dense(num_classes, activation='softmax',\n                          name='Output')\n        ]\n\nnvis = NetworkVisualiser(layers)\n\nnvis.fit()\nnvis.show_conf_matrix()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22caea6977a4fc771141ee8eb0197058afb85b05"},"cell_type":"markdown","source":"Our new complex network improves the accuracy from 97% to above 98%. By training for more epochs, adding dropout layers, and performing transformations on the input layers it could achieve much better accuracies. The network struggles most with 9, actually getting a worse score than the previous network, but overall does better and correctly identifies six 99.9% of the time!"},{"metadata":{"_uuid":"d4d28d4b7bf540aacb3132370f86b24e2df9f068"},"cell_type":"markdown","source":"### UMAP Visualisations\n\nWith the additional convolutional layers it becomes clear that they improve the separation of the digits into clusters, whereas it wasn't so apparent in these plots before with only one layer. By the output of the 4th convolutional layer clearly defined groups are visible for each digit. The two dense layers then finish the job and tighten and separate the clusters even further."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"58646e43a7bdb8dc32a26e69caf269d6c8168c6e"},"cell_type":"code","source":"nvis.umap_layers(range(1000))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1ec446f1cdd424f738c6527fa74d5733fe8507d"},"cell_type":"markdown","source":"### Visualising the Network\n\nWith all the extra layers we've added there's now much more going on in the plots from `nvis.visualise_classbest()`. After each convolutional layer the filters progressively focus on smaller, more specific regions of the image. After the `MaxPool2D` layer the image is barely recognisable to a human anymore in most cases, but as far as the network is concerned they are clearer than ever before."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"42ef23a805dbbc826140c4a924433d2c5a42fe1f"},"cell_type":"code","source":"nvis.visualise_classbest()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2ec7fe5a6f21f07d3d0b0f75253652baa02e5fb"},"cell_type":"markdown","source":"### Images the Network Gets Wrong\n\nThe images the network incorrectly identifies now all have clear issues, which in some cases would also confuse a human:\n* **0:** incomplete loop tricks the network into seeing a 6.\n* **1:** curved style confuses the network and it sees a 2.\n* **2:** poorly drawn/squashed vertically.\n* **3:** poorly drawn\n* **4:** appears to be a 7 incorrectly labelled as a 4\n* **5:** thick brush strokes make it difficult to identify true digit.\n* **6:** rotated and poorly drawn.\n* **7:** very difficult to distinguish from a 1.\n* **8:** poorly drawn\n* **9:** short tail makes it difficult to distinguish from a 0."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"acc6fe4265b614f98f5101c257ac9b9c3fd45bf5"},"cell_type":"code","source":"nvis.visualise_classworst()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ee5217d52c81be3f016d4b681834baa87a5022f"},"cell_type":"markdown","source":"### Images That Confuse the Network\n\nAgain, most of these difficulties are also understandable. The first image, of an 8, I'd also be more likely to recognise as a 9. The 5 appears to be clearer, but it is drawn strangely and the networks seems to pick up the clear 7 shape in the lower right corner of the image."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"3399d03fb58065d995091a6c73ffd1b39f981af5"},"cell_type":"code","source":"nvis.visualise_unsure()\nplt.savefig('unsure9.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe7e19ebcd1c7d3526bd79f64c49f23a898dacbf"},"cell_type":"markdown","source":"## Summary\n\nThat's all from me for now. Thanks for reading and I hope some of you find it useful! Feel free to ask any questions and like I said at the beginning I'd love to see other people playing with the `NetworkVisualiser` class."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}