{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2e785226-0868-b5e2-651c-4069bae32381"},"source":"#### As the competition is coming to a close, and MNIST has been an integral part of the Machine Learning journey since I started 9 Months ago, going from barely passing 98% accuracy on MNIST to a Single Hidden Layer MLP model that gets 99.5 on the actual MNIST, I thought it would be nice to do something new that no one has done as a \"tribute\".(so many kernels here, can't see all of them).. Also, because it's fun\n\n### Therefore, we create a Residual Network which differs from the Originally proposed models in\n1. Number of Layers\n2. Parameters, specifically filter sizes and number of filters (hey. This is just MNIST after all)\n3. Absence of an Average Pooling layer, and presence of 2 fully connected layers\n4. Presence of Dropout between the final few fully connected layers\n\n# Lets start"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3dca1d38-5474-d966-459f-f77919aa09a2"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nnp.random.seed(2142)\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout, Convolution2D, Input,Activation, ZeroPadding2D, MaxPooling2D, Flatten, merge\nfrom keras.optimizers import SGD\nfrom keras.objectives import sparse_categorical_crossentropy as scc"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7fe289a8-05b7-1b59-f900-a7ca8c8d6cb7"},"outputs":[],"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd392356-7935-74c4-48cf-cfcd4073833b"},"outputs":[],"source":"# let's separate stuff to make it more manageable"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b709c39b-94ad-8af8-c689-e29094c21393"},"outputs":[],"source":"y_train = train['label']\ntrain.drop(['label'], axis=1, inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8a7ff797-8451-ffb9-63cc-f91404f24872"},"source":"## Preprocessing Involved\n\n### 1. Feature Scaling (division by 255)\n### 2. Per Image Normalization"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"944edb0e-6a04-b9ed-0f7c-0071ea0072b2"},"outputs":[],"source":"x_train = train.values.astype('float32') / 255\nx_test = test.values.astype('float32') / 255"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b326a40-9e6e-6962-7d91-65ede7426c3b"},"outputs":[],"source":"# below is a custom code for per image normalization. \n# It is faster than looping\n\n# the constant term is as Advised by Andrew Ng in his UFLDL Tutorials\n\ndef per_image_normalization(X, constant=10.0, copy=True):\n    if copy:\n        X_res = X.copy()\n    else:\n        X_res = X\n\n    means = np.mean(X, axis=1)\n    variances = np.var(X, axis=1) + constant\n    X_res = (X_res.T - means).T\n    X_res = (X_res.T / np.sqrt(variances)).T\n    return X_res"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e51e997b-ff99-a7fb-1292-dc888b1cfa15"},"outputs":[],"source":"x_train = per_image_normalization(x_train)\nx_test = per_image_normalization(x_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4cf4af55-855d-3b4c-2add-bfde375ea0b6"},"source":"### Now, we'll reshape the input to the shape 1, 28, 28   \n\n### I am running theano. An epoch on tensorflow was taking 48 seconds!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39cfd916-73ee-dfbe-5d85-cb20f5346e18"},"outputs":[],"source":"x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\nx_test = x_test.reshape(x_test.shape[0], 1, 28, 28)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5321fdc8-c8b2-d363-9c58-2b14bc62b2e7"},"source":"## Now let's give a brief about Residual Nets, or Simply ResNets\n\n### Residual Networks was an Architecture Proposed by He et al from Microsoft Research Asia, that won the \n### ILSVRC 2015 Classification Competition\n\n### The ILSVRC 2015 ResNet consisted of 152 layers. That is right. 152 layers! In residual networks, the input to a \"residual block\" has a shortcut connection that is merged into the output of the residual block, as given below"},{"cell_type":"markdown","metadata":{"_cell_guid":"2d882e71-3a27-701c-60a4-8766ad7649ff"},"source":"### If someone wants to have a closer look at the original Research Article, visit this link\n### https://arxiv.org/abs/1512.03385"},{"cell_type":"markdown","metadata":{"_cell_guid":"fdea0975-b207-0d91-3ef8-28ae536d6413"},"source":"### In this notebook, we'll look at a 15 layer Residual Network (counting only those layers which have trainable parameters. Activation, Merge, and Dropout Layers are not counted)\n\n### Salient features of the Resnet implemented\n\n1. As originally proposed, no MaxPooling Layers are used. Down Sampling is done by varying the strides and kernels of the Convolutional2D layers only\n\n2. Even though the network is so deep, the number of parameters is very small (as you'll notice later)\n\n3. We use only 2 residual blocks.\n\n4. The original proposal didn't use Dropout. The implemented model has Dropout between the final Dense Layers.\n\n5. The layers are well labelled and it makes it a little easier to see what's actually going on\n\n6. NO HYPERPARAMETERS WERE TUNED, PERIOD. Every parameter, including the number of Feature maps at every Convolution, was arbitrarily chosen. Therefore, there is a lot of space for hyperparam Tuning. \n\n7. The implementation has been done using the Keras Functional API."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88576e55-e700-362d-df72-0eb480674b0e"},"outputs":[],"source":"# lets get to it and define the function that will make up the network"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4ba70fa-dd45-ce45-6674-970344d36ae5"},"outputs":[],"source":"def get_resnet():\n    # In order to make things less confusing, all layers have been declared first, and then used\n    \n    # declaration of layers\n    input_img = Input((1, 28, 28), name='input_layer')\n    zeroPad1 = ZeroPadding2D((1,1), name='zeroPad1', dim_ordering='th')\n    zeroPad1_2 = ZeroPadding2D((1,1), name='zeroPad1_2', dim_ordering='th')\n    layer1 = Convolution2D(6, 3, 3, subsample=(2, 2), init='he_uniform', name='major_conv', dim_ordering='th')\n    layer1_2 = Convolution2D(16, 3, 3, subsample=(2, 2), init='he_uniform', name='major_conv2', dim_ordering='th')\n    zeroPad2 = ZeroPadding2D((1,1), name='zeroPad2', dim_ordering='th')\n    zeroPad2_2 = ZeroPadding2D((1,1), name='zeroPad2_2', dim_ordering='th')\n    layer2 = Convolution2D(6, 3, 3, subsample=(1,1), init='he_uniform', name='l1_conv', dim_ordering='th')\n    layer2_2 = Convolution2D(16, 3, 3, subsample=(1,1), init='he_uniform', name='l1_conv2', dim_ordering='th')\n\n\n    zeroPad3 = ZeroPadding2D((1,1), name='zeroPad3', dim_ordering='th')\n    zeroPad3_2 = ZeroPadding2D((1,1), name='zeroPad3_2', dim_ordering='th')\n    layer3 = Convolution2D(6, 3, 3, subsample=(1, 1), init='he_uniform', name='l2_conv', dim_ordering='th')\n    layer3_2 = Convolution2D(16, 3, 3, subsample=(1, 1), init='he_uniform', name='l2_conv2', dim_ordering='th')\n\n    layer4 = Dense(64, activation='relu', init='he_uniform', name='dense1')\n    layer5 = Dense(16, activation='relu', init='he_uniform', name='dense2')\n\n    final = Dense(10, activation='softmax', init='he_uniform', name='classifier')\n    \n    # declaration completed\n    \n    first = zeroPad1(input_img)\n    second = layer1(first)\n    second = BatchNormalization(0, axis=1, name='major_bn')(second)\n    second = Activation('relu', name='major_act')(second)\n\n    third = zeroPad2(second)\n    third = layer2(third)\n    third = BatchNormalization(0, axis=1, name='l1_bn')(third)\n    third = Activation('relu', name='l1_act')(third)\n\n    third = zeroPad3(third)\n    third = layer3(third)\n    third = BatchNormalization(0, axis=1, name='l1_bn2')(third)\n    third = Activation('relu', name='l1_act2')(third)\n\n\n    res = merge([third, second], mode='sum', name='res')\n\n\n    first2 = zeroPad1_2(res)\n    second2 = layer1_2(first2)\n    second2 = BatchNormalization(0, axis=1, name='major_bn2')(second2)\n    second2 = Activation('relu', name='major_act2')(second2)\n\n\n    third2 = zeroPad2_2(second2)\n    third2 = layer2_2(third2)\n    third2 = BatchNormalization(0, axis=1, name='l2_bn')(third2)\n    third2 = Activation('relu', name='l2_act')(third2)\n\n    third2 = zeroPad3_2(third2)\n    third2 = layer3_2(third2)\n    third2 = BatchNormalization(0, axis=1, name='l2_bn2')(third2)\n    third2 = Activation('relu', name='l2_act2')(third2)\n\n    res2 = merge([third2, second2], mode='sum', name='res2')\n\n    res2 = Flatten()(res2)\n\n    res2 = layer4(res2)\n    res2 = Dropout(0.4, name='dropout1')(res2)\n    res2 = layer5(res2)\n    res2 = Dropout(0.4, name='dropout2')(res2)\n    res2 = final(res2)\n    model = Model(input=input_img, output=res2)\n    \n    \n    sgd = SGD(decay=0., lr=0.01, momentum=0.9, nesterov=True)\n    model.compile(loss=scc, optimizer=sgd, metrics=['accuracy'])\n    return model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3de3584e-af1a-cb9b-dc3f-ec42402560d9"},"outputs":[],"source":"res = get_resnet()"},{"cell_type":"markdown","metadata":{"_cell_guid":"24f83cdf-6f3b-36f7-dee8-aa501e934a7b"},"source":"### Let's checkout the network a bit first.\n\nFirst, we'll print a summary"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24c13648-4f94-a000-3f73-5e78c5263f07"},"outputs":[],"source":"res.summary()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd4ca266-6d14-dbc2-51e3-f5dfff5ef8df"},"source":"### As you can see, for a network this big, the number of parameters is TINY!!!!!!\n### This will greatly aid in avoiding overfitting!\n### Let's visualize the network next.. This will be a big, scrollable picture !!!! ( no pydot here. try on ur own. I'll post the image in comments)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09e7843c-b16f-98eb-589c-c79903978bc2"},"outputs":[],"source":"#from IPython.display import SVG\n#from keras.utils.visualize_util import model_to_dot\n\n#SVG(model_to_dot(res).create(prog='dot', format='svg'))"},{"cell_type":"markdown","metadata":{"_cell_guid":"c1b674f1-3c69-a21b-80c9-480dd4d493bb"},"source":"#### I wish I had a better way to visualize the network. But nonetheless, this looks SICK!"},{"cell_type":"markdown","metadata":{"_cell_guid":"a69bbd08-9ed2-65a4-e44e-a44bc8a19f6b"},"source":"### Now it's time to put the network to test. (note, on theano, can take 1-3 mins to compile)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6bbf4a8d-e494-a11b-3f75-d31fc6f75d6b"},"outputs":[],"source":"# we'll use a simple cross validation split of 5%, because any other cross validation scheme doesn't make sense"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7db0d110-6df7-0e9e-c16e-5822674404de"},"outputs":[],"source":"history = res.fit(x_train, y_train, validation_split=0.05, verbose=2, nb_epoch=1, batch_size=32)"},{"cell_type":"markdown","metadata":{"_cell_guid":"dda44b1c-07f4-08c1-b39a-48eee7b12782"},"source":"### I have only trained the model for a mere 1 epochs because\n\n1. Kaggle notebooks take toooooooo long to run the model.\n2. On my laptop with 8gb ram and gt740m 2gb graphics card, and Theano backend, it takes merely 13 seconds per epoch.\n\n## NOTE : I will be writing about the results obtained on training for 30 epochs.\n\n### Anyways, if there is a lot happening here\n\n1. The max validation accuracy achived is 0.9886. Considering our rather naive validation scheme (meh) and the final results obtained on the leader board (0.98500), this might not seem somethin to be too excited about. However, upon comparing the validation scores with the (max) training accuracy achieved by the model which is a mere 0.9449, one can easily see that the model is nowhere near convergence. (I dont know about you, but on seeing this, my jaw dropped to the floor).\n\n2. Even MLP's easily overfit MNIST, whereas here, you can probably continue to train it for a lot, lot more epochs!\n\n3. Did I mention how good the train accuracy vs validation accuracy setting is for a network that is THIS DEEP!!!!"},{"cell_type":"markdown","metadata":{"_cell_guid":"dc1dd762-feae-a95a-a3f1-63e02165a866"},"source":"### Let's visualize the training process     ( plot attached separately in comments)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"34591d69-5b93-a0a9-ae67-a8ade9cd23f7"},"outputs":[],"source":"import matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6198141a-a1e7-0b8a-2e17-08e1a518a986"},"outputs":[],"source":"#plt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\n#plt.legend(['train', 'val'])\n#plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"09991047-28fc-9d61-d2a2-2be7a14d1054"},"source":"#### Like I said, the model is no-where near convergence. (The plot is for 30 epochs trained on my system)  MY advice: train it for atleast 50 more epochs with early stopping.\n\n## LETS GET SOME PREDICTIONS!!!!\n\n### Well..... Not so fast, there is a lil work here.The Keras functional API doesnt suppost predict_classes function, so we are gonna have to do this manually"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0625aa2c-999e-1e98-afa2-ad9777fa60ef"},"outputs":[],"source":"# SUMMON sklearn's LabelBinarizer\nfrom sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer().fit(y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66a35045-65ac-ddb9-728a-5c2c150aebbf"},"outputs":[],"source":"# lets get predictions now"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3adf155b-47f5-9526-45ec-8dc27c0b62a2"},"outputs":[],"source":"preds = res.predict(x_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b87c99a5-cdf9-ab7e-9c90-b53c232217f9"},"outputs":[],"source":"classes = (preds > 0.5).astype('int32')\n\n\n# for those that dont know what happened, the above statement gave us binarized labels for each class\n# this will give us labels as we need for submission\np = lb.inverse_transform(classes)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1bf01b15-f648-b82d-8087-cec3428a990d"},"source":"### Submission     (uncomment the lines below to submit)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8d08012c-c087-a80b-3800-6def9007b27d"},"outputs":[],"source":"# sub = pd.DataFrame()\n# ids = [x for x in range(1, 28001)]\n# sub['ImageId'] = ids\n# sub['Label'] = p\n# sub.to_csv('resnet.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"be514107-b6db-a4e4-62e8-58cca473495f"},"source":"### With exactly this model, I got a leaderboard score of 0.98500 after training for 30 epochs. To me, that's awesome"},{"cell_type":"markdown","metadata":{"_cell_guid":"92020b7f-fa56-da2d-ab29-d3326144a141"},"source":"## TAKEAWAYS\n\n### ResNets are excellent if you can afford the memory requirements.\n\n1. Even though the network is so deep, the number of parameters stays relatively small, and so does training time.\n\n2. This notebook demonstrates us the reason WHY RESNETS PERFORM SO WELL ON problems like CIFAR10 (results in paper) without any data augmentation.\n\n3. Considering the small number of epochs trained and virtually no hyper parameter optimization at all (I selected kernel params, strides and sizes just to ensure that a valid model is created) I believe there is a lot of space for improvement here!!!!\n\n4. Some such possible places where improvements can be made are\n    a. Using a different optimizer: Different learning rates, different algorithm altogether\n    b. different kernel sizes, filter sizes and strides\n    c. more layers (if you can, once you adjust kernel sizes)\n    d. basically any type of hyperparameter tweaking\n    e. regularization\n    \n5. I said it before, and I say it again, the accuracy of the model on the training set is a mere 0.9467, and THIS AMOUNT OF UNDERFITTING has given us a leaderboard score of 0.98500. THIS IS FRICKIN INSANE!!!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1abfb19-9f22-264f-d339-d21a05414bec"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}