{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"57687896-0a99-6fd6-6ee2-c1177b38d11f"},"source":"Loading the dataset and create lists with the features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b7bb829a-1519-e7ed-9917-dc5e8c77d7ef"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as xgb # XGBoost implementation\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n# read data\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\nfeatures = [x for x in train.columns if x not in ['id','loss']]\n#print(features)\n\ncat_features = [x for x in train.select_dtypes(include=['object']).columns if x not in ['id','loss']]\nnum_features = [x for x in train.select_dtypes(exclude=['object']).columns if x not in ['id','loss']]\n#print(cat_features)\n#print(num_features)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"358cc639-51de-2729-faa4-7c0cbd36b581"},"source":"Now let's see if the loss follows a log-normal distribution!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"678dadbb-89e2-23d8-7cd0-aa425df85065"},"outputs":[],"source":"from scipy.stats import norm, lognorm\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\n\ntrain['log_loss'] = np.log(train['loss'])\n\n# fit the normal distribution on ln(loss)\n(mu, sigma) = norm.fit(train['log_loss'])\n\n# the histogram of the ln(loss)\nn, bins, patches = plt.hist(train['log_loss'], 60, normed=1, facecolor='green', alpha=0.75)\n\n# add the fitted line\ny = mlab.normpdf( bins, mu, sigma)\nl = plt.plot(bins, y, 'r--', linewidth=2)\n\n#plot\nplt.xlabel('Ln(loss)')\nplt.ylabel('Probability')\nplt.title(r'$\\mathrm{Histogram\\ of\\ Ln(Loss):}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(mu, sigma))\nplt.grid(True)\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8bc46af2-bb33-f9ca-c6f0-7a84c20ba718"},"source":"Yepp! It's not perfect, but works for me. It's important to change the target variable in this way as RMSE algorithms (default for all regressions) prefer symmetric data: asymmetric output would bias the prediction towards higher losses."},{"cell_type":"markdown","metadata":{"_cell_guid":"37bb667f-61ad-d9b8-fb1b-caf7ccfd0f00"},"source":"Ok, now change the categorical variables into numeric. I prefer to use only 1 column as the XGBoost algorithm doesn't really benefit from several columns. So I'm going to add the category codes to each categorical grouping value."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1963759a-72be-b3be-ee59-8803bcc59e37"},"outputs":[],"source":"ntrain = train.shape[0]\nntest = test.shape[0]\ntrain_test = pd.concat((train[features], test[features])).reset_index(drop=True)\nfor c in range(len(cat_features)):\n    train_test[cat_features[c]] = train_test[cat_features[c]].astype('category').cat.codes\n\ntrain_x = train_test.iloc[:ntrain,:]\ntest_x = train_test.iloc[ntrain:,:]"},{"cell_type":"markdown","metadata":{"_cell_guid":"642f8314-9b96-dbdb-4448-9224cb38a352"},"source":"Now build up the XGBoost model! As I have not taken care of the ordering of the categorical variables I need to make a larger tree ensemble - let's start with 1000 trees."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"857d3686-e27e-373e-ab85-a8177ec8ca1a"},"outputs":[],"source":"xgdmat = xgb.DMatrix(train_x, train['log_loss']) # Create our DMatrix to make XGBoost more efficient\n\nparams = {'eta': 0.01, 'seed':0, 'subsample': 0.5, 'colsample_bytree': 0.5, \n             'objective': 'reg:linear', 'max_depth':6, 'min_child_weight':3} \n\n# Grid Search CV optimized settings\nnum_rounds = 1000\nbst = xgb.train(params, xgdmat, num_boost_round = num_rounds)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f03385f8-fecc-c808-2bd4-aada3fe8e705"},"source":"What are the most important features?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4291340-6116-8a2d-e37e-f184880ffd0c"},"outputs":[],"source":"import operator\n\ndef ceate_feature_map(features):\n    outfile = open('xgb.fmap', 'w')\n    i = 0\n    for feat in features:\n        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n        i = i + 1\n\n    outfile.close()\n    \nceate_feature_map(features)\n\nimportance = bst.get_fscore(fmap='xgb.fmap')\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\n\ndf = pd.DataFrame(importance, columns=['feature', 'fscore'])\ndf['fscore'] = df['fscore'] / df['fscore'].sum()\n\nplt.figure()\ndf.plot()\ndf.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\nplt.title('XGBoost Feature Importance')\nplt.xlabel('relative importance')\nplt.gcf().savefig('feature_importance_xgb.png')\n\ndf"},{"cell_type":"markdown","metadata":{"_cell_guid":"d6978f1d-aac6-356a-d0bf-44d481843e99"},"source":"It's time for making our prediction."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6bd3678b-cd08-a54b-9619-4b1e5bbfb7e7"},"outputs":[],"source":"test_xgb = xgb.DMatrix(test_x)\nsubmission = pd.read_csv(\"../input/sample_submission.csv\")\nsubmission.iloc[:, 1] = np.exp(bst.predict(test_xgb))\nsubmission.to_csv('xgb_starter.sub.csv', index=None)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}