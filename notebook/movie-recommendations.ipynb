{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"3d9e6727-dbbb-82b3-7807-a6b25a7519b0"},"source":"Here's an attempt to create a recommendation engine with this dataset. Our Naive assumption is that a person's taste in film does not evolve with time."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26da2a2f-dd97-c4cd-6bd8-a7570b95dce9"},"outputs":[],"source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n%pylab inline\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a073912-e84f-f2aa-51f1-853a89b8b8e6"},"outputs":[],"source":"df = pd.read_csv('../input/movie_metadata.csv')\ndf.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"5ec12d92-e36c-f201-50f3-15b3f9333fa0"},"source":"Since cleaning the data is not the focus of this notebook, I'll just dump it all in one cell. That way we can skip over to the nice parts."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71196701-30ef-1c69-2736-7aa8ae69aac0"},"outputs":[],"source":"first_actors = set(df.actor_1_name.unique())\nsecond_actors = set(df.actor_2_name.unique())\nthird_actors = set(df.actor_3_name.unique())\nprint('Those only in first name', len(first_actors - second_actors - third_actors))\nprint('Those only in second name', len(second_actors - first_actors - third_actors))\nprint('Those only in third name', len(third_actors - first_actors - second_actors))\n# ----is it color or not\ndf.color = df.color.map({'Color': 1, ' Black and White':0})\n# ---- Genres as on-off flags instead of strings\nunique_genre_labels = set()\nfor genre_flags in df.genres.str.split('|').values:\n    unique_genre_labels = unique_genre_labels.union(set(genre_flags))\nfor label in unique_genre_labels:\n    df['Genre='+label] = df.genres.str.contains(label).astype(int)\ndf = df.drop('genres', axis=1)\n\n# Titles are supposed to be unique right?\nif len(df.drop_duplicates(subset=['movie_title',\n                                  'title_year',\n                                  'movie_imdb_link'])) < len(df):\n    print('Duplicate Titles Exist')\n    # Let's see these duplicates.\n    duplicates = df[df.movie_title.map(df.movie_title.value_counts() > 1)]\n    duplicates.sort('movie_title')[['movie_title', 'title_year']]\n    # Looks like there are duplicates after all. Let's drop those.\n    df = df.drop_duplicates(subset=['movie_title', 'title_year', 'movie_imdb_link'])\n    # df.info()\ncounts = df.language.value_counts()\ndf.language = df.language.map(counts)\n#df.language\ncount = df.country.value_counts()\ndf.country = df.country.map(count)\n#df.country\ncounts = df.content_rating.value_counts()\ndf.content_rating = df.content_rating.map(counts)\n#df.content_rating\n#df.plot_keywords.head()\nunique_words = set()\nfor wordlist in df.plot_keywords.str.split('|').values:\n    if wordlist is not np.nan:\n        unique_words = unique_words.union(set(wordlist))\nplot_wordbag = list(unique_words)\nfor word in plot_wordbag:\n    df['plot_has_' + word.replace(' ', '-')] = df.plot_keywords.str.contains(word).astype(float)\ndf = df.drop('plot_keywords', axis=1)\n# Is anything left to be done other than imputing?\nprint(df.select_dtypes(include=['O']).columns)\n# We replace director name with counts of movies they've done\ndf.director_name = df.director_name.map(df.director_name.value_counts())\n# We replace actor names with the number of movies they appear in.\ncounts = pd.concat([df.actor_1_name, df.actor_2_name, df.actor_3_name]).value_counts()\n#counts.head()\ndf.actor_1_name = df.actor_1_name.map(counts)\ndf.actor_2_name = df.actor_2_name.map(counts)\ndf.actor_3_name = df.actor_3_name.map(counts)\n# I have no clue what to do with the title. I'll keep it for now in order to search by name\ndf = df.drop(['movie_imdb_link'], axis=1)\n# Let's check if anything is left as object\ndf.select_dtypes(include=['O']).columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b52b8647-e1e6-f60d-ac28-8f61a7d29a36"},"outputs":[],"source":"df.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"e884a5d2-f853-4654-eea4-1aaa3f0c2f9b"},"source":"# Now the data is clean enough. Recommend already!\nIt's filled with holes though. Pun intended. :D\n\nI wanted to try out some fancy imputation (there's a package by that name too) so here goes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40fe4659-32ce-b5e0-7a44-a6b8206ed607"},"outputs":[],"source":"# hold your horses, we still need to fill those missing values.\nnew_style = {'grid': False}\nmatplotlib.rc('axes', **new_style)\nplt.matshow(~df.isnull())\nplt.title('Missing values in the data')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a879086-6506-4b81-acec-9c090ff7f886"},"outputs":[],"source":"# Let's get those rows which are mostly incomplete. I suspect this was because of our\n# new features being created from old ones which were null.\nnullcount = df.isnull().sum(axis=1)\n# Let's just keep those who have less than a hundred missing values\nndf = df.dropna(thresh=100)\nprint(ndf.shape, df.shape)\n# Let's see those nulls again\nplt.matshow(~ndf.isnull())\nplt.title('Missing values in the data')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bea5151d-285f-e2f9-f609-b3c17ed8bc70"},"outputs":[],"source":"# We'll treat fillna as a regression / classification problem here.\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n\ndef reg_class_fill(df, column, classifier):\n    \"\"\"Treat missing values as a classification / regresion problem\"\"\"\n    ndf = df.dropna(subset=[col for col in df.columns if col != column])\n    nullmask = ndf[column].isnull()\n    train, test  = ndf[~nullmask], ndf[nullmask]\n    train_x, train_y = train.drop(column, axis=1), train[column]\n    classifier.fit(train_x, train_y)\n    if len(test) > 0:\n        test_x, test_y = test.drop(column, axis=1), test[column]\n        values = classifier.predict(test_x)\n        test_y = values\n        new_x, new_y = pd.concat([train_x, test_x]), pd.concat([train_y, test_y])\n        newdf = new_x[column] = new_y\n        return newdf\n    else:\n        return ndf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3bcdbcf-d9b0-a2a4-eba0-874afd2809c5"},"outputs":[],"source":"r, c = KNeighborsRegressor, KNeighborsClassifier  # Regress or classify\ntitle_encoder = LabelEncoder()\ntitle_encoder.fit(ndf.movie_title)\nndf.movie_title = title_encoder.transform(ndf.movie_title)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"10652ebc-f97b-f755-9c1d-9d02502b5e03"},"outputs":[],"source":"print(ndf[ndf.columns[:25]].isnull().sum())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d914191-bc45-552a-06d8-428beab17cba"},"outputs":[],"source":"# Since our imputation will impact other imputations, we specify an order\n# Typically we should do this independently and then combine the results, but meh for now\nimpute_order = [('director_name', c), ('title_year', c),\n                ('actor_1_name', c), ('actor_2_name', c), ('actor_3_name', c),\n                ('gross', r), ('budget', r), ('aspect_ratio', r),\n                ('content_rating', r), ('num_critic_for_reviews', r)]\nfor col, classifier in impute_order:\n    ndf = reg_class_fill(ndf, col, classifier())\n    print(col, 'Done')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b2f424a-bf29-3947-9da7-54b5f0ea63fb"},"outputs":[],"source":"# Again we check for what else needs to be imputed.\nndf[ndf.columns[:25]].isnull().sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"840b2ac6-3c18-86dd-fbe5-1e585d3469f2"},"outputs":[],"source":"# Did we get everything?\nndf.isnull().sum().sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"231e40ad-7f59-b553-02c7-6b3b987550fd"},"outputs":[],"source":"# YAY! We did indeed get everything, though it may not have been very good.\n# Now we redo the movie title transformation for our searches.\ntitles = title_encoder.inverse_transform(ndf.movie_title)\n#titles = [i.lower().strip() for i in titles]"},{"cell_type":"markdown","metadata":{"_cell_guid":"04b7b550-410b-75c2-0386-e71968cf3a51"},"source":"# And we are ready to recommend stuff to you love :D\nWe build a simple KD tree recommender."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"430d50a5-e19d-464a-d147-0f63f52628a4"},"outputs":[],"source":"# Give us 5 movies that you liked\ndef get_movies(names):\n    movies = []\n    for name in names:\n        found = [i for i in titles if name.lower() in i.lower()]\n        if len(found) > 0:\n            movies.append(found[0])\n            print(name, ': ', found, 'added', movies[-1], 'to movies')\n        else:\n            print(name, ': ', found)\n    print('-'*10)\n    print(movies)\n    moviecodes = title_encoder.transform(movies)\n    return moviecodes, movies\nnames = ['fight club', 'gump', # This one is Forrest Gump\n                 'usual suspects', 'silence of the lambs']\nmoviecodes, movies = get_movies(names)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ad0d1dc-6463-34e2-c607-1621db382254"},"outputs":[],"source":"data = ndf.drop('movie_title', axis=1)\ndata = MinMaxScaler().fit_transform(data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5232c5b-a853-d9d3-e347-ccf00b371052"},"outputs":[],"source":"# We assume KNN's assumptions as valid and proceede to compute a distance_matrix\nfrom sklearn.neighbors import KDTree\nfrom collections import Counter"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5525c269-3a8a-bbc4-30fe-80cda7104fab"},"outputs":[],"source":"movies"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb779843-2087-0693-0037-338f7cf57714"},"outputs":[],"source":"titles"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79828a7f-f51a-8596-8a1e-604c35c73ce3"},"outputs":[],"source":"tree = KDTree(data, leaf_size=2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"052892fb-85b8-e806-f8ee-575cc30b8eb1"},"outputs":[],"source":"def recommend(movies, tree, titles, data):\n    \"\"\"\n    It is assumed that the movies are in order of decreasing like-able-ness\n    Recommend movies on the basis of the KDTree generated.\n    Return them in order of increasing distance form knowns.\n    \"\"\"\n    titles = list(titles)\n    length, recommendations = len(movies) + 1,[]\n    \n    for i, movie in enumerate(movies):\n        weight = length - i\n        dist, index = tree.query([data[titles.index(movie)]], k=3)\n        for d, m in zip(dist[0], index[0]):\n            recommendations.append((d*weight, titles[m]))\n    recommendations.sort()\n    # Stuff is reorganized by frequency.\n    rec = [i[1].strip() for i in recommendations if i[1] not in movies]\n    rec = [i[1] for i in sorted([(v, k) for k, v in Counter(rec).items()],\n                                reverse=True)]\n    return rec"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c63a473-5399-e8f3-8d23-cd24acfdcf12"},"outputs":[],"source":"\nrec = recommend(movies, tree, titles, data)\n\nprint('Rank | Movie')\nprint('-----|------')\nfmt = '{}.   | {}'\nfor index, movie in enumerate(rec[:9]):\n    print(fmt.format(index + 1, movie))"},{"cell_type":"markdown","metadata":{"_cell_guid":"da79ebde-e74e-7a8f-4b76-0df5bab6d8b6"},"source":"# Tadaa!\nIt's not very neat and awesome! But I did like Untraceable to be honest. \nSome movies are recommended twice! Probably because they are quiet close to multiple choices.\n\n## What else can be done?\n\n- Feature generation: I've done a nasty job of generating features. That could be cleaned up.\n- Imputation: A better way of imputing is welcome. Perhaps even need I say.\n- Some other recommendation method: So far I've only been able to discover KDTrees. If someone could write another one, awesome!\n\n*Upvote* to show your appreciation. :D\n\n# The final product\n\n1. Get movie titles\n2. Recommend"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9fe7a2e8-4792-a6e6-2a6a-6e71fd32691d"},"outputs":[],"source":"names = ['hesher', 'leaving las vegas'] # dedicated to A.S.\nmoviecodes, movies = get_movies(names)\nrec = recommend(movies, tree, titles, data)\nprint('-'*50)\nprint('Recommending on the basis of the above movies')\nprint('-'*50)\nprint()\nprint('+-----|------')\nprint('|Rank | Movie')\nprint('+-----|------')\nfmt = '|{}.   | {}'\nfor index, movie in enumerate(rec[:9]):\n    print(fmt.format(index + 1, movie))\nprint('+-----|------')"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}