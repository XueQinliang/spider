{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a346c810-05f1-1eda-050b-275c6c3ace14"},"source":"## Temporal corelations\n\nThe idea of this notebook is to see how corelation between various features and target variable change over time.\n\nAs has been observed in other EDA notebooks, there is very little corelation between given features and y variable.\n\nAs pointed by Raddar here, distribution of features seem to vary quite a bit for many features with time."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c84f66c-eb06-f9e6-be15-0bd68234644b"},"outputs":[],"source":"import kagglegym\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = 8, 6"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07b152db-e4f9-6f1b-e9c2-23104dabf707"},"outputs":[],"source":"train_all = pd.read_hdf('../input/train.h5')\n\n#Skipping alternate timestamps to be able to run this kernel\n\n#odd_timestamps = [t for t in  train_all.timestamp.unique() if t%2 !=0]\n#print(len(odd_timestamps))\n\n#train_all = train_all.loc[train_all.timestamp.isin(odd_timestamps)]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f71563e-a939-b5d5-5c44-59753d941daa"},"outputs":[],"source":"#Lets get all features and devide into derived, fundamental and technical categories\nfeats = [col for col in train_all.columns if col not in ['id', 'timestamp', 'y']]"},{"cell_type":"markdown","metadata":{"_cell_guid":"07432bc2-a049-e084-1dfe-5303f39de4f7"},"source":"It would be good idea to do a box plot of all variables after clip them appropriately (as there are lot of very high values)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b56f2475-6768-3ce1-14fe-2f2acf98372d"},"outputs":[],"source":"train_all[feats] = train_all[feats].clip(upper=5, lower=-5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e4c0e6f-639b-9515-7181-3226860411df"},"outputs":[],"source":"train_all[feats].plot(kind='box')\nplt.ylim([-2,2])"},{"cell_type":"markdown","metadata":{"_cell_guid":"d86fecf5-125c-43b0-07e5-45452f099490"},"source":"All the features lie mostly within -2 to 2. So, clipping features at -2 and 2 could be a good idea for modeling purposes"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4892921-7693-efad-e2c8-75d74e68bd84"},"outputs":[],"source":"#Group by mean, median and 2 sigma. (Note! Many of the features are not normal, \n#hence 2 sigma  does not give complete picture but good for starters) \n#(No pun intended for sponsor of competition ;)\n\ntrain_all_mean = train_all.groupby('timestamp').apply(lambda x: x.mean())\n\ntrain_all_2stdp = train_all.groupby('timestamp').apply(lambda x: x.mean() + 2*x.std())\n\ntrain_all_2stdm = train_all.groupby('timestamp').apply(lambda x: x.mean() - 2*x.std())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8598c055-54f9-5442-fc0c-53353f603bd7"},"outputs":[],"source":"tmp1 = pd.melt(train_all_mean, value_vars=feats, var_name='features1', value_name='mean')\ntmp2 = pd.melt(train_all_2stdp, value_vars=feats, var_name='features2', value_name='2_sigma_plus')\ntmp3 = pd.melt(train_all_2stdm, value_vars=feats, var_name='features3', value_name='2_sigma_minus')\n\ntmp = pd.concat([tmp1, tmp2, tmp3], axis=1)\n\nfg = sns.FacetGrid(data=tmp, col='features1', col_wrap=3, size=2.7)\nfg.map(plt.plot, 'mean', color='blue')\nfg.map(plt.plot, '2_sigma_plus', color='black')\nfg.map(plt.plot, '2_sigma_minus', color='black')\n#fg.map(plt.fill_between, 'mean', '2_sigma_minus', '2_sigma_plus', color='Purple', alpha=0.3)\n\n#plt.ylim([-4.5, 4.5])\n\ndel tmp1, tmp2, tmp"},{"cell_type":"markdown","metadata":{"_cell_guid":"2fcf4ac4-755e-281a-5cf3-e679d2b0f2b3"},"source":"* As has been already been pointed out by raddar, variables that are stable (flat mean and variance) over time are more suitable for modeling.**\n\n* Removing(or modeling separately) two periods of high variance in y, can provide additional paramters for modeling \n\n\nNow, lets see how corelation with y changes for different variables"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c325702-6e11-0c02-ed28-c3e18749ae14"},"outputs":[],"source":"#I had to impute missing data otherwise either it takes very long time or we get lot of NaN \nimport time\ndef get_corr(x):\n    s= time.time()\n    #for f in feats:\n    #    corr.append(np.corrcoef(x[f],x['y'],rowvar=0)[0,1])\n    corr = np.corrcoef(x.values.T)[-1,2:-1]\n    #print(time.time()- s)\n    return corr\n    \ntrain_all_imputed = train_all.fillna(0)\ntrain_all_corr = train_all_imputed.groupby('timestamp').apply(get_corr) #This will take some time\ntrain_all_corr = pd.DataFrame(np.vstack(train_all_corr), columns=feats)\ntrain_all_corr.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54901e39-f895-d537-a398-df76ba4a2daa"},"outputs":[],"source":"tmp3 = pd.melt(train_all_corr, value_vars=feats, var_name='features3', value_name='corr')\n\nfg = sns.FacetGrid(data=tmp3, col='features3', col_wrap=3, size=2.8)\nfg.map(plt.plot, 'corr', color='blue').add_legend()\ndel tmp3"},{"cell_type":"markdown","metadata":{"_cell_guid":"cc1c9812-d39a-da51-df61-66e3d0269d68"},"source":"All the corelation coefficients are oscillating quite a lot. Almost all features show positive and negative corelations with high frequency, most likely because of high volatility of y."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b70c7e49-6255-26c6-97ec-5e0deddd7903"},"outputs":[],"source":"def get_corr2(x):\n    corr = np.corrcoef(x.values.T)[-1,2:-2]\n    return corr\n    \n    \ntrain_all_imputed['abs_y'] = abs(train_all_imputed['y'])\ntrain_all_corr2 = train_all_imputed.groupby('timestamp').apply(get_corr2) #This will take some time\ntrain_all_corr2 = pd.DataFrame(np.vstack(train_all_corr2), columns=feats)\ntrain_all_corr2.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b0f58f6-b60c-5e02-50a2-99a9a1dc3e83"},"outputs":[],"source":"tmp4 = pd.melt(train_all_corr2, value_vars=feats, var_name='features3', value_name='corr')\n\nfg = sns.FacetGrid(data=tmp4, col='features3', col_wrap=3, size=2.7)\nfg.map(plt.plot, 'corr', color='blue').add_legend()\n\ndel tmp4"},{"cell_type":"markdown","metadata":{"_cell_guid":"e7cf6826-57eb-1e61-7afd-243b55a63936"},"source":"** Many features are showing strong corelations with absolute values of y. **\n\n** Building separate models for absolute values of y and direction of y can be a good approach **\n\nAlso, they seem to be varying over time.**\n\nNormalize everything and plot together"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22016fdf-d1e1-fde5-be5f-676b23921807"},"outputs":[],"source":"tmp1 = pd.melt(train_all_mean, value_vars=feats, var_name='features1', value_name='mean')\ntmp2 = pd.melt(train_all_2stdp, value_vars=feats, var_name='features2', value_name='2std')\ntmp3 = pd.melt(train_all_corr, value_vars=feats, var_name='features3', value_name='corr')\ntmp4 = pd.melt(train_all_corr2, value_vars=feats, var_name='features3', value_name='yabs_corr')\n\ntmp = pd.concat([tmp1, tmp2, tmp3, tmp4], axis=1)\ncols = ['mean', '2std', 'corr', 'yabs_corr']\ntmp[cols] = tmp[cols].apply(lambda x: (x - x.mean())/x.std())\nfg = sns.FacetGrid(data=tmp, col='features1', col_wrap=3, size=2.7)\nfg = fg.map(plt.plot, 'mean', color='red')\nfg = fg.map(plt.plot, 'corr', color='green', alpha=0.4)\nfg = fg.map(plt.plot, '2std', color='black')\nfg = fg.map(plt.plot, 'yabs_corr', color='purple', alpha=0.4)\nfg.add_legend()\n\n\ndel tmp1, tmp2, tmp3, tmp4, tmp"},{"cell_type":"markdown","metadata":{"_cell_guid":"e43660c2-f99f-2dd5-17f2-0c912ec319fe"},"source":"**Many features have same trend as their correlation with absolute values of y. Some sort of temporal correction for features can improve lineal models **"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a0ad130-7cf9-cacb-f292-4870a3caf4c6"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}