{"cells":[{"metadata":{"_uuid":"9fbf7c8505970bdafe52d4993cd1c4981747eeaa"},"cell_type":"markdown","source":"## Baseline Model Pipeline   \n\nHi, This kernel is automatically generated by the [Aster](https://github.com/shivam5992/aster) - The kaggle bot to generate baseline kernels for a variety of datasets / competitions. In this kernel, I am using the given dataset for exploration, preprocessing, modelling purposes. Let me walk you through the contents of this kernel:\n\n### Contents \n\n1. Environment Preparation\n2. Quick Exploration   \n&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Dataset Preparation   \n&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Dataset Snapshot and Summary    \n&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Target Variable Distribution    \n&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Missing Values    \n&nbsp;&nbsp;&nbsp;&nbsp; 2.5 Variable Types  \n&nbsp;&nbsp;&nbsp;&nbsp; 2.6 Variable Correlations\n3. Preprocessing  \n&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Label Encoding    \n&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Missing Values Treatment     \n&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Feature Engineering (text fields)  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3.1 TF-IDF Vectorizor  \n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3.2 Top Keywords - Wordcloud    \n&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Train Test Split    \n4. Modelling   \n&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Logistic Regression  \n&nbsp;&nbsp;&nbsp;&nbsp; 4.2 Decision Tree    \n&nbsp;&nbsp;&nbsp;&nbsp; 4.3 Random Forest  \n&nbsp;&nbsp;&nbsp;&nbsp; 4.4 ExtraTrees Classifier  \n&nbsp;&nbsp;&nbsp;&nbsp; 4.5 Extereme Gradient Boosting  \n5. Feature Importance   \n6. Model Ensembling  \n&nbsp;&nbsp;&nbsp;&nbsp; 6.1 A simple Blender  \n7. Creating Submission"},{"metadata":{"_uuid":"4aec26a38c021a8af580835478d0e067e62f7097"},"cell_type":"markdown","source":"## Step 1: Prepare Environment\nAs the first step, lets load all the required libraries to be used in the kernel"},{"metadata":{"_uuid":"d16bf2cddf62860c827d4d81038ce5bc1591dde2","trusted":false},"cell_type":"code","source":"## modelling libraries\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\n\n## preprocessing libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np \nimport itertools\nimport os \n\n## visualization libraries\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint (\"all libraries imported successfully\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc56657249e2d8661cdcdacb8b7c38e15686d4cc"},"cell_type":"markdown","source":"## Step 2: Quick Exploration\nIn the next step, lets load the dataset into my memory and perform a quick exploratory analysis \n\n### 2.1 Dataset Preparation"},{"metadata":{"_uuid":"b389ca63daf657821d90ec0280414e836f86de5d","trusted":false},"cell_type":"code","source":"## read dataset\ntrain_path = \"../input/mushrooms.csv\"\ntrain_df = pd.read_csv(train_path)\ntrain_copy = train_df.copy()\n\ntest_path = \"../input/test.csv\"\ntest_df = pd.DataFrame()\nif os.path.exists(test_path):\n    test_df = pd.read_csv(test_path)\n\nprint (\"dataset loaded\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"388b991d6890103e65eb03e18d0a44942ce01d99","trusted":false},"cell_type":"code","source":"## separate predictors and target variables\n_target = \"class\"\nY = train_df[_target]\ndistinct_Y = Y.value_counts().index\n\n## separate the id column\n_id = \"\"\nif _id == \"\": ## if id is not present, create a dummy \n    _id = \"id\"\n    train_df[_id] = 1\n    test_df[_id] = 1\nif _id not in list(test_df.columns):\n    test_df[_id] = 1\n    \n## drop the target and id columns\ntrain_df = train_df.drop([_target, _id], axis=1)\ntest_id = test_df[_id]\ntest_df = test_df.drop([_id], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b0e8d664504817c2f500b5da7fcb2e6434eda48","trusted":false},"cell_type":"code","source":"## flag variables (used by bot to write the relevant code)\ntextcol = \"\"\ntag = \"num\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31696787fd49b839e9435d6bb8b6e2f5f37e543c"},"cell_type":"markdown","source":"### 2.2 Dataset snapshot and summary\n\nLets look at the dataset snapshot and the summary"},{"metadata":{"_uuid":"3af8f396fd4d2d965030309fb7f152821156924c","trusted":false},"cell_type":"code","source":"## snapshot of train and test\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6eb1b1201b8b0bb0e5a351feb037bd51c341894d","trusted":false},"cell_type":"code","source":"## summary of train and test\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5e13bb41a9dcd133bd835fb9c9d47e1cefd30ae"},"cell_type":"markdown","source":"### 2.3 Target variable distribution\n\nLets plot the distribution of target variable"},{"metadata":{"_uuid":"3a380db8a2f6acbbfbf8ee0ec9b6c4fb76751b00","trusted":false},"cell_type":"code","source":"tar_dist = dict(Counter(Y.values))\n\nxx = list(tar_dist.keys())\nyy = list(tar_dist.values())\n\nplt.figure(figsize=(5,3))\nsns.set(style=\"whitegrid\")\nax = sns.barplot(x=xx, y=yy, palette=\"rocket\")\nax.set_title('Distribution of Target')\nax.set_ylabel('count');\nax.set_xlabel(_target);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0e4edcf1449e23fbf8740b0a48fcf2a9077b802"},"cell_type":"markdown","source":"lets generate some plots related to dataset"},{"metadata":{"_uuid":"5933ec0b1421ae74319e49748e47e1ecbdaf3437","trusted":false},"cell_type":"code","source":"if tag == \"doc\":\n    txts = []\n    for i, y in enumerate(distinct_Y):\n        txt = \" \".join(train_copy[train_copy[_target] == y][\"text\"]).lower()\n        txts.append(txt)\n\n    for j, text in enumerate(txts):\n        wc = WordCloud(background_color=\"black\", max_words=2000, stopwords=STOPWORDS)\n        wc.generate(text)\n        plt.figure(figsize=(9,8))\n        plt.axis(\"off\")\n        plt.title(\"Most frequent words - \" + distinct_Y[j], fontsize=20)\n        plt.imshow(wc.recolor(colormap= 'cool' , random_state=17), alpha=0.95)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c23ad1b627be5fe86a0e48ac66d479aaaed5ec38"},"cell_type":"markdown","source":"### 2.4 Missing Value Counts \n\nLets check the count of missing values in the datasets"},{"metadata":{"_uuid":"5c0e222a1492915d6a6b65d434b56d681bb5008b","trusted":false},"cell_type":"code","source":"mcount = train_df.isna().sum() \nxx = mcount.index \nyy = mcount.values\n\nmissing_cols = 0\nfor each in yy:\n    if each > 0:\n        missing_cols += 1\nprint (\"there are \" + str(missing_cols) + \" columns in the dataset having missing values\")\n\nif missing_cols > 0:\n    plt.figure(figsize=(12,5))\n    sns.set(style=\"whitegrid\")\n    ax = sns.barplot(x=xx, y=yy, palette=\"gist_rainbow\")\n    ax.set_title('Number of Missing Values')\n    ax.set_ylabel('Number of Columns');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec4cbfefacea383333241d36db0ee3f08b6d94d6"},"cell_type":"markdown","source":"### 2.5 Variable Types\n\nLets count the number of numerical and categorical columns in the dataset"},{"metadata":{"_uuid":"5949964cc5fef8d6e2b929fca14e0a5b5957cc8d","trusted":false},"cell_type":"code","source":"## find categorical columns in the dataset \nnum_cols = train_df._get_numeric_data().columns\ncat_cols = list(set(train_df.columns) - set(num_cols))\n\nprint (\"There are \" + str(len(num_cols)) + \" numerical columns in the dataset\")\nprint (\"There are \" + str(len(cat_cols)) + \" object type columns in the dataset\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1925b30ba09a6545cb367c5a29c8ade0d7e99586"},"cell_type":"markdown","source":"### 2.6 Variable Correlations (Only Numerical Fields)\n\nLets plot the correlations among the variables. The generated graph can give an idea about features which are highly, moderately or least correlated with one another."},{"metadata":{"_uuid":"38077429d9191d93ded2f0dce77305e31f3cbb61","trusted":false},"cell_type":"code","source":"get_corr = False\ncorr = train_df.corr()\nif len(corr) > 0:\n    get_corr = True\n    colormap = plt.cm.BrBG\n    plt.figure(figsize=(10,10));\n    plt.title('Pearson Correlation of Features', y=1.05, size=15);\n    sns.heatmap(corr, linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True);\nelse:\n    print (\"No variables available for correlation\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e903375e2414a39ba65e81fe3679ef2763f5b458"},"cell_type":"markdown","source":"## Step 3: Data Preprocessing\n\nIn the data preprocessing step, we will perform label encoding of categorical variables and handle missing values.\n\n### 3.1 Label Encoding\nIn this step, convert the categorical variables into label encoded forms"},{"metadata":{"_uuid":"1c6f46f3970284aa67cc9c50e3a8aa0729ed147b","trusted":false},"cell_type":"code","source":"columns = train_df.columns\nnum_cols = train_df._get_numeric_data().columns\ncat_cols = list(set(columns) - set(num_cols))\n    \nif tag == \"doc\":\n    print (\"No columns available for label encoding\")\nelif len(cat_cols) > 0:\n    for col in cat_cols: \n        le = LabelEncoder()\n        \n        if col in list(test_df.columns):\n            le.fit(list(train_df[col].values) + list(test_df[col].values))\n        else:\n            le.fit(list(train_df[col].values))\n        \n        train_df[col] = le.transform(list(train_df[col].values))\n        try:\n            test_df[col] = le.transform(list(test_df[col].values))\n        except:\n            pass\n        \n## label encode the target variable (if object type)\nif Y.dtype.name == \"object\":\n    le = LabelEncoder()\n    Y = le.fit_transform(Y.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6003a90f831788e84791ba862789b2d89b29a11"},"cell_type":"markdown","source":"### 3.2 Missing Values Treatment\n\nHandle the missing values, for continuous variables, replace by mean. For categorical variables, replace by mode"},{"metadata":{"_uuid":"ffe9436bb02f3d2576321680ff26333d53ee6f2f","trusted":false},"cell_type":"code","source":"if tag == \"doc\":\n    train_df[textcol] = train_df[textcol].fillna(\"\")\n    if textcol in test_df:\n        test_df[textcol] = test_df[textcol].fillna(\"\")\nelse:\n    ## for numerical columns, replace the missing values by mean\n    train_df[num_cols] = train_df[num_cols].fillna(train_df[num_cols].mean())\n    try:\n        test_df[num_cols] = test_df[num_cols].fillna(test_df[num_cols].mean())\n    except:\n        pass \n    \n    ## for categorical columns, replace the missing values by mode\n    train_df[cat_cols] = train_df[cat_cols].fillna(train_df[cat_cols].mode())    \n    try:\n        test_df[cat_cols] = test_df[cat_cols].fillna(test_df[cat_cols].mode())\n    except:\n        pass\nprint (\"Treated missing values in the dataset\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aff5f9a5e22e5575b015511b74e87f9dda74e447"},"cell_type":"markdown","source":"### 3.3 Feature Engineering (only for text fields)\n\nIn this section, we will create relevant features which can be used in the modelling\n\n#### 3.3.1 Tf IDF features"},{"metadata":{"_uuid":"e0e5eae1d0610ae51df0e06ce95eea82eaf9c8f2","trusted":false},"cell_type":"code","source":"if tag == \"doc\":\n    tfidf = TfidfVectorizer(min_df=3,  max_features=None, analyzer='word', \n                            token_pattern=r'\\w{1,}', stop_words = 'english')\n    tfidf.fit(list(train_df[textcol].values))\n    xtrain = tfidf.transform(train_df[textcol].values) \n    if textcol in test_df.columns:\n        xtest = tfidf.transform(test_df[textcol].values)\nelse:\n    xtrain = train_df\n    xtest = test_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d31cdd10ea7937df0adbb4cc0546334de5f31882","trusted":false},"cell_type":"code","source":"if tag != \"doc\":\n    print (\"Lets plot the dataset distributions after preprocessing step ... \")\n    ## pair plots\n    sns.pairplot(train_df, palette=\"cool\")\n    \n    ## distributions\n    columns=train_df.columns\n    plt.subplots(figsize=(18,15))\n    length=len(columns)\n    for i,j in itertools.zip_longest(columns,range(length)):\n        plt.subplot((length/2),3,j+1)\n        plt.subplots_adjust(wspace=0.2,hspace=0.5)\n        train_df[i].hist(bins=20, edgecolor='white')\n        plt.title(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e59cc4df77e7bc829fc0f8eb1ac550593bc80ef6"},"cell_type":"markdown","source":"### 3.4 Train and Validation sets split\n\nCreate the training and validation sets for training the model and validating it"},{"metadata":{"_uuid":"cb361220e3699fd55a0ff154c784dc0ef0afeec6","trusted":false},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(xtrain, Y, test_size=0.20, random_state=2018)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0eb651ba57d81349ad8d8cdb16e90dd86a94af4a"},"cell_type":"markdown","source":"### 3.4 Train and Validation sets split\n\nCreate the training and validation sets for training the model and validating it"},{"metadata":{"_uuid":"e07140ae4f3ad801607e209ade9929088e5f7a2e","trusted":false},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(xtrain, Y, test_size=0.20, random_state=2018)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4014e65f037594b0f1e0e2cee9e653ab96ad9cfb"},"cell_type":"markdown","source":"## Step 4 : Create baseline model\n\nNext step is the modelling step, lets start with the simple linear model \n\n### 4.1 : Logistic Regression\n\nTrain a binary classifier logistic regression"},{"metadata":{"_uuid":"ca42032b9f321466e107f895c3aa044475f97605","trusted":false},"cell_type":"code","source":"model1 = LogisticRegression()\nmodel1.fit(X_train, y_train)\nvalp = model1.predict(X_valid)\n\ndef generate_auc(y_valid, valp, model_name):\n    auc_scr = roc_auc_score(y_valid, valp)\n    print('The AUC for ' +model_name+ ' is :', auc_scr)\n\n    fpr, tpr, thresholds = roc_curve(y_valid, valp)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure(figsize=(6,5))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'purple', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'upper left')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\nif len(distinct_Y) == 2:\n    generate_auc(y_valid, valp, model_name=\"logistic regression\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a169c8d8ecc92b80d14750596827c010555379a","trusted":false},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.figure(figsize=(6,5));\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    plt.grid(False)\n\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n        \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ncnf_matrix = confusion_matrix(y_valid, valp)\nnp.set_printoptions(precision=2)\n\nplt.figure(figsize=(8,8))\nplot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"876bf19b0607b7b62832179e9de83b1d336b528d"},"cell_type":"markdown","source":"### 4.2 : Decision Tree Classifier\n\nLets train a decision tree classifier"},{"metadata":{"_uuid":"78d35fbf03a7afcd4077dadc4850c99859fded7a","trusted":false},"cell_type":"code","source":"model2 = DecisionTreeClassifier()\nmodel2.fit(X_train, y_train)\nvalp = model2.predict(X_valid)\n\nif len(distinct_Y) == 2:\n    generate_auc(y_valid,valp, model_name=\"decision tree classifier\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcbd3ee5b504e335a1ce87c5677405adbb34736a","trusted":false},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_valid, valp)\nnp.set_printoptions(precision=2)\n\nplt.figure(figsize=(6,5));\nplot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"841e047e2e39949d67749dadd8b81dffd95feead"},"cell_type":"markdown","source":"### 4.3 : Random Forest Classifier\n\nNow, lets train a tree based model : random forest"},{"metadata":{"_uuid":"9278aa01e1146da92c60cb147df2b56a015e44da","trusted":false},"cell_type":"code","source":"model3 = RandomForestClassifier()\nmodel3.fit(X_train, y_train)\nvalp = model3.predict(X_valid)\n\nif len(distinct_Y) == 2:\n    generate_auc(y_valid,valp, model_name=\"random forest classifier\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0af0542fdba1a3df7b1356712f03883d0a8fffe","trusted":false},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_valid, valp)\nnp.set_printoptions(precision=2)\n\nplt.figure(figsize=(6,5));\nplot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e41ed55b228887855fe6d2519775471efbf58a2b"},"cell_type":"markdown","source":"### 4.4 : ExtraTrees Classifier\n\nNow, lets train another tree based model : extra trees classifier"},{"metadata":{"_uuid":"bddfd3f876fc165d01bc5f083709e28fe9eb02c6","trusted":false},"cell_type":"code","source":"model4 = ExtraTreesClassifier()\nmodel4.fit(X_train, y_train)\nvalp = model4.predict(X_valid)\n\nif len(distinct_Y) == 2:\n    generate_auc(y_valid,valp, model_name=\"extratrees classifier\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94747cd9c6fe8cdffe864c81605ab1ebed0d3177","trusted":false},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_valid, valp)\nnp.set_printoptions(precision=2)\n\nplt.figure(figsize=(6,5));\nplot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57e94bbbdc919e59bb8aa99b6a65f08d68ff41ff"},"cell_type":"markdown","source":"### 4.5 : xgBoost Classifier\n\nLets train the extereme gradient boosting : xgboost classifier"},{"metadata":{"_uuid":"4f1d3a59ae1f4b4b6239e50960d1942b6e7a4e74","trusted":false},"cell_type":"code","source":"model5 = xgb.XGBClassifier(n_estimators=300, learning_rate=0.01)\nmodel5.fit(X_train, y_train)\nvalp = model5.predict(X_valid)\n\nif len(distinct_Y) == 2:\n    generate_auc(y_valid,valp, model_name=\"xgboost\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d77d925c76ff8641f4c0b5b44620b25e5c3f7ac","trusted":false},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_valid, valp)\nnp.set_printoptions(precision=2)\n\nplt.figure(figsize=(6,5))\nplot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20234f838d7732e4e9691de6026f6e0be174186e"},"cell_type":"markdown","source":"## Step 5: Feature Importance\n\nLets look at some of the important features from the dataset"},{"metadata":{"_uuid":"d28e104cc89d1b733c92e8fb336f1a2c399acc64","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nxgb.plot_importance(model5, max_num_features=10);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17566bc622adf6562c2a06994c8b8ec7b605d136"},"cell_type":"markdown","source":"## Step 6 : Model Ensembling\n\nLets create a simple blender. Other options to extend are stacking / majority voting / rank averaging etc."},{"metadata":{"_uuid":"e2a482aa01aa717fe54eda17d1c383b7df10810f","trusted":false},"cell_type":"code","source":"models = [model1, model2, model3, model4, model5]\npreds = np.zeros(shape=(xtest.shape[0],))\nif xtest.shape[0] == 0:\n    print (\"this is a dataset kernel, no test data for predictions\")\nelse:\n    for model in models:\n        pred = model.predict(xtest)/ len(models)\n        preds += pred\n    print (preds[:100])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"988fbe17fa9af3e4222c1368c39b0412143502c6"},"cell_type":"markdown","source":"## Step 7 : Create Submission File\n\nFinally, create the submission file from the extereme graident boosting model"},{"metadata":{"_uuid":"8ec898edfae7a663522ed8354cdd00c095c2c79b","trusted":false},"cell_type":"code","source":"if xtest.shape[0] == 0:\n    print (\"This is a dataset kernel, no need to create a submission file :)\")\nelse:\n    pred = model5.predict(xtest)\n    sub = pd.DataFrame()\n    sub[_id] = test_id\n    sub[_target] = pred\n    sub.to_csv(\"baseline_submission.csv\", index=False)\n    print (\"Submission File Generated, here is the snapshot: \")\n    print (sub.head(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bb98c920d2f034d8f102c5141ba15193960de08"},"cell_type":"markdown","source":"Thanks for viewing this kernel, hopefully you can get ideas to start your own kernel."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}