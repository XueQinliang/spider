{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"1db5f616-18ba-d8b5-5766-e34fa91c8d7c"},"source":"# Predicting house prices in Ames, Iowa"},{"cell_type":"markdown","metadata":{"_cell_guid":"24382380-63c5-775d-b941-0c3ec2c2e150"},"source":"The purpose of this notebook is for me to learn something about Python programming, how to process data using Python, and how to perform machine learning tasks in Python. It may perhaps also provide some decent predictions of house prices in Ames, IA.\n\nThis data holds quite a bit of interest for me, because my wife and I are currently going through the process on both sides of the table - we've put our current house on the market, and we have a new house under contract.\n\nI'd also like to invite anyone who takes the time to read this to point out ways in which my methods or code could be better, as I'm still figuring all this out. Also, this is a work in progress, so it will have many updates.\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"a5b0e146-8769-7436-822f-49464314ad31"},"source":"## 1 Import libraries"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0aa69e99-ef1e-51f8-a62d-691f209f9f74"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport matplotlib.ticker as tick\nimport seaborn as sb\nfrom sklearn import linear_model\nfrom sklearn import model_selection\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb"},{"cell_type":"markdown","metadata":{"_cell_guid":"d17624f4-6f79-ade1-26b7-c37628638993"},"source":"# 2 Read in the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32dd30dd-4639-8be4-c55a-c069611d6f4a"},"outputs":[],"source":"train = pd.read_csv(\"../input/train.csv\", header=0)\ntest = pd.read_csv(\"../input/test.csv\", header=0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3260232b-47a8-9c70-43e5-9fdc80eb5de2"},"source":"## 3 Data cleaning"},{"cell_type":"markdown","metadata":{"_cell_guid":"42875395-c759-c055-f367-650fb90ea546"},"source":"Let's check to what extent we need to clean up the data before throwing a model at it. On a previous version I had my own definition of these, but the results ended up being terrible, so I shamelessly stole large portions from [this kernel](https://www.kaggle.com/mountaindata/house-prices-advanced-regression-techniques/house-price-regression)."},{"cell_type":"markdown","metadata":{"_cell_guid":"1d95c53e-fa99-cd08-4fe5-35df644fca92"},"source":"### 3.1 Get rid of NaNs\nIt looks like several columns have a lot of missing values. From the data dictionary provided, it seems that this should be expected. These should be interpreted as the absence of a given feature rather than a missing value. However, we're going to have to do something with them, because scikit-learn doesn't like them. We'll convert them to 0s."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fbb2268-d803-057d-26ae-33f4de3d758a"},"outputs":[],"source":"train = train.fillna(0)\ntest = test.fillna(0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9d6b6f45-a238-c431-5804-372b83893aff"},"source":"### 3.2 Separating types of features\nWe want to get lists of all the features - one grand list, and also separate lists for categorical and numerical features. The former we will convert to Pandas's category type, and the latter we will leave alone. We won't carry the Id column (which is useless except as an identifier) and the sale price (the target). The middle set I've identified as \"ordered categories\", and I'll code them myself in what I feel is a natural way."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f9d187e-8403-aa7a-7e81-c5b3557e984c"},"outputs":[],"source":"features = [x for x in train.columns if x not in ['id','SalePrice']]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0271c85-9e7f-3ef4-3fd5-4d7de30da257"},"outputs":[],"source":"cat_features = ['MSSubClass', 'MSZoning', 'Street','Alley', \n'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n'HouseStyle','OverallQual','OverallCond',\n'Foundation', 'BsmtFinType1','BsmtFinType2', \n'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType',\n'Heating','CentralAir','Electrical','BsmtFullBath',\n'BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n'TotRmsAbvGrd','Functional','Fireplaces','GarageType',\n'GarageFinish','GarageCars','PavedDrive',\n'Fence','MiscFeature','SaleType','SaleCondition']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db6f3eed-3ca3-b9c4-c5f9-4ee3f4792dfa"},"outputs":[],"source":"ord_cat_features = ['ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure',\n                   'HeatingQC','KitchenQual','FireplaceQu','GarageQual','GarageCond','PoolQC']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e265b24-3959-42fc-fc36-40a3f32b1bfc"},"outputs":[],"source":"num_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2',\n'BsmtUnfSF','TotalBsmtSF', 'LowQualFinSF','1stFlrSF','2ndFlrSF',\n'GrLivArea', 'YearRemodAdd', 'YearBuilt','GarageYrBlt','GarageArea','WoodDeckSF',\n'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','MoSold','YrSold']"},{"cell_type":"markdown","metadata":{"_cell_guid":"72ed3760-41ec-7bcd-7dbe-502fc9da513b"},"source":"### 3.3 Converting categorical values to numeric values\nWe need to get rid of all the string values and replace them with numeric values. For this we'll lean on Pandas's categorical type, which didn't exist the last time I messed with Python extensively. They're a bit like R's \"factors\", which come in handy."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82dd285f-bb9c-6c68-fd98-3d3a9560c6e5"},"outputs":[],"source":"rows_train = len(train)\nrows_test = len(test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e23429e4-4d19-82ae-dbd3-0d54865fda30"},"source":"We'll now glue the training and test set together, for all but the sale price, which the test set doesn't have in this case. Later, we'll split them back apart. We do this because the test data might have instances within a category not seen in the test data, in which case Pandas won't pick them up. For example, if there are no values of 'I' in the 'MSZoning' field in the training data but not the test data, they'll be ignored when we convert the categories to codes. Gluing the datasets together prevents this. It wouldn't help at all if you had to run the model against data you didn't have access to (which is a possibility in this case).\n\nAnother thing that annoys me is that some of these categoricals could justifiably be considered to be ordered, in which case I want the order to be preserved. Pandas won't figure this out automatically. Some are easy, like OverallQual - which is already numbered from 1-10. Nothing more needs to be done. Others have 'excellent', 'good', etc., and we'd like these to mean something as well.\n\nFor predictions - it may not matter - it's possible to just assign a number to each category, regardless of order, and move along our merry ways. However, it's important in business to be able to interpret and explain model results, and you might not get to be there to walk through your arbitrary encoding scheme with The Powers That Be(tm)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d39ea4bb-9e01-a30d-4b57-ded902143173"},"outputs":[],"source":"def transform_nb(x):\n    if x in (\"NoRidge\", \"NridgHt\", \"StoneBr\",\"Timber\",\"Veenker\"):\n        return \"Hi\"\n    elif x in (\"Mitchel\",\"OldTown\",\"BrkSide\",\"Sawyer\",\"NAmes\",\"IDOTRR\",\"MeadowV\",\"Edwards\",\"NPkVill\",\"BrDale\",\"Blueste\"):\n        return \"Low\"\n    else:\n        return \"Mid\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08a3bdd9-ad46-d59f-03c1-68142bae1701"},"outputs":[],"source":"# glue data sets together\ntrain_test = pd.concat((train[features], test[features])).reset_index(drop=True)\n# Convert categoricals to codes\nfor c in range(len(cat_features)):\n    train_test[cat_features[c]] = train_test[cat_features[c]].astype('category').cat.codes"},{"cell_type":"markdown","metadata":{"_cell_guid":"32f4f800-9ca6-2f87-7e19-553986734529"},"source":"First the coding helper function, stolen from [here](https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation/)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fcdf7b3-e7dd-857c-b166-79fc3fbdb777"},"outputs":[],"source":"#Define a generic function using Pandas replace function\ndef coding(col, codeDict):\n  colCoded = pd.Series(col, copy=True)\n  for key, value in codeDict.items():\n    colCoded.replace(key, value, inplace=True)\n  return colCoded"},{"cell_type":"markdown","metadata":{"_cell_guid":"a9db5410-79dc-2a59-5187-4b2685b81b95"},"source":"Now the coding:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5209c5bb-3e2b-e9b8-dcc1-c276df76a8a6"},"outputs":[],"source":"train_test['ExterQual'] = coding(train_test['ExterQual'], {'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1})\ntrain_test['ExterCond'] = coding(train_test['ExterCond'], {'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1})\ntrain_test['BsmtQual'] = coding(train_test['BsmtQual'], {'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1, 0:0})\ntrain_test['BsmtCond'] = coding(train_test['BsmtCond'], {'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1, 0:0})\ntrain_test['BsmtExposure'] = coding(train_test['BsmtExposure'], {'Gd':4,'Av':3,'Mn':2,'No':1, 0:0})"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cad115bb-17fb-de3b-3439-e45e1869145e"},"outputs":[],"source":"train_test['HeatingQC'] = coding(train_test['HeatingQC'], {'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1})\ntrain_test['KitchenQual'] = coding(train_test['KitchenQual'], {'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1})\ntrain_test['FireplaceQu'] = coding(train_test['FireplaceQu'], {'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1, 0:0})\ntrain_test['GarageQual'] = coding(train_test['GarageQual'], {'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1, 0:0})\ntrain_test['GarageCond'] = coding(train_test['GarageCond'], {'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1, 0:0})\ntrain_test['PoolQC'] = coding(train_test['PoolQC'], {'Ex':5, 'Gd':4,'TA':3,'Fa':2,'Po':1, 0:0})"},{"cell_type":"markdown","metadata":{"_cell_guid":"518a47c3-19ed-5daf-ba23-94ba496374c6"},"source":"### 3.4 Restore training and test sets\nI keep forgetting about iloc. It makes cutting up Pandas DataFrames work kind of like dataframes in R do. We'll put the training and test sets back in the same boxes as before."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80dcb6b2-08ab-967a-11eb-f31ec287aca9"},"outputs":[],"source":"trainX = train_test.iloc[:rows_train,:]\ntestX = train_test.iloc[rows_train:,:]"},{"cell_type":"markdown","metadata":{"_cell_guid":"f40c315f-fa48-dab6-4973-2031624dd39e"},"source":"## 4 Data exploration\nNow it's time to look at the data and see what, if anything, looks weird or notable."},{"cell_type":"markdown","metadata":{"_cell_guid":"9bf6f120-b121-a512-2824-e47919606e3d"},"source":"### 4.1 Distribution of target variable\nLet's look at the distribution of the target variable to see if we need to adjust it. Since RMSE of the logs is the standard by which we'll be measured, we'll probably have to log-transform the sale prices."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86c942d5-07b7-ebe6-7295-e90c3617763a"},"outputs":[],"source":"fig = plt.figure()\nax = plt.axes()\nn,bins,patches=plt.hist(train['SalePrice'], 30, facecolor='dimgrey')\nplt.xlabel(\"Sale price (000s)\")\nvals=ax.get_xticks()\nax.set_xticklabels(['${:,.0f}'.format(x/1000) for x in vals])\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0cdedf44-8f3d-2be7-5b1e-7b6315598084"},"outputs":[],"source":"target = np.log(train['SalePrice'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b07dd2a4-dee5-78ca-1f7a-13a5601c17bd"},"outputs":[],"source":"fig = plt.figure()\nax = plt.axes()\nn,bins,patches=plt.hist(target, 30, facecolor='dimgrey')\nplt.xlabel(\"Log(Sale price)\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"792d4184-4a46-f956-5ada-59ac6511db34"},"source":"That makes the sale prices look more normally distributed, so it might behave more amenably to models"},{"cell_type":"markdown","metadata":{"_cell_guid":"4ac3c0e1-5154-8e4b-d71d-ebf6efa200ec"},"source":"### 4.2 Exploring categorical features\nLet's take a look at the categorical features. First, a helper function to enable my laziness."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"236d6b44-1b6a-e5d0-c7ed-67908ad35623"},"outputs":[],"source":"def doPlots(x, data, ii, fun):\n    fig, axes = plt.subplots(len(ii) // 2, ncols = 2)\n    fig.tight_layout()\n    for i in range(len(ii)):\n        fun(x=x[ii[i]], data=data, ax=axes[i // 2, i % 2], color='dimgrey')\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f17a8bee-0e37-0d24-63ba-88a6f46902fe"},"outputs":[],"source":"doPlots(cat_features, train, range(0,8), sb.countplot)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"febce0b2-a086-fe2f-3f1b-2400da519cd0"},"outputs":[],"source":"doPlots(cat_features, train, range(8,16), sb.countplot)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"134646b2-e12e-7e38-646f-4fac74dd109f"},"outputs":[],"source":"doPlots(cat_features, train, range(16,24), sb.countplot)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"70460f1e-9f1a-289d-3522-5d09c2450eb3"},"outputs":[],"source":"doPlots(cat_features, train, range(24,32), sb.countplot)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74b2eac3-24fb-58d1-12ca-fddd94f3c13d"},"outputs":[],"source":"doPlots(cat_features, train, range(32,40), sb.countplot)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aea1fc39-925a-93ff-1f92-6f720bd2aaaa"},"outputs":[],"source":"doPlots(cat_features, train, range(40,44), sb.countplot)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1023636b-a48f-cd20-bb92-f1029bb087a3"},"source":"**Observations:**\n\n* Most houses have been built since 1946. That makes sense - as that corresponds with the beginning of the post-war housing boom in the Midwest.\n* Almost all are zoned for residential, are on paved streets, and do not have access to an alley.\n* Most are on regular or slightly irregular lots (although degrees of irregularity aren't defined).\n* Most are on level ground and have access to all public utilities.\n* Few were on cul-de-sacs.\n* Very few were close to arterial or feeder streets, which seems surprising.\n* Almost all are single family homes of one or two stories. \n* I assume the overall quality and condition assessments were subjective. If so, people seemed a lot more ambivalent about house condition than materials quality.\n* Most houses had either poured concrete or cinder block foundations. I feel like every cinder block or brick foundation I've ever seen has been associated with leaking or buckling. My wife observed that these are associated with older houses, and may actually be better because they shift with the earth, whereas concrete foundations just crack. Furthermore, the latter tend to be older than the former:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e878618-f372-0314-aca7-1e5da4fc96be"},"outputs":[],"source":"np.average(train['YearBuilt'][train['Foundation']=='PConc'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2ee18bc-3d81-cbf3-a792-88b8533995c2"},"outputs":[],"source":"np.average(train['YearBuilt'][train['Foundation']=='CBlock'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"faf8f7de-5b34-2a4b-0f6a-4a3f83b6b8c1"},"source":"### 4.3 Exploring numeric features\nLet's look at histograms of the numerical features to see what we can see."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6d776c5a-f612-7ab2-b227-7dc8d092388f"},"outputs":[],"source":"def doHistPlots(x, data, ii, fun):\n    fig, axes = plt.subplots(len(ii) // 2, ncols = 2)\n    fig.tight_layout()\n    for i in range(len(ii)):\n        fun(data[x[ii[i]]], color='b', ax=axes[i // 2, i % 2], kde=False)\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b7c8642-4b48-20de-7131-37450ae37212"},"outputs":[],"source":"doHistPlots(x=num_features, data=train, ii=range(0,6), fun=sb.distplot)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f66ff30-d23a-0cad-c3fd-ffb3fba94f32"},"outputs":[],"source":"doHistPlots(x=num_features, data=train, ii=range(6,12), fun=sb.distplot)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a07d2055-d899-9876-5f64-289b41de4902"},"outputs":[],"source":"doHistPlots(x=num_features, data=train, ii=range(12,18), fun=sb.distplot)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07bdd320-d227-74dd-26c8-30357acb343c"},"outputs":[],"source":"doHistPlots(x=num_features, data=train, ii=range(18,24), fun=sb.distplot)"},{"cell_type":"markdown","metadata":{"_cell_guid":"31c59d43-33d7-5856-f875-66e90bf4e2b0"},"source":"**Observations:**\n\n - There are a small number of houses with humongous (by comparison) lots. Similarly, there appears to be a house with a 6,000 square foot basement.\n - There are houses with a lot frontage / area of 0, which makes no\n   sense.\n - Other than the month of sale, ages, and sizes, none of these look very useful."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"619c1975-3420-ccb6-2236-d83f66845c61"},"outputs":[],"source":"train[train['TotalBsmtSF']>4000]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d308a728-ab85-172f-45c6-ba2ff09f7ad5"},"outputs":[],"source":"print(train['TotalBsmtSF'].iloc[1298,])\nprint(train['GrLivArea'].iloc[1298,])\nprint(train['OverallCond'].iloc[1298,])\nprint(train['OverallQual'].iloc[1298,])"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a81da0e-3a8d-4982-bd54-8321c12c1a56"},"source":"So, that's a really big house that sold for $160,000, was a 10 in quality, has a 480 square foot pool, and was a 5 in condition. And it's a football field's distance from the street. That looks weird, so we're just going to cut that one out of the training set."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42da4c70-2771-53d1-eb31-7777229d9666"},"outputs":[],"source":"trainX = trainX.drop(train.index[[1298]])"},{"cell_type":"markdown","metadata":{"_cell_guid":"003a456d-6e94-4e26-0af6-4b72209025ca"},"source":"## 5 Modeling\nNow let's throw some models at what we've got and see how we do."},{"cell_type":"markdown","metadata":{"_cell_guid":"2598356e-cf37-2c50-4a30-69c57dabc3ca"},"source":"### 5.1 Features to use\nLet's select the features we're going to use. There were a couple of columns I wanted to look at in more detail."},{"cell_type":"markdown","metadata":{"_cell_guid":"d699392c-bad1-fcda-d177-c3e0609cef6d"},"source":"**Neigborhood:** There are quite a few neighborhoods. Surely these can be reduced to a few classes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39caaf23-1a0d-2002-7626-dda3054fcbb4"},"outputs":[],"source":"plt.rcParams['figure.figsize'] = (8.75, 7.0)\nax = plt.axes()\nplot1 = sb.boxplot(data=train, x='Neighborhood', y='SalePrice')\nax.set_title(\"Price distribution by neighborhood\")\nsb.despine(offset=10, trim=True)\nplt.xticks(rotation=90)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4969157c-85bc-8863-d23d-4168d7d13105"},"source":"Only one has a median sale price clearly below $100,000. It looks like there is another class with medians between $100k and $150k, another class with medians between $150k and $200k, one for medians between $200k and $250k, and finally a class for medians above $250k."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d502bfe4-7e09-e02c-5ae0-d5f808bfc28b"},"outputs":[],"source":"def transform_nb(x):\n    if x in (\"NoRidge\", \"NridgHt\", \"StoneBr\"):\n        return 5 #Over 250\n    elif x in ('CollgCr','Veenker','Crawfor','Somerst','Timber','ClearCr'):\n        return 4 #200-250\n    elif x in ('Mitchel','NWAmes','SawyerW','Gilbert','Blmngtn','SWISU', 'Blueste'):\n        return 3 #150-200\n    elif x in ('OldTown','BrkSide','Sawyer','NAmes','IDOTRR','Edwards','BrDale', 'NPkVill'):\n        return 2 #100 - 150\n    elif x in ('MeadowV'):\n        return 1\n    else:\n        return 9 # Catch mistakes"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4902a683-ee3e-87bd-46c1-c3d1871da975"},"outputs":[],"source":"train['NbdClass'] = train['Neighborhood'].apply(transform_nb)\ntest['NbdClass'] = test['Neighborhood'].apply(transform_nb)\ntrainX['NbdClass'] = train['NbdClass']\n# This is messed up and gives a warning, I couldn't figure out how to make it go away\nz = test.loc[:,'NbdClass']\nz=np.asarray(z)\ntestX.loc[:,'NbdClass'] = z"},{"cell_type":"markdown","metadata":{"_cell_guid":"de07f433-9526-e744-a684-2b95ca2af6bb"},"source":"Let's redo the sale price plot with our new class and see how it comes out."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79384906-c3f3-bfba-be41-ee347871500d"},"outputs":[],"source":"trainX[trainX['NbdClass']==9]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3bec5d32-fe74-fe7e-a3e6-20c6790a2f27"},"outputs":[],"source":"plt.rcParams['figure.figsize'] = (8.75, 7.0)\nax = plt.axes()\nplot1 = sb.boxplot(data=train, x='NbdClass', y='SalePrice')\nax.set_title(\"Price distribution by neighborhood\")\nsb.despine(offset=10, trim=True)\nplt.xticks(rotation=90)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"24e9382d-6b7a-5233-f51c-fc78503bc683"},"source":"That looks halfway decent. Now let's make a list of features to use."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7789b1f8-9d26-57db-7a9a-7691b92bc092"},"outputs":[],"source":"use_features = ['OverallQual','YearBuilt','YearRemodAdd','MasVnrArea','GrLivArea','TotalBsmtSF','FullBath','Fireplaces',\n           'GarageYrBlt','GarageCars','NbdClass','HouseStyle','Alley','LotShape','LotConfig','Exterior1st',\n           'Exterior2nd','ExterQual','ExterCond','Foundation','BsmtExposure','BsmtFinType1','HeatingQC','CentralAir',\n           'FireplaceQu','GarageType','GarageFinish']"},{"cell_type":"markdown","metadata":{"_cell_guid":"9caf7362-8867-c06e-86e6-d53eefcf437d"},"source":"### 5.2 Lasso"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4da4f89a-915e-8107-b47c-c85d12c71dcc"},"outputs":[],"source":"# Test shrinkage parameters in cross-validation\nalphas = np.logspace(-10,-1,num=20)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4a409c4-ea45-b2ee-2a89-6e935700a74b"},"outputs":[],"source":"# Kick out that one weird house\ntrainY = train['SalePrice']\ntrainY = trainY.drop(train.index[[1298]])\ntarget = np.log(trainY)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80a2ea66-fed5-964f-96fa-2dc4e39702ff"},"outputs":[],"source":"lasso = linear_model.LassoCV(alphas=alphas, tol=0.0001, selection='random', random_state=17, max_iter=1000)\nlasso.fit(trainX[use_features], target)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ebef0392-4caf-213f-be37-b2f7c18ee96d"},"outputs":[],"source":"print(\"Best alpha is\",lasso.alpha_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a37aa993-c17c-6e2d-34ef-09e7f43cf870"},"outputs":[],"source":"lasso.pred = lasso.predict(trainX[use_features])\nprint(\"Competition training RMSE:\", np.sqrt(np.sum(lasso.pred - target)**2))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d845ca36-b29b-92d5-b4a7-938cea839bc5"},"outputs":[],"source":"# Look at what did and didn't get squeezed out\nindices = np.arange(len(trainX[use_features].columns))\nindices_nz = np.nonzero(lasso.coef_)\nindices_z = np.setdiff1d(indices, indices_nz)\nset([use_features[i] for i in indices_z])"},{"cell_type":"markdown","metadata":{"_cell_guid":"fa3b470f-5ce0-db81-cbe9-692a6b34c609"},"source":"Whoops - It didn't really reduce any from the ones I selected. Well, let's try it anyway and see how it comes out. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9a81b9b-7851-dc14-93b5-2010aa089fa6"},"outputs":[],"source":"# see most important and least important features\ntmp = [use_features[i] for i in indices_nz[0]]\ntmp = np.array(tmp)\ntmp2 = lasso.coef_[lasso.coef_ != 0]\nlasso.c = pd.DataFrame({'cols':tmp, 'coefs':tmp2})\nlasso.c = lasso.c.sort_values(by='coefs',ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d3df227-e8b1-5bd5-dd16-4e85f85731d7"},"outputs":[],"source":"lasso.c[:5]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3b71955-641c-1333-8d89-60266949a057"},"outputs":[],"source":"lasso.c[-5:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a924a1f-7487-edef-8fd1-8fb4c12fcc08"},"outputs":[],"source":"sollasso = pd.DataFrame({'Id':test['Id'], 'SalePrice':np.exp(lasso.predict(testX[use_features]))})"},{"cell_type":"markdown","metadata":{"_cell_guid":"16d2f38c-e62a-9285-46dc-4b0dc3188841"},"source":"### 5.3 Random forest\nI'm going to do 10-fold cross-validation to select the best number of trees and depth of each tree in the forest."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d6d0477-0260-8337-5618-92173d3e1f49"},"outputs":[],"source":"ntrees = np.arange(100,1001,100)\ndepths = np.arange(1,8)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76a6f641-3bdd-3647-5ecd-60832c037d14"},"outputs":[],"source":"# Warning: This takes a bit\nscores=[]\nX = trainX[use_features]\nY = target\nfor n in ntrees:\n    for d in depths:\n        run_tot = 0\n        for k in np.arange(0,10):\n            \n            # Get the kth fold of data\n            testXcv = X[k*146:k*146+146]\n            trainX_left = X[:k*146]\n            trainX_right = X[k*146+146:]\n            trainXcv = pd.concat([trainX_left, trainX_right])\n            \n            testYcv = Y[k*146:k*146+146]\n            trainY_left = Y[:k*146]\n            trainY_right = Y[k*146+146:]\n            trainYcv = np.concatenate([trainY_left, trainY_right])\n            \n            # Fit a model and make predictions using the kth fold\n            rf = RandomForestRegressor(n_estimators=n, max_depth=d, n_jobs=-1)\n            rf.fit(trainXcv, trainYcv)\n            preds = rf.predict(testXcv)\n            \n            # Add this fold's score to the previous ones\n            run_tot = run_tot + np.sqrt(np.sum((preds - testYcv)**2))\n            \n        # Now we load the scores table with the parameters we're testing and the 10-fold average score\n        scores.append({'ntrees':n, 'depth':d, 'score':run_tot/10.0})\n        # Tell me where we're at - Comment this out if you find it annoying\n    print(\"ntrees =\",n)\nscoredf = pd.DataFrame(scores)\nprint(\"Done!\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"54831391-f43a-0909-f1c7-695bd0104836"},"source":"Now, we'll plot our results. Number of trees will be along the x-axis, score along the y-axis, and the line color will be tied to the tree depth."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b35574d-110e-17a0-d874-9905c86f4d53"},"outputs":[],"source":"fig=plt.figure()\nax = plt.axes()\nfor d in depths:\n    x = scoredf['ntrees'][scoredf['depth']==d]\n    y = scoredf['score'][scoredf['depth']==d]\n    plt.plot(x,y, label='Depth ' + str(d))\nplt.legend(loc=9, ncol=len(depths)//2) # upper center\nplt.xlabel(\"Number of trees\")\nplt.ylabel(\"Competition RMSE\")\nax.set_title(\"Random forest - Competition RMSE by number of trees and depth\")\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fa3723c-7b15-98b7-97e0-4d4fd3830928"},"outputs":[],"source":"rf_best = RandomForestRegressor(n_jobs=-1, n_estimators=1000, max_depth=7, random_state=17)\nrf_best.fit(trainX[use_features], target)"},{"cell_type":"markdown","metadata":{"_cell_guid":"45281c33-de4f-03bb-998a-a5b8f3315733"},"source":"One of the cool things about random forests is that you can get an assessment of which features contributed the most to the predictions. This is a good check of common sense. In this version they add up to 100%."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29d266d0-ede5-4998-ecd6-f033ce83d998"},"outputs":[],"source":"importances = pd.DataFrame({'Feature':trainX[use_features].columns, 'Importance':rf_best.feature_importances_})\nimportances = importances.sort_values('Importance',ascending=False).set_index('Feature')\nimportances[0:10].iloc[::-1].plot(kind='barh',legend=False)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"11c0cada-1724-a64c-2302-2d88ecbf4032"},"source":"So the model at least makes some logical sense. Let's see how it did:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf302a52-5342-b109-898d-f886f619aca3"},"outputs":[],"source":"rf.preds = np.exp(rf_best.predict(testX[use_features]))\nsolrf = pd.DataFrame({'Id':test['Id'], 'SalePrice':rf.preds})\nsolrf.to_csv(\"./solrf2.csv\", index=False)\nsolrf.head(3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c838018b-5de1-5421-1c3d-0aa3c1cc80ce"},"source":"### 5.4 Random forest - full dataset\nWe'll try it with everything but the kitchen sink."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36a1a05c-17d7-2ef7-1d6e-85c85bcdec69"},"outputs":[],"source":"rf_all = RandomForestRegressor(n_jobs=-1, n_estimators=1000, max_depth=7, random_state=17)\nrf_all.fit(trainX, target)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2965c87c-8c17-4f9a-414c-132927747afc"},"outputs":[],"source":"importances = pd.DataFrame({'Feature':trainX.columns, 'Importance':rf_all.feature_importances_})\nimportances = importances.sort_values('Importance',ascending=False).set_index('Feature')\nimportances[0:10].iloc[::-1].plot(kind='barh',legend=False)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dcf20722-aa4d-fd74-0639-33ef3ede6540"},"outputs":[],"source":"rf.preds = np.exp(rf_all.predict(testX))\nsolrf = pd.DataFrame({'Id':test['Id'], 'SalePrice':rf.preds})\nsolrf.to_csv(\"./solrf-full.csv\", index=False)\nsolrf.head(3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"09d0acef-4a25-2695-12a6-2a3395f37479"},"source":"**Results:** The two random forest models scored at 0.15453 and 0.15129 - neither of which beat the \"lasso\" model. I put the lasso in quotes because it didn't actually squelch out any of the columns, but hey - if it's stupid but it works, it's not stupid."},{"cell_type":"markdown","metadata":{"_cell_guid":"984cc9ed-6561-cbbf-5dfe-c080b2ad7650"},"source":"### 5.5 XGBoost\nThis is the one the competition description said to try, so I'm trying it."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd259445-bf01-7c77-1ab5-86ce6cfa2903"},"outputs":[],"source":"train_X = trainX.iloc[:,1:].as_matrix()\ntest_X = testX.iloc[:,1:].as_matrix()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b70688b5-279a-98fc-0309-73fe5c12432c"},"outputs":[],"source":"gbm = xgb.XGBRegressor(max_depth=3, n_estimators=300, learning_rate=0.05).fit(train_X, target)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"154e223a-7d82-50aa-7477-1b06d8224162"},"outputs":[],"source":"importances = pd.DataFrame({'Feature':trainX.iloc[:,1:].columns, 'Importance':gbm.feature_importances_})\nimportances = importances.sort_values('Importance',ascending=False).set_index('Feature')\nimportances[0:10].iloc[::-1].plot(kind='barh',legend=False)\nplt.title(\"XGBoost - Feature importance\")\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c2f97cf-3ffc-74c0-ab86-c8c809a291bb"},"outputs":[],"source":"xgb_preds = np.exp(gbm.predict(test_X))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6b79cee-d9a4-82e9-5185-a18b6e46e772"},"outputs":[],"source":"solxgb = pd.DataFrame({'Id':test['Id'], 'SalePrice':xgb_preds})\nsolxgb.to_csv(\"./solxgb.csv\", index=False)\nsolxgb.head(3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"77af90f0-2fd5-59ec-94f2-7d8a0d76489a"},"source":"**Results:** This very vanilla model gave a score of 0.13175, good for 1,512th place as of this writing. I'm impressed. This is with no parameter tweaking at all, and very little feature engineering."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}