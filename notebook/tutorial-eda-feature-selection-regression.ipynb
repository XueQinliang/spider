{"cells":[{"metadata":{"_uuid":"5d3ad82b52b08056d4c9a08ef91f173a688f956d"},"cell_type":"markdown","source":"In this tutorial we are going to predict number of points, based on input features. I will try to show whole pipeline in solving this type of problems.\n\n<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\nMachine learning problem typically are divided into two groups.\nLet's define our problem in simple math terms. \n\n* x will be our input\n* y will be vale of  <mi>f(x)</mi> - in our case points.\n\n<center>\n  <mi>f</mi>\n  <mo>:</mo>\n  <mi>x</mi>\n  <mo stretchy=\"false\">&#x2192;<!-- ¡ú --></mo>\n  <mi>y</mi>\n</math></center>\n\n1. **Regression** - If y is real number or continuous\n * Classification predictions can be evaluated using accuracy, whereas regression predictions cannot.\n2. **Classification** - If y is discrete or categorical variable\n * Regression predictions can be evaluated using root mean squared error, whereas classification predictions cannot.\n\nWe can easily figure out that our problem is Regression problem, because we want to predict number of points, which is continuose variable. \n\nWe have few ways to solve this problem. Actually most common ways are: \n\n1. **Neural networks**\n2. Bagging and Boosting decision trees - **Random forest**\n\n> * Bagging (Bootstrap Aggregation) is used when our goal is to reduce the variance of a decision tree. Here idea is to create several subsets of data from training sample chosen randomly with replacement. Now, each collection of subset data is used to train their decision trees. As a result, we end up with an ensemble of different models. Average of all the predictions from different trees are used which is more robust than a single decision tree.\n> * Random Forest is an extension over bagging. It takes one extra step where in addition to taking the random subset of data, it also takes the random selection of features rather than using all features to grow trees. When you have many random trees. It¡¯s called Random Forest\n\n> **Reference**: https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9\n\nSo, what we have to do first? \n\nIn this kernel I will do it in that pattern, which is commonly used for solving this type of problems: \n1. Data analaysis\n2. Data visualisation\n3. Feature selection\n4. Model training\n5. Feature extraction with PCA \n\nThere is great visualisation of typical pipeline:\n![logo](https://cdn-images-1.medium.com/max/2000/1*2T5rbjOBGVFdSvtlhCqlNg.png)\n        A standard machine learning pipeline (source: Practical Machine Learning with Python, Apress/Springer)"},{"metadata":{"_uuid":"b2e4e4ea41c7d09fa89390fb0bd3219279215a0b"},"cell_type":"markdown","source":"# 1. Data analaysis\nBefore we go into more complicated work, first we have to explore our dataset.\n\nLet's have a quick look at our features."},{"metadata":{"trusted":true,"_uuid":"b6ed743cf4f1f1ef0ec6de70745bb7c12eeabd1f","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.metrics import mean_squared_error\n\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb7b8e9513753570f29380ab115fabb7f4d07b9e","collapsed":true},"cell_type":"code","source":"data=pd.read_csv('../input/winemag-data-130k-v2.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c16db0aaa86992a015993a389bd1ceff76146e56"},"cell_type":"markdown","source":"We see that there is a column called 'Unnamed: 0', which contains IDs of each wine. IDs are of course can't help us in order to regression, so we should drop this column.  We will also drop description column, because in this kernel we will not play with NLP."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"93d896cb4858a75809ca875e543518072cd503aa"},"cell_type":"code","source":"data=data.drop(columns=['Unnamed: 0', 'description'])\ndata=data.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28d62b944130b0c82f6526f50444ee568337df92"},"cell_type":"markdown","source":"Now, we want to explore our features in more statistic way.\nWe will use describe method from pandas. \nIt will return us information about:\n* mean\n* standard deviation\n* minimum value\n* maximum value\n* 25%,50%,75% quantille"},{"metadata":{"trusted":true,"_uuid":"736a38c79a85a023200e2e490b75207c7afc0105","collapsed":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"341daddfad1dd68b1b35c191ee866e9367f85597"},"cell_type":"markdown","source":"As we can see only price is continous variable in our input. As we can see on minimum and maximium value there is really high \ndiverse in price feature. There is wine which cost 3300 dollars , but we can see that 75 percent of wines are cheaper than 42 dollars."},{"metadata":{"_uuid":"44698c6641906f60673374ace791b600ce848e87"},"cell_type":"markdown","source":"### **Duplicates.**\nFirst of all let's explore our data. On first look into data we can see that there are many duplicates, which we have to drop.\n\nLet's see how many duplicates are in the data."},{"metadata":{"trusted":true,"_uuid":"9b23d4bd3595f82efe7a91449fbdf6843ef54505","collapsed":true},"cell_type":"code","source":"print(\"Total number of examples: \", data.shape[0])\nprint(\"Number of examples with the same title and description: \", data[data.duplicated(['description','title'])].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"473a04acf4d4802954d9b0484d2be0993e0b6b08"},"cell_type":"markdown","source":"We can see that there are almost 10k records with the same title and description. We should drop rows columns in order to get proper result."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6a7cca387e5e1507a4c9d234b95691e29da23e67"},"cell_type":"code","source":"data=data.drop_duplicates(['description','title'])\ndata=data.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c1d55436ea3ccf6de917739153e27cdd1ca7bb5"},"cell_type":"markdown","source":"### Missing values.\nNow, we will investigate our dataset in order to see how many missing values there is. "},{"metadata":{"trusted":true,"_uuid":"974db9d428321503f637ef8096a63bd2f90d12a1","collapsed":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f039adbb7ccb3d675c8ca3997572aac0c70ffa6d"},"cell_type":"markdown","source":"We see that there is huge number of missing values. Let's see how many percent."},{"metadata":{"trusted":true,"_uuid":"b208130dace7954b8207474b3126ea825da88870","collapsed":true},"cell_type":"code","source":"total = data.isnull().sum().sort_values(ascending = False)\npercent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\nmissing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a9560c52f01c60cbf64b6bff6a50b9c22694b01"},"cell_type":"markdown","source":"The most missing values are in region, destination, tester name and price columns.\n\nI'm worried the most about wines with NaN in price columns. We don't want to predict points for wines which price are undeclared. We will drop rows with NaN value in this column.\n\nUsefulness of other columns will be investigate on the **Feature extraction ** stage. Maybe that NaN values are meaningful for particular columns.."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ef784a5d1f18a9d4f868021956c6bccb19595974"},"cell_type":"code","source":"data=data.dropna(subset=['price'])\ndata=data.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"515180fb081df71de277ae99ef4379a80acf4248"},"cell_type":"markdown","source":"Let's take a quick look also on highest priced wines. "},{"metadata":{"trusted":true,"_uuid":"3421ecb46601e47a69abc94ad37716577ee96462","collapsed":true},"cell_type":"code","source":"data[(data['price'] > 2200)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c568b87a0c6c2a28d256052ae8735e8a9a604aaf"},"cell_type":"markdown","source":"All of 3 highest priced wines are comes from France. "},{"metadata":{"_uuid":"8bd55491dc750ac5440f239597121c0e864a6c76"},"cell_type":"markdown","source":"# Data visualization\nRemeber that in this stage our goal is not only to explore our data in order to get better predictions. We also want to get better understanding what is in data and explore data in 'normal' way. This kind of approch can be useful if we have to do some feature engineering, where good data understanding can really help to produce better features. \n\nThe most common ways to visualize data are:\n* histograms\n* box plots\n* swarm plots\n* joint plot\n* heatmaps\n\nData can be visualized by **matplotlib, seaborn library** and **built in methods from pandas dataframes**"},{"metadata":{"trusted":true,"_uuid":"65df5e2c4cd7794f8c315dacb0721a0363d68ba5","collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef pastel_plot(data, x, y):\n    plt.figure(figsize = (15,6))\n    plt.title('Points histogram - whole dataset')\n    sns.set_color_codes(\"pastel\")\n    sns.barplot(x = x, y=y, data=df)\n    locs, labels = plt.xticks()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"254c217580a97f244f8c879eb2d51e6852124f0c","collapsed":true},"cell_type":"code","source":"temp = data[\"points\"].value_counts()\ndf = pd.DataFrame({'points': temp.index,\n                   'number_of_wines': temp.values\n                  })\n\npastel_plot(df,'points', 'number_of_wines')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a46f8b7a31ed413d23f016618c05718bba5b498b"},"cell_type":"markdown","source":"We can see that all wines have number of points above 80. And points has normal distribution. The most wines have 88 points.\n\nWe can also get exact distribution not only the histogram. We will show it on price column"},{"metadata":{"trusted":true,"_uuid":"dd5ccd85b16894c0736fb4257e18c6f02bb8c016","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.title(\"Distribution of price\")\nax = sns.distplot(data[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b26b135bc056d6a1eaae316e446984b688d1cd03"},"cell_type":"markdown","source":"We see that if we want to see better price distribution we have to scale our price or drop the tail. \nWe will drop the tail, so the values that are above 200 dollars. We are also want to calculate how many wines are more expensive then 200 dolars. "},{"metadata":{"trusted":true,"_uuid":"d48289f4c9a004014569b370531da899e7ff2c4b","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.title(\"Distribution of price\")\nax = sns.distplot(data[data[\"price\"]<200]['price'])\n\npercent=data[data['price']>200].shape[0]/data.shape[0]*100\nprint(\"There are :\", percent, \"% wines more expensive then 200 USD\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb7fab8637c3fafb714bc3fa0b81bd8a403a7013"},"cell_type":"markdown","source":"As we can see we dropped only 0.59 percent of wines and now we can see that price distribution is also normal. "},{"metadata":{"_uuid":"430c9777c369059492411a5fda558a5a11109705"},"cell_type":"markdown","source":"Let's investigate which country have most expensive and most high rated wines. First of all we will sort it by price and then plot."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f5bf1ec10339f850d2ab4bb502b7414fd4cae63f","collapsed":true},"cell_type":"code","source":"z=data.groupby(['country'])['price','points'].mean().reset_index().sort_values('price',ascending=False)\nz[['country','price']].head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8745317e388ff90f51754d8b42e4a5c15c0c551","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (14,6))\nplt.title('Wine prices in diffrent countries')\nsns.barplot(x = 'country', y=\"price\", data=z.head(10))\nlocs, labels = plt.xticks()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7de318972a8f54cc5d972b18a7711042ab6c4e0","collapsed":true},"cell_type":"code","source":"z=z.sort_values('points', ascending=False)\nz[['country','points']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfebdd6310baca3ac3d3696477d88b7467b9c20f","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (14,6))\nplt.title('Points for wines in diffrent countries')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'country', y=\"points\", data=z.head(5))\nlocs, labels = plt.xticks()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7be8809c3987ce7de2cb33dc9a72f7d619229637"},"cell_type":"markdown","source":"We can easily note, that the wines in Switzerland are the most expensive one. I think the most impactful factor is much higher prices for all goods in this country. \nThe highest mean of points came to England \nBased on our data let's try make some guesses why England wines are the best.\n* Most sommeliers come from England\n* England provide information only for thier best wines\n* They are simply the best :)\n\nWe can partly check our second guess. Let's see how many wines are in dataset from particular country."},{"metadata":{"trusted":true,"_uuid":"60ed8c6215db48d87619bf7be0b8a54d949f763e","collapsed":true},"cell_type":"code","source":"country=data['country'].value_counts()\ncountry.head(10).plot.bar()\ncountry.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2629b9e9ab3551c1a7d3a44a71965e71a2f63fad"},"cell_type":"markdown","source":"We can see that England isn't even in first 20's, so our guess make more sense. ;)"},{"metadata":{"_uuid":"7e32e267e23008e2b50f2d3974a9b0ef35f253be"},"cell_type":"markdown","source":"To solve our 'problem' Important thing to investegate will be also price/quality factor."},{"metadata":{"trusted":true,"_uuid":"1ac778da249f4bcce69654f87f94bb15a6570fc9","collapsed":true},"cell_type":"code","source":"z['quality/price']=z['points']/z['price']\nz.sort_values('quality/price', ascending=False)[['country','quality/price']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ade3a35b5ad1d3479ef00bcafba01100a521896"},"cell_type":"markdown","source":"What can we see now? England was first on points ranking, but on points/quality ranking they are the second from the end. \nSo, yeah, they provided information only for let's say 'premium' wines.  "},{"metadata":{"_uuid":"b5352b16998e889975c9a60a97629ddb68f4ac15"},"cell_type":"markdown","source":"We can also can explore data with box plots. There is a nice visualisation what box plot can tell us. \n\n![logo](https://www.wellbeingatschool.org.nz/sites/default/files/W@S_boxplot-labels.png)\n**Resource**: https://www.wellbeingatschool.org.nz/sites/default/files/W@S_boxplot-labels.png"},{"metadata":{"trusted":true,"_uuid":"ef7d3b0bc030eab55e12f835641636740fb2a363","collapsed":true},"cell_type":"code","source":"df1= data[data.variety.isin(data.variety.value_counts().head(6).index)]\n\nplt.figure(figsize = (14,6))\nsns.boxplot(\n    x = 'variety',\n    y = 'points',\n    data = df1\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4701b13818994b99712d75a7e218323bdce1b01"},"cell_type":"markdown","source":"What we can read from this plot? For example, that the Red Blend has low points variance. If the boxes are taller then the variance is higher. If box is higher then other box, then it have more high values then other. \n\n**If you want to more info visit: ** https://www.wellbeingatschool.org.nz/information-sheet/understanding-and-interpreting-box-plots"},{"metadata":{"_uuid":"25b066a06c71c242dd914a65aaa9dcbedaa7162b"},"cell_type":"markdown","source":"# 3. Feature selection\nOn this stage we want to make our dataset smaller without loosing acuracy of model. \nSo how can we do it? \nWe can make correlation plot and drop columns which correlation will be close to 1 or -1. \nWhat is correlation?\n\n> Correlation is a statistical measurement of the relationship between two variables. Possible correlations range from +1 to ¨C1. A zero correlation indicates that there is no relationship between the variables. A correlation of ¨C1 indicates a perfect negative correlation, meaning that as one variable goes up, the other goes down. A correlation of +1 indicates a perfect positive correlation, meaning that both variables move in the same direction together.\n\nReference: https://www.verywellmind.com/what-is-correlation-2794986\n\nThere are also some 'automatic' algorithm to do feature selection:\n\n1. Feature selection with correlation - find out which features are correleted and then drop \nall except one.\n\n2. Univariate feature selection - Univariate feature selection works by selecting the best features based on univariate statistical tests.\n\n3. Recursive feature elimination - Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features.\n\n4. Tree based feature selection - Tree-based estimators can be used to compute feature importances, which in turn can be used to discard irrelevant features (when coupled with the sklearn.feature_selection.SelectFromModel meta-transformer):\n\n**Reference**: http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n\nWe will use Univariate feature selection based on feature importances from CatboostRegressor. "},{"metadata":{"_uuid":"ee6cd661e5e9ba961a48b98a5bf224a82ff87a42"},"cell_type":"markdown","source":"# Feature importance with Catboost.\nFirst we will prepare our train and test data. We will use sklearn Library. "},{"metadata":{"trusted":true,"_uuid":"df40739e92c674ee95da02dcef8bd82bba0abc6e","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom catboost import Pool, CatBoostRegressor, cv\n\nX=data.drop(columns=['points'])\n\nX=X.fillna(-1)\nprint(X.columns)\ncategorical_features_indices =[0,1, 3,4,5,6,7,8,9,10]\ny=data['points']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    random_state=42)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, \n                                                    random_state=52)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aabe16c26c4a779557e3d82932a408500db0d735","collapsed":true},"cell_type":"code","source":"categorical_features_indices","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20a8f4d686fe5d5e0a55e9acc4a1424971c4b32a"},"cell_type":"markdown","source":"Create CatBoostRegressor model with Mean squared error loss function."},{"metadata":{"trusted":true,"_uuid":"6f796adb0a8b583d98d09b6b7f59fed9bf9917b1","collapsed":true},"cell_type":"code","source":"def perform_model(X_train, y_train,X_valid, y_valid,X_test, y_test):\n    model = CatBoostRegressor(\n        random_seed = 400,\n        loss_function = 'RMSE',\n        iterations=400,\n    )\n    \n    model.fit(\n        X_train, y_train,\n        cat_features = categorical_features_indices,\n        eval_set=(X_valid, y_valid),\n        verbose=False\n    )\n    \n    print(\"RMSE on training data: \"+ model.score(X_train, y_train).astype(str))\n    print(\"RMSE on test data: \"+ model.score(X_test, y_test).astype(str))\n    \n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a01896ff9bc4fc543605c10a129558fc4e44d822"},"cell_type":"markdown","source":"Let's run our model and check score."},{"metadata":{"trusted":true,"_uuid":"b39343072cf2b94d176d37c785f56fba5367c1bb","scrolled":true,"collapsed":true},"cell_type":"code","source":"model=perform_model(X_train, y_train,X_valid, y_valid,X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1f4b1dc50934c87a2ccc1166a160ab84e41af7d"},"cell_type":"markdown","source":"Now, we are ready to create feature importance plot. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"933b3b8727cada2eeb12ef026daa4b5c4f798fd0"},"cell_type":"code","source":"feature_score = pd.DataFrame(list(zip(X.dtypes.index, model.get_feature_importance(Pool(X, label=y, cat_features=categorical_features_indices)))),\n                columns=['Feature','Score'])\n\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e7a040d3adf8238d64eb292237ed83d4a2cd317","collapsed":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (12,7)\nax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\nax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\nax.set_xlabel('')\n\nrects = ax.patches\n\nlabels = feature_score['Score'].round(2)\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 0.35, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8112cd10645b24452ec6e63ce5254bab0089071"},"cell_type":"markdown","source":"Let's try to drop 3 columns which gives least information."},{"metadata":{"trusted":true,"_uuid":"9cc6505ab5eb12d27cde916d36110a2e14de7d20","collapsed":true},"cell_type":"code","source":"X=data.drop(columns=['points','title', 'region_1'])\nX=X.fillna(-1)\n\nprint(X.columns)\ncategorical_features_indices =[0,1,3,4,5,6,7,8]\ny=data['points']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    random_state=42)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, \n                                                    random_state=52)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aea73f4df1f3a439035063ebc329432daefc6bf4"},"cell_type":"markdown","source":"And now perform the model once again. "},{"metadata":{"trusted":true,"_uuid":"57e959413ab80e8b8536eb40355da6b90f67b76b","collapsed":true},"cell_type":"code","source":"model=perform_model(X_train, y_train,X_valid, y_valid,X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c57d0a319f3cbcc4c466d8259a58693cd4532502"},"cell_type":"markdown","source":"As we can see our model perform only a little worse, but we save some computing time and RAM usability. Feature selecion technique is much more useful with larger dataset, where a lot of columns are useless."},{"metadata":{"_uuid":"cada8ca4bfed55ff1d1b5bd0d06117f8afa0a330"},"cell_type":"markdown","source":"As we can see the most important feature is price. Tester has also big impact for the points score."},{"metadata":{"_uuid":"44e52cb7a2c0f5c0dafac78fb9ba6dccea380656"},"cell_type":"markdown","source":"What is next step? You can play with tunning model. Good idea will be also testing XgBoost or neural netoworks approch. If you want to maximize the score you should also read about model stacking and genetic programming. If you want to know how to do NLP on the description, you should see my other kernel. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cf33a6011a4a22f04cc2e9a084e37a635137cc15"},"cell_type":"markdown","source":"# ** If you are intrested in NLP, please check my other kernel on the same data https://www.kaggle.com/mistrzuniu1/catboost-points-predictions-with-simple-nlp/**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0ff19cef650231f7bf76b724d288590ba9e53427"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}