{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"361275c7-da9d-2ce0-ff4f-bb6cbb8130b5"},"source":"## Predicting the Helpfulness of Amazon Fine Food Reviews"},{"cell_type":"markdown","metadata":{"_cell_guid":"953efcc7-c5ce-3a2a-0a1d-ab72f32a4fec"},"source":"### Purpose\nBuild a model to predict the helpfulness of Amazon Fine Food Reviews. This will improve Amazon's selection of helpful reviews at the top of the review section and improve customer's purchasing decisions. It could also help other reviewers as a guide to writing helpful reviews.\n\nThis dataset comes from over 568,0454 Amazon Fine Food Reviews. "},{"cell_type":"markdown","metadata":{"_cell_guid":"36cbb209-3bef-864a-d0a0-1598b5161921"},"source":"Variable: Description | Type of Variable\n\nHelpfulnessNumerator: number of users who found the review helpful | continuous\n\nHelpfulnessDenominator: number of users who indicated whether they found the review helpful or not helpful | continuous\n\nScore: rating between 1 and 5 | categorical\n\nText: text of the review | text"},{"cell_type":"markdown","metadata":{"_cell_guid":"bca8bd20-bb38-be10-6afd-708d4bae5a7f"},"source":"## Load the Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4bad00e-affa-5f32-4d26-7aaa58519369"},"outputs":[],"source":"#imports\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c30e2a97-2f73-04ee-cd0b-009f3af9ab73"},"outputs":[],"source":"# read data into a DataFrame\ndata = pd.read_csv(\"../input/Reviews.csv\")\n\n#make a copy of columns I need from raw data\ndf1 = data.iloc[:, [4,5,6,9]]\ndf1.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"382082a3-13be-9193-5a82-d2634b07240e"},"outputs":[],"source":"#change data type of non-Text features from string to integer\ndf1.iloc[:, 1:3] = df1.iloc[:, 1:3].apply(pd.to_numeric)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c33f28ec-5cf7-94a5-30c8-2c9121915c06"},"outputs":[],"source":"#include reviews that have more than 10 helpfulness data point only\ndf1 = df1[(df1.HelpfulnessDenominator > 10)]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0114e255-abba-7649-87c1-f0a20486a32e"},"outputs":[],"source":"df1['Score'].shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"833004d2-7d3c-a472-a390-a5593ff3c1a6"},"source":"## Notes\nI have only included reviews that have more than 10 votes from users on whether the review was helpful or not. With this filter, the dataset is significantly reduce from 560,000+ reviews to 21,463 reviews."},{"cell_type":"markdown","metadata":{"_cell_guid":"60bbfbc4-2831-9a26-33b5-fd7d9f64fa06"},"source":"# Clean the Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce9ce1c5-033c-cee0-80a8-e77a7f2bd532"},"outputs":[],"source":"#check for missing values\ndf1.isnull().sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a24ac696-221c-7651-6257-a61c362b4ed5"},"outputs":[],"source":"# convert text to lowercase\ndf1.loc[:, 'Text'] = df1['Text'].str.lower()\ndf1[\"Text\"].head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51e3e58b-8e24-ad43-97fc-ced1f60a1b57"},"outputs":[],"source":"#remove html tags\n#import bleach\n#df1[\"Text\"] = df1['Text'].apply(lambda x: bleach.clean(x, tags=[], strip=True))\n#df1[\"Text\"].head(4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9db67830-1522-e487-019f-07b7561278cb"},"outputs":[],"source":"#remove punctuation\nimport unicodedata\nimport sys\n\ntbl = dict.fromkeys(i for i in range(sys.maxunicode)\n                      if unicodedata.category(chr(i)).startswith('P'))\ndef remove_punctuation(text):\n    return text.translate(tbl)\n\ndf1['Text']=df1['Text'].apply( lambda x: remove_punctuation(x))\ndf1[\"Text\"].head(4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be8e207d-cf2c-9696-b674-073d9c327126"},"outputs":[],"source":"df1['Score'].shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"e22cddae-1a69-908e-ea25-d43eb66c2560"},"source":"#### Notes\nI chose not to use the Porter Stemmer method after reviewing other kernels on Kaggle where the method generated less accurate predictions."},{"cell_type":"markdown","metadata":{"_cell_guid":"d5fd5461-ee21-f4b0-e6e5-ae7366383067"},"source":"## Exploratory Data Analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45605253-c0a8-f172-a398-501ec5d57113"},"outputs":[],"source":"#transform Helpfulness into a binary variable with 0.50 ratio\ndf1.loc[:, 'Helpful'] = np.where(df1.loc[:, 'HelpfulnessNumerator'] / df1.loc[:, 'HelpfulnessDenominator'] > 0.50, 1, 0)\ndf1.head(3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6f66c98-2186-0de7-f7f4-fd0d8c101281"},"outputs":[],"source":"df1.groupby('Helpful').count()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0c079a4-61d8-2768-27e9-c7d37d1888bf"},"outputs":[],"source":"df1.corr()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1cb38ca4-fbdc-5713-fc7e-a853225fec96"},"source":"### (Bag of Words model)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61608f06-bc19-8190-a29e-3488aed98ebc"},"outputs":[],"source":"#make a copy\ndf2 = df1.copy(deep = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d026b5f-675b-bee2-c335-4b3ed936e295"},"outputs":[],"source":"#tokenize text with Tfidf\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(min_df = 0.1, max_df=0.9,\n                             ngram_range=(1, 4), \n                             stop_words='english')\nvectorizer.fit(df2['Text'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"844b94f2-38f1-0ea0-85c4-cc2901a600ee"},"outputs":[],"source":"X_train = vectorizer.transform(df2['Text'])\nvocab = vectorizer.get_feature_names()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6dba1aac-1c34-3433-6e09-853634f7d928"},"outputs":[],"source":"#find best logistic regression parameters\nfrom sklearn import grid_search, cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfeature_set = X_train\ngs = grid_search.GridSearchCV(\n    estimator=LogisticRegression(),\n    param_grid={'C': [10**-i for i in range(-5, 5)], 'class_weight': [None, 'balanced']},\n    cv=cross_validation.StratifiedKFold(df1.Helpful,n_folds=10),\n    scoring='roc_auc'\n)\n\n\ngs.fit(X_train, df2.Helpful)\ngs.grid_scores_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1edf4443-68df-f86f-02d2-b8b5a099671d"},"outputs":[],"source":"#plot ROC/AUC curve\nfrom sklearn.metrics import roc_auc_score, roc_curve\nactuals = gs.predict(feature_set) \nprobas = gs.predict_proba(feature_set)\nplt.plot(roc_curve(df2[['Helpful']], probas[:,1])[0], roc_curve(df2[['Helpful']], probas[:,1])[1])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"843cbcc5-bcb0-369c-059d-2c03ea613fda"},"outputs":[],"source":"# ROC/AUC score\ny_score = probas\ntest2 = np.array(list(df2.Helpful))\ntest2 = test2.reshape(21463,1)\ny_true = test2\nroc_auc_score(y_true, y_score[:,1].T)"},{"cell_type":"markdown","metadata":{"_cell_guid":"850f03df-2df1-6143-4c43-4711ee23c71e"},"source":"#### Notes\nThe Bag of Words model performs poorly with only 72% accuracy."},{"cell_type":"markdown","metadata":{"_cell_guid":"0073e3cd-6ccb-e38d-3b5b-cd80fd73401f"},"source":"## Improving Prediction with K-Means Clustering of Reviews\nHypothesis: There's a natural clustering to review vocabulary. I can use the most descriptive clusters to simplify the model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f41ef3ab-5326-f34c-2ac1-fe70c7dbb3a4"},"outputs":[],"source":"#Apply TfidfVectorizer to review text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"767adc00-48a7-e9b3-a570-b4817d1a3c97"},"outputs":[],"source":"model = KMeans(n_clusters=4, init='k-means++', max_iter=100, n_init=1,random_state=5)\n\nvectorizer = TfidfVectorizer(min_df = 0.05, max_df=0.95,\n                             ngram_range=(1, 2), \n                             stop_words='english')\nvectorizer.fit(df1['Text'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"b5f9d609-aa6e-9754-2215-e49a469d73f7"},"source":"### Select Top 10 words per cluster"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fad223cf-02dd-b6f9-5465-c931293d09b0"},"outputs":[],"source":"X_train = vectorizer.transform(df1['Text'])\nvocab = vectorizer.get_feature_names()\nsse_err = []\nres = model.fit(X_train)\nvocab = np.array(vocab)\ncluster_centers = np.array(res.cluster_centers_)\nsorted_vals = [res.cluster_centers_[i].argsort() for i in range(0,np.shape(res.cluster_centers_)[0])]\nwords=set()\nfor i in range(len(res.cluster_centers_)):\n    words = words.union(set(vocab[sorted_vals[i][-10:]]))\nwords=list(words)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0244066-2db9-2955-f12f-8063faf5f9f7"},"outputs":[],"source":"#top 10 words for each cluster\nwords"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c84fe455-2f62-16da-7dc6-adeb5caac9a1"},"outputs":[],"source":"#add top words to train set\ntrain_set=X_train[:,[np.argwhere(vocab==i)[0][0] for i in words]]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca6771db-01d8-e494-6ee5-b614281f799b"},"outputs":[],"source":"# how many observations are in each cluster\ndf1['cluster'] = model.labels_\ndf1.groupby('cluster').count()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf618b80-1bde-a2e8-7a2a-c601b056a9f7"},"outputs":[],"source":"# what does each cluster look like\ndf1.groupby('cluster').mean()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12aa1cee-2b7b-6b12-abf7-11e552f09221"},"outputs":[],"source":"# correlation matrix\ndf1.corr()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6976443a-b746-e87f-45ab-c068231b68d5"},"source":"#### Notes\nThere doesn't seem to be a clear trend to the clusters. I cannot make a silhoute coefficient plot due to computer storage capacity, so I chose 4 clusters. With more clusters, the number of overlapping \"top words\" from each cluster seems to increase. In total there are only 30 \"top words\" instead of 40, because some top words overlapped among clusters. There may be some common words that I should consider removing in further analysis, like \"food\"\" or \"coffee\"."},{"cell_type":"markdown","metadata":{"_cell_guid":"eaae8a36-e155-9b28-70e7-0975dade39c3"},"source":"## Logistic Regression to Predict Review Helpfulness with Top Cluster Words "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da9a1ccd-d446-f524-40eb-b48e21ba60a1"},"outputs":[],"source":"print(train_set.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5cb8548c-3b16-53f8-afd8-95280367c40d"},"outputs":[],"source":"#add Score column to top words\nimport scipy as scipy\n\nscore = np.array(list(df1.Score))\nscore = score.reshape(21463, 1)\n\nfeatures = scipy.sparse.hstack((train_set,scipy.sparse.csr_matrix(score)))\n\nfeatures = scipy.sparse.csr_matrix(features)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a86a96bd-5317-fd29-9240-4f72a9a6924b"},"outputs":[],"source":"features.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ace5bef7-54c6-3cd6-ea36-f4b89bb43df9"},"outputs":[],"source":"#find best logistic regression parameters\nfrom sklearn import grid_search, cross_validation\nfrom sklearn.linear_model import LogisticRegression\nfeature_set = features\ngs = grid_search.GridSearchCV(\n    estimator=LogisticRegression(),\n    param_grid={'C': [10**-i for i in range(-5, 5)], 'class_weight': [None, 'balanced']},\n    cv=cross_validation.StratifiedKFold(df1.Helpful,n_folds=10),\n    scoring='roc_auc'\n)\n\n\ngs.fit(features, df1.Helpful)\ngs.grid_scores_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b94e703b-2fa5-1bdc-658d-658a6406af51"},"outputs":[],"source":"print(gs.best_estimator_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d815355-6790-9d5a-e9a3-3f2fa9eda535"},"outputs":[],"source":"y_pred = gs.predict(feature_set)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9db7230-5d1e-375a-12c7-ef8914677fe4"},"outputs":[],"source":"# Coefficients represent the log-odds\nprint(gs.best_estimator_.coef_)\nprint(gs.best_estimator_.intercept_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9981ce0a-9aa3-041f-10fe-548178b523dc"},"outputs":[],"source":"#roc curve\nfrom sklearn.metrics import roc_auc_score, roc_curve\nactuals = gs.predict(feature_set) \nprobas = gs.predict_proba(feature_set)\nplt.plot(roc_curve(df1[['Helpful']], probas[:,1])[0], roc_curve(df1[['Helpful']], probas[:,1])[1])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0017275c-bd8d-5c6b-a4b1-7e84e683499c"},"outputs":[],"source":"#roc auc score\ny_score = probas\ntest2 = np.array(list(df1.Helpful))\ntest2 = test2.reshape(21463,1)\ny_true = test2\n\nroc_auc_score(y_true, y_score[:,1].T)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b7c5393-9716-773f-74ae-d181ca3e5c80"},"outputs":[],"source":"#plot a confusion matrix\nfrom sklearn.metrics import confusion_matrix\ndef plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    plt.tight_layout()\n    plt.ylabel('True Helpfulness')\n    plt.xlabel('Predicted Helpfulness')\n\n\n# Compute confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix, without normalization')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm)\n\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nprint('Normalized confusion matrix')\nprint(cm_normalized)\nplt.figure()\nplot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d96ee379-52b4-efc8-9cb3-31e603303967"},"outputs":[],"source":"#view top parameters\nwords.extend(['score'])\nsorted(zip(words,gs.best_estimator_.coef_[0]),key=lambda x:x[1])"},{"cell_type":"markdown","metadata":{"_cell_guid":"2c954a07-521a-a53a-693d-fbe594b8e20e"},"source":"#### Notes\nThere seem to be common words that I should remove from the text in further analysis, like food, product, or Amazon.\n\nMy model is 82% accurate, which is 10% increase in accuracy over the Bag of Words model."},{"cell_type":"markdown","metadata":{"_cell_guid":"021bf756-d347-f1df-2226-49caab4f2fcd"},"source":"## Recommendations\nPrice, Flavor, and Great are the top indicators of a helpful review. This indicates a possible bias among customers to mark a review as helpful when the review is positive. Eating, Like, Don't, Order, Good, and Eat are all negatively correlated with a helpful review, which is difficult to interpret. These may be more common words to remove.\n\nMoving forward, I would explore the following methods to improve this analysis:\n\n1) I would explore alternative definitions of an \"unhelpful\" review. For example, reviews that are not market as \"helpful\" could be classified as unhelpful. This may help counter consumer-bias if consumers are less likely to mark a negative review as helpful, because it did not enable them to buy the product. This problem requires more domain expertise on consumer behavior.\n\n2) I would make Score into dummy variables to further explore potential biases related to Score. For example, consumers may find that reviews with a score of 1 or 5 are more helpful than scores of 2, 3, and 4.\n\n3) I would explore curating a domain-specific dictionary for this project to avoid common food words and Amazon words in reviews.\n\n4) I would explore using these findings as a guide for reviewers. For example, when writing a review, Amazon could show \"Tips for writing a helpful review\": \"Describe the flavor of this product\" (\"Flavor\" is the most highly correlated parameter with \"helpfulness\"), \"Describe the value of this product compared to its price\", etc."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}