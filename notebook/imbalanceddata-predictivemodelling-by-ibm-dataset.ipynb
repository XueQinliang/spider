{"cells":[{"metadata":{"_uuid":"5573ecb6f00a6cececed466bab2773075d01bc88"},"cell_type":"markdown","source":"# IBM HR Employee Attrition & Performance."},{"metadata":{"_uuid":"a2a49b69b87986c749779b0891b054d5466d892c"},"cell_type":"markdown","source":"## [Please star/upvote in case you find it helpful.]"},{"metadata":{"trusted":true,"_uuid":"550c6f69742dbbce3d3e8eba4485e972a958d9aa","collapsed":true},"cell_type":"code","source":"from IPython.display import Image\nImage('../input/imagesibm/image-logo.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07eb5ae77022a8823151f6a22ab7a2592fa9f562"},"cell_type":"markdown","source":"## CONTENTS ::->"},{"metadata":{"_uuid":"98461f172e2794384b0964c47ce6e2beb70de8b6"},"cell_type":"markdown","source":"[ **1 ) Exploratory Data Analysis**](#content1)"},{"metadata":{"_uuid":"8110699ced15979ab5f5f5ecb43a0e22fbb3ee44"},"cell_type":"markdown","source":"[ **2) Corelation b/w Features**](#content2)"},{"metadata":{"_uuid":"3b6db7d78239f77f6588f4fd2370a908464774cb"},"cell_type":"markdown","source":"[** 3) Feature Selection**](#content3)"},{"metadata":{"_uuid":"fcd189707bfd2294868b8c3cabc9ee2155b50d64"},"cell_type":"markdown","source":"[** 4) Preparing Dataset**](#content4)"},{"metadata":{"_uuid":"fc0d6022d40b3afcc0da6afaa56a99b233451753"},"cell_type":"markdown","source":"[ **5) Modelling**](#content5)\n\n Note that this notebook uses traditional ML algorithms. I have another notebook in which I have used an ANN on the same dataset. To  check it out please follow the below link-->\n\nhttps://www.kaggle.com/rajmehra03/an-introduction-to-ann-keras-with-ibm-hr-dataset/"},{"metadata":{"_uuid":"2f95d591b2dfb9f9b97255b92c52da8d8e6a1589"},"cell_type":"markdown","source":"[ **6) Conclusions**](#content6)"},{"metadata":{"_uuid":"6c247a69ac4a4a10051d39525442c13357255a83"},"cell_type":"markdown","source":"<a id=\"content1\"></a>\n## 1 ) Exploratory Data Analysis"},{"metadata":{"_uuid":"54ce15d50b1d1483b23e8d9a15edcbfde46a4737"},"cell_type":"markdown","source":"## 1.1 ) Importing Various Modules"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f30714bc74672a6875641e3676105cde992ffd36"},"cell_type":"code","source":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport missingno as msno\n\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import the necessary modelling algos.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom imblearn.over_sampling import SMOTE\n\n#preprocess.\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder,OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56e9ddd43a3d952419cceb0de51afa385eeb867d"},"cell_type":"markdown","source":"## 1.2 ) Reading the data from a CSV file"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5d94f0f560e6b6818110244c3de50a7cb2495141"},"cell_type":"code","source":"df=pd.read_csv(r'../input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0b16016495cbe6159461e2ee7e734140a3a1d06","collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"577da1ae33750af121897560a41c56016732adce"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d5c274ce66e7646e792ceeef960309adb0492a74"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2bfa68e86ff79143d094ade796e3ec0d345f13e"},"cell_type":"markdown","source":"## 1.3 ) Missing Values Treatment"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"205c76492ba24cebd2f32de2db9497015a3dfd66"},"cell_type":"code","source":"df.info()  # no null or 'Nan' values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f691a56799e70b60bbe7f345a8902735e7fa3e48"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7ce64e0f91eb7c32b95a10f8a67b4d70e64425b8"},"cell_type":"code","source":"msno.matrix(df) # just to visualize. one final time.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fdac59f4b936424f7ec22aed72730b29476ea50"},"cell_type":"markdown","source":"## 1.4 ) The Features and the 'Target'"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"caeb0d1642dfd1de3c52702a01cca9dc9c695cd0"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3567d83a487201185ae84977e737ea39c8876cbd"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d07700abdfe2e67d86e8775b50841299d4a6195"},"cell_type":"markdown","source":" In all we have 34 features consisting of both the categorical as well as the numerical features. The target variable is the \n 'Attrition' of the employee which can be either a Yes or a No. This is what we have to predict."},{"metadata":{"_uuid":"f3152911eb6d4302d1a68af70f4616f21239b8a3"},"cell_type":"markdown","source":"**Hence this is a Binary Classification problem. **"},{"metadata":{"_uuid":"eeb32c0e019ae4ba72c44f2d83ebcdfccb58573a"},"cell_type":"markdown","source":"## 1.5 ) Univariate Analysis"},{"metadata":{"_uuid":"3b1488724386facd974772710039e183b6b9f8aa"},"cell_type":"markdown","source":"In this section I have done the univariate analysis i.e. I have analysed the range or distribution of the values that various features take. To better analyze the results I have plotted various graphs and visualizations wherever necessary. Univariate analysis helps us identify the outliers in the data."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1fee86666c9e899b09e74c00fe427a499c26f7d5"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b1b3b4d7bb09050fbb7347fc0f2118bcdf41c55"},"cell_type":"markdown","source":" Let us first analyze the various numeric features. To do this we can actually plot a boxplot showing all the numeric features. Also the distplot or a histogram is a reasonable choice in such cases."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6a724a1eab0d4c13a73f53e02a0932e4acb230f5"},"cell_type":"code","source":"sns.factorplot(data=df,kind='box',size=10,aspect=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac67399c808272974dfb05a3023b8fe566d2a1d0"},"cell_type":"markdown","source":"Note that all the features have pretty different scales and so plotting a boxplot is not a good idea. Instead what we can do is plot histograms of various continuously distributed features.\n\nWe can also plot a kdeplot showing the distribution of the feature. Below I have plotted a kdeplot for the 'Age' feature.\nSimilarly we plot for other numeric features also. Similarly we can also use a distplot from seaborn library which combines most.."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"72757979f706754d6e0937d98a65987134c7c8b9"},"cell_type":"code","source":"sns.kdeplot(df['Age'],shade=True,color='#ff4125')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ef21a1953b845f6b8554c3f03de45b7ef10eceeb"},"cell_type":"code","source":"sns.distplot(df['Age'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ac5ac2a28318b6904433788dbe675664cb388eb"},"cell_type":"markdown","source":"Similarly we can do this for all the numerical features. Below I have plotted the subplots for the other features."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2527245673c5bc6598963ea02f2ccbfbbebae596"},"cell_type":"code","source":"warnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\nfig,ax = plt.subplots(5,2, figsize=(9,9))                \nsns.distplot(df['TotalWorkingYears'], ax = ax[0,0]) \nsns.distplot(df['MonthlyIncome'], ax = ax[0,1]) \nsns.distplot(df['YearsAtCompany'], ax = ax[1,0]) \nsns.distplot(df['DistanceFromHome'], ax = ax[1,1]) \nsns.distplot(df['YearsInCurrentRole'], ax = ax[2,0]) \nsns.distplot(df['YearsWithCurrManager'], ax = ax[2,1]) \nsns.distplot(df['YearsSinceLastPromotion'], ax = ax[3,0]) \nsns.distplot(df['PercentSalaryHike'], ax = ax[3,1]) \nsns.distplot(df['YearsSinceLastPromotion'], ax = ax[4,0]) \nsns.distplot(df['TrainingTimesLastYear'], ax = ax[4,1]) \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"751679e5bf01026d46ca2ff2638bf2be251af582"},"cell_type":"markdown","source":"Let us now analyze the various categorical features. Note that in these cases the best way is to use a count plot to show the relative count of observations of different categories."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"58611243829c6420c07472537eb49d2a5e09aef9"},"cell_type":"code","source":"cat_df=df.select_dtypes(include='object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"887a77ab99a387081b5ee1f823e68b2c9bb962a1"},"cell_type":"code","source":"cat_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7c8113aa4b1e91d2ed9062e07c2ffe52e76502b9"},"cell_type":"code","source":"def plot_cat(attr,labels=None):\n    if(attr=='JobRole'):\n        sns.factorplot(data=df,kind='count',size=5,aspect=3,x=attr)\n        return\n    \n    sns.factorplot(data=df,kind='count',size=5,aspect=1.5,x=attr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03f11e6526e7cacaeb9888c2f6786638b50cfa00"},"cell_type":"markdown","source":"I have made a function that accepts the name of a string. In our case this string will be the name of the column or attribute which we want to analyze. The function then plots the countplot for that feature which makes it easier to visualize."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"218685308dc339ad2c770407a56a8736ea0aae60"},"cell_type":"code","source":"plot_cat('Attrition')   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d5ce66637b0a3860800acee7d899febe8baa22f"},"cell_type":"markdown","source":"**Note that the number of observations belonging to the 'No'  category is way greater than that belonging to 'Yes' category. Hence we have skewed classes and this is a typical example of the 'Imbalanced Classification Problem'. To handle such types of problems we need to use the over-sampling or under-sampling techniques. I shall come back to this point later.**"},{"metadata":{"_uuid":"46edb1a3c85dc50f155e137a6f55fef98517f9ac"},"cell_type":"markdown","source":"**Let us now similalry analyze other categorical features.**"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"97977aca437577892e99c3d9aa83d98fc8d44f2f"},"cell_type":"code","source":"plot_cat('BusinessTravel')   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acbf6e5486407032377c605a569afe36e7af33df"},"cell_type":"markdown","source":"The above plot clearly shows that most of the people belong to the 'Travel_Rarely' class. This indicates that most of the people did not have a job which asked them for frequent travelling."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a49e458694079abd498d3e1ac132a95a72ae3e37"},"cell_type":"code","source":"plot_cat('OverTime')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"384c5ad34be24dd5344f5012466ad68838cfe6be"},"cell_type":"code","source":"plot_cat('Department')   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e5fdc730da4957cf9b7643ee6e71bc3e230d97d5"},"cell_type":"code","source":"plot_cat('EducationField')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2bbacc20f834a1dcd4d1da3526167d1530063185"},"cell_type":"code","source":"plot_cat('Gender') ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f391db1f8e77a6977b6d23549cd2e7f49cf1161"},"cell_type":"markdown","source":"Note that males are present in higher number."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b0e340d2bf30f9a93dc4dd354518950a12dacd7d"},"cell_type":"code","source":"plot_cat('JobRole')   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8ad8eacec5069095f2a5bfc64595ccb60ec6d72"},"cell_type":"markdown","source":"** Similarly we can continue for other categorical features. **"},{"metadata":{"_uuid":"3e578b6445dfa49d27a359415dea56853f02bd28"},"cell_type":"markdown","source":"**Note that the same function can also be used to better analyze the numeric discrete features like 'Education','JobSatisfaction' etc... "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7de70aeea73607b7958ba2347ff9f5b951871e57"},"cell_type":"code","source":"# just uncomment the following cell.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bf39d394c8e1c964a8f17e2b604e3d2e1b463fb1"},"cell_type":"code","source":"# num_disc=['Education','EnvironmentSatisfaction','JobInvolvement','JobSatisfaction','WorkLifeBalance','RelationshipSatisfaction','PerformanceRating']\n# for i in num_disc:\n#     plot_cat(i)\n\n# similarly we can intrepret these graphs.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e18672442ce943794e6d7c8e0688f19cadd99ef"},"cell_type":"markdown","source":"<a id=\"content2\"></a>\n## 2 ) Corelation b/w Features\n"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"77754b09eb27cd6806bf0c0ab404de2e80ae5c33"},"cell_type":"code","source":"#corelation matrix.\ncor_mat= df.corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d701523379499dc296220f48cc9addb7f209061c"},"cell_type":"markdown","source":"###### SOME INFERENCES FROM THE ABOVE HEATMAP\n\n1. Self relation ie of a feature to itself is equal to 1 as expected.\n\n2. JobLevel is highly related to Age as expected as aged employees will generally tend to occupy higher positions in the company.\n\n3. MonthlyIncome is very strongly related to joblevel as expected as senior employees will definately earn more.\n\n4. PerformanceRating is highly related to PercentSalaryHike which is quite obvious.\n\n5. Also note that TotalWorkingYears is highly related to JobLevel which is expected as senior employees must have worked for a larger span of time.\n\n6. YearsWithCurrManager is highly related to YearsAtCompany.\n\n7. YearsAtCompany is related to YearsInCurrentRole.\n\n  "},{"metadata":{"_uuid":"2f497a029c01e79750e92435dd3a74b21ca7bc32"},"cell_type":"markdown","source":"**Note that we can drop some highly corelated features as they add redundancy to the model but since the corelation is very less in genral let us keep all the features for now. In case of highly corelated features we can use something like Principal Component Analysis(PCA) to reduce our feature space.**"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fb589e8bea1f03a39baf0e4a5afce1e5b262975c"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9be888bd2dfabbd6f50a65a801f2b7bea5002c0"},"cell_type":"markdown","source":"<a id=\"content3\"></a>\n## 3 ) Feature Selection\n"},{"metadata":{"_uuid":"1078663f65c667fd102fcbe9175d8ba22b4fa34f"},"cell_type":"markdown","source":"## 3.1 ) Plotting the Features against the 'Target' variable."},{"metadata":{"_uuid":"50c4211533258caf79927c10219d6d65565e23f4"},"cell_type":"markdown","source":"####  3.1.1 ) Age"},{"metadata":{"_uuid":"cb5a242603ba76641ff1648bd17bb57f5a4d466d"},"cell_type":"markdown","source":"Note that Age is a continuous quantity and therefore we can plot it against the Attrition using a boxplot."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ab053268eace723980a8c9aaee0709a468c6067b"},"cell_type":"code","source":"sns.factorplot(data=df,y='Age',x='Attrition',size=5,aspect=1,kind='box')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"363c385b0e55b4bb9f4c095188aead7145d52ce2"},"cell_type":"markdown","source":"Note that the median as well the maximum age of the peole with 'No' attrition is higher than that of the 'Yes' category. This shows that people with higher age have lesser tendency to leave the organisation which makes sense as they may have settled in the organisation."},{"metadata":{"_uuid":"075008ffbebb4cedb31627a9df4d2ee7756af6ba"},"cell_type":"markdown","source":"#### 3.1.2 ) Department"},{"metadata":{"_uuid":"9a657ce1fadd19c0c5f49bb5aa854a89908a4485"},"cell_type":"markdown","source":"Note that both Attrition(Target) as well as the Deaprtment are categorical. In such cases a cross-tabulation is the most reasonable way to analyze the trends; which shows clearly the number of observaftions for each class which makes it easier to analyze the results."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"dfe209299d1976c8e2f9ea13a09f44b7cfdcfb80"},"cell_type":"code","source":"df.Department.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"dc7295b776c99149fb8770e6891716cb4ae277f3"},"cell_type":"code","source":"sns.factorplot(data=df,kind='count',x='Attrition',col='Department')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cfab52f59ade24b30702a81c41eeec386e2b1522"},"cell_type":"code","source":"pd.crosstab(columns=[df.Attrition],index=[df.Department],margins=True,normalize='index') # set normalize=index to view rowwise %.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e07f16edf031768932a89975acd7297f06a1277"},"cell_type":"markdown","source":"Note that most of the observations corresspond to 'No' as we saw previously also. About 81 % of the people in HR dont want to leave the organisation and only 19 % want to leave. Similar conclusions can be drawn for other departments too from the above cross-tabulation."},{"metadata":{"_uuid":"483bf4e61a17667c903b53ff0fc0a018c40cff72"},"cell_type":"markdown","source":"#### 3.1.3 ) Gender"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2a13a849d815b93bfe91fc4190e35e5e0e9a2f57"},"cell_type":"code","source":"pd.crosstab(columns=[df.Attrition],index=[df.Gender],margins=True,normalize='index') # set normalize=index to view rowwise %.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71042c1002cff111ae6e44c333b71a7bc2493a42"},"cell_type":"markdown","source":"About 85 % of females want to stay in the organisation while only 15 % want to leave the organisation. All in all 83 % of employees want to be in the organisation with only being 16% wanting to leave the organisation or the company."},{"metadata":{"_uuid":"d343d503325f1c803ecb38115c28db81f7468c4a"},"cell_type":"markdown","source":"#### 3.1.4 ) Job Level"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2f5d27e9733d77762c015f57a7a374215f0022b9"},"cell_type":"code","source":"pd.crosstab(columns=[df.Attrition],index=[df.JobLevel],margins=True,normalize='index') # set normalize=index to view rowwise %.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14affcb2465cfdc4e6d0b94b5895298f7f41ded2"},"cell_type":"markdown","source":"People in Joblevel 4 have a very high percent for a 'No' and a low percent for a 'Yes'. Similar inferences can be made for other job levels."},{"metadata":{"_uuid":"255241f287407a5f6532f39234bb55dc7a833a92"},"cell_type":"markdown","source":"#### 3.1.5 ) Monthly Income"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5bb7c3def88aea6e44a8b5981bafac2f69d7e830"},"cell_type":"code","source":"sns.factorplot(data=df,kind='bar',x='Attrition',y='MonthlyIncome')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89218410c41010b9e52b79b91b4cb087e5e8a17f"},"cell_type":"markdown","source":" Note that the average income for 'No' class is quite higher and it is obvious as those earning well will certainly not be willing to exit the organisation. Similarly those employees who are probably not earning well will certainly want to change the company."},{"metadata":{"_uuid":"badf29263314bb63e6c36219be04234b4f08036c"},"cell_type":"markdown","source":"#### 3.1.6 ) Job Satisfaction"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6285b6f408ee83a2b5409901efd2dee8ebf4e469"},"cell_type":"code","source":"sns.factorplot(data=df,kind='count',x='Attrition',col='JobSatisfaction')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b6ffe4e71f5117fecafe8ff15c18a0c062be1b73"},"cell_type":"code","source":"pd.crosstab(columns=[df.Attrition],index=[df.JobSatisfaction],margins=True,normalize='index') # set normalize=index to view rowwise %.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"973c1edce73123ff4c30f64293b3a44ffa51546e"},"cell_type":"markdown","source":"Note this shows an interesting trend. Note that for higher values of job satisfaction( ie more a person is satisfied with his job) lesser percent of them say a 'Yes' which is quite obvious as highly contented workers will obvioulsy not like to leave the organisation."},{"metadata":{"_uuid":"48df46142e9b985ef3cf2882bce2cc7518a4c753"},"cell_type":"markdown","source":"#### 3.1.7 ) Environment Satisfaction "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c9251ef01eb1de871d8ea2a5350f08a7c3ffb2f3"},"cell_type":"code","source":"pd.crosstab(columns=[df.Attrition],index=[df.EnvironmentSatisfaction],margins=True,normalize='index') # set normalize=index to view rowwise %.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd23bb2fa6e5d649c5938d4ae81be13bd052438a"},"cell_type":"markdown","source":"Again we can notice that the relative percent of 'No' in people with higher grade of environment satisfacftion which is expected."},{"metadata":{"_uuid":"7bcff93ae7b1a1621ff1d9c3e305ebdf1578bf2b"},"cell_type":"markdown","source":"#### 3.1.8 ) Job Involvement"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"010d467648aea594ec635ab32d4ca5bd0b5bbcc7"},"cell_type":"code","source":"pd.crosstab(columns=[df.Attrition],index=[df.JobInvolvement],margins=True,normalize='index') # set normalize=index to view rowwise %.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cbe2215d380326c846a6381ef9a75813dcdab71"},"cell_type":"markdown","source":"#### 3.1.9 ) Work Life Balance"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0f6e5de34d8448b753240d25fb8716d71b7865a7"},"cell_type":"code","source":"pd.crosstab(columns=[df.Attrition],index=[df.WorkLifeBalance],margins=True,normalize='index') # set normalize=index to view rowwise %.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdaca58ce582fab36101adb7554c05ead83425d2"},"cell_type":"markdown","source":"Again we notice a similar trend as people with better work life balance dont want to leave the organisation."},{"metadata":{"_uuid":"99ade5eb5ae77c4ee287c548617d8af36668fc34"},"cell_type":"markdown","source":"#### 3.1.10 ) RelationshipSatisfaction"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cdb35b697148eef431463b15a6bb5cd3da77dca5"},"cell_type":"code","source":"pd.crosstab(columns=[df.Attrition],index=[df.RelationshipSatisfaction],margins=True,normalize='index') # set normalize=index to view rowwise %.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3e507101c80280cb7747e9e8d638ee283d84ece"},"cell_type":"markdown","source":"###### Notice that I have plotted just some of the important features against out 'Target' variable i.e. Attrition in our case. Similarly we can plot other features against the 'Target' variable and analye the trends i.e. how the feature effects the 'Target' variable."},{"metadata":{"_uuid":"09caa29ebd8b587a4c0feb1c973b38833357fa03"},"cell_type":"markdown","source":"## 3.2 ) Feature Selection"},{"metadata":{"_uuid":"826fd29c4402119bb4d99bfb96cd4cfaa5753bfa"},"cell_type":"markdown","source":"The feature Selection is one of the main steps of the preprocessing phase as the features which we choose directly effects the model performance. While some of the features seem to be less useful in terms of the context; others seem to equally useful. The better features we use the better our model will perform.  **After all Garbage in Garbage out;)**.\n\nWe can also use the Recusrive Feature Elimination technique (a wrapper method) to choose the desired number of most important features.\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\n\nIt uses the model accuracy to identify which attributes (and/or combination of attributes) contribute the most to predicting the target attribute.\n\nWe can use it directly from the scikit library by importing the RFE module or function provided by the scikit. But note that since it tries different combinations or the subset of features;it is quite computationally expensive and I shall ignore it here."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6836fa8e5ce1eca71a1349470b70ddac152d0e84"},"cell_type":"code","source":"df.drop(['BusinessTravel','DailyRate','EmployeeCount','EmployeeNumber','HourlyRate','MonthlyRate'\n          ,'NumCompaniesWorked','Over18','StandardHours', 'StockOptionLevel','TrainingTimesLastYear'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6511896ee45357cf6cf16bae12c09987b8c0735e"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"a672649819f8f6871ea5bbe730a0641d8e763f5d"},"cell_type":"markdown","source":"<a id=\"content4\"></a>\n##  4 ) Preparing Dataset\n"},{"metadata":{"_uuid":"4f2ba256aa5d01a55503a513730de3666233a429"},"cell_type":"markdown","source":"## 4.1 ) Feature Encoding "},{"metadata":{"_uuid":"1a959f6ebf4d2fc6fc9da28763617c8d960af80d"},"cell_type":"markdown","source":"I have used the Label Encoder from the scikit library to encode all the categorical features."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d134c2a82229935bd94f87a435b79c2216d189b9"},"cell_type":"code","source":"def transform(feature):\n    le=LabelEncoder()\n    df[feature]=le.fit_transform(df[feature])\n    print(le.classes_)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9879edc4ba99a0e05e0a2fefcd6557272687b5c0"},"cell_type":"code","source":"cat_df=df.select_dtypes(include='object')\ncat_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"be9415a976e32a3b5e1166d490b3013208a74c00"},"cell_type":"code","source":"for col in cat_df.columns:\n    transform(col)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"abad5777fcfedc5cf37a645dfb162a0ddc03c00b"},"cell_type":"code","source":"df.head() # just to verify.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89bc0b38e7ea5ad2393815ebaf0e27303a89df79"},"cell_type":"markdown","source":"## 4.2 ) Feature Scaling."},{"metadata":{"_uuid":"6bfae0dbf9147f0ecfd6d4c5b7525229da6d0161"},"cell_type":"markdown","source":"The scikit library provides various types of scalers including MinMax Scaler and the StandardScaler. Below I have used the StandardScaler to scale the data."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a1e0169d1d393aafa538bf8015d71b09575930ea"},"cell_type":"code","source":"scaler=StandardScaler()\nscaled_df=scaler.fit_transform(df.drop('Attrition',axis=1))\nX=scaled_df\nY=df['Attrition'].as_matrix()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f29fa3003e7fc145f7d89c614446b2afafb3ccb"},"cell_type":"markdown","source":"## 4.3 ) Splitting the data into training and validation sets"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c35ef5eadf7f7293a01976c15b15c439b1049042"},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fcd4a30b10e5bf2f390190e6f3ac1c86ebc43b0"},"cell_type":"markdown","source":"<a id=\"content5\"></a>\n## 5 ) Modelling\n"},{"metadata":{"_uuid":"580910bf7249867d41d6e8dd3452ea23957d5dc1"},"cell_type":"markdown","source":"## 5.1 ) Handling the Imbalanced dataset"},{"metadata":{"_uuid":"3d5bf7e6b610b175c27ec99d5ca1041f2a486698"},"cell_type":"markdown","source":"Note that we have a imbalanced dataset with majority of observations being of one type ('NO') in our case. In this dataset for example we have about 84 % of observations having 'No' and only 16 % of 'Yes' and hence this is an imbalanced dataset.\n\nTo deal with such a imbalanced dataset we have to take certain measures, otherwise the performance of our model can be significantly affected. In this section I have discussed two approaches to curb such datasets."},{"metadata":{"_uuid":"a688635ff5f411431df07b9de4dd3878fcc6f2fe"},"cell_type":"markdown","source":"## 5.1.1 ) Oversampling the Minority or Undersampling the Majority Class\n \n  "},{"metadata":{"_uuid":"b6f5d3e512f97d779d8af9cf7e18143ed716aa86"},"cell_type":"markdown","source":"In an imbalanced dataset the main problem is that the data is highly skewed ie the number of observations of certain class is more than that of the other. Therefore what we do in this approach is to either increase the number of observations corressponding  to the minority class (oversampling) or decrease the number of observations for the majority class (undersampling).\n\nNote that in our case the number of observations is already pretty low and so oversampling will be more appropriate.\n\nBelow I have used an oversampling technique known as the SMOTE(Synthetic Minority Oversampling Technique) which randomly creates some 'Synthetic' instances of the minority class so that the net observations of both the class get balanced out.\n\nOne thing more to take of is to use the SMOTE before the cross validation step; just to ensure that our model does not overfit the data; just as in the case of feature selection."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"70d4c5d318b491369a4f255fab93accae30bd009"},"cell_type":"code","source":"oversampler=SMOTE(random_state=42)\nx_train_smote,  y_train_smote = oversampler.fit_sample(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"828bcc54df1961afc7e519dfb7073ffe7500cc15"},"cell_type":"markdown","source":"## 5.1.2 ) Using the Right Evaluation Metric"},{"metadata":{"_uuid":"5433705b1cec9ea048549755784563dc841f584e"},"cell_type":"markdown","source":"Another important point while dealing with the imbalanced classes is the choice of right evaluation metrics. \n\nNote that accuracy is not a good choice. This is because since the data is skewed even an algorithm classifying the target as that belonging to the majority class at all times will achieve a very high accuracy. \nFor  eg if we have 20 observations of one type 980 of another ; a classifier predicting the majority class at all times will also attain a accuracy of 98 % but doesnt convey any useful information.\n\nHence in these type of cases we may use other metrics such as -->\n\n\n**'Precision'**-- (true positives)/(true positives+false positives)\n\n**'Recall'**-- (true positives)/(true positives+false negatives)\n\n**'F1 Score'**-- The harmonic mean of 'precision' and 'recall'\n\n'**AUC ROC'**-- ROC curve is a plot between 'senstivity' (Recall) and '1-specificity' (Specificity=Precision)\n\n**'Confusion Matrix'**-- Plot the entire confusion matrix"},{"metadata":{"_uuid":"03ccf90027cafbd672c26084794d0ea42c4ae0a4"},"cell_type":"markdown","source":"## 5.2 ) Building A Model & Making Predictions"},{"metadata":{"_uuid":"38d16e3259a5a3cf3250b772adcce0669e413508"},"cell_type":"markdown","source":"In this section I have used different models from the scikit library and trained them on the previously oversampled data and then used them for the prediction purposes."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7c5e9cbdef3c7019c0bf5d116c29affa47bf94d0"},"cell_type":"code","source":"def compare(model):\n    clf=model\n    clf.fit(x_train_smote,y_train_smote)\n    pred=clf.predict(x_test)\n    \n    # Calculating various metrics\n    \n    acc.append(accuracy_score(pred,y_test))\n    prec.append(precision_score(pred,y_test))\n    rec.append(recall_score(pred,y_test))\n    auroc.append(roc_auc_score(pred,y_test))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"feed8ce6cf2aebef10bd1de9298dca4e8d0f3933"},"cell_type":"code","source":"acc=[]\nprec=[]\nrec=[]\nauroc=[]\nmodels=[SVC(kernel='rbf'),RandomForestClassifier(),GradientBoostingClassifier()]\nmodel_names=['rbfSVM','RandomForestClassifier','GradientBoostingClassifier']\n\nfor model in range(len(models)):\n    compare(models[model])\n    \nd={'Modelling Algo':model_names,'Accuracy':acc,'Precision':prec,'Recall':rec,'Area Under ROC Curve':auroc}\nmet_df=pd.DataFrame(d)\nmet_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d74e48a7d887524399923f72b0d138c53e6f791"},"cell_type":"markdown","source":"## 5.3 ) Comparing Different Models"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"88dfa6d91a8f328c8ff87e7d178af81a21fc45b0"},"cell_type":"code","source":"def comp_models(met_df,metric):\n    sns.factorplot(data=met_df,x=metric,y='Modelling Algo',size=5,aspect=1.5,kind='bar')\n    sns.factorplot(data=met_df,y=metric,x='Modelling Algo',size=7,aspect=2,kind='point')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b7a31c56e4a2df5e408ecc93a70faed863ac7d15"},"cell_type":"code","source":"comp_models(met_df,'Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"72b0bb735eff66a793d46cf3145311a37ab09801"},"cell_type":"code","source":"comp_models(met_df,'Precision')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9ffade72abe8d2799466227399557d251dc85312"},"cell_type":"code","source":"comp_models(met_df,'Recall')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"08870a256f57e1b43002f82a549b7d1f0d794762"},"cell_type":"code","source":"comp_models(met_df,'Area Under ROC Curve')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4faabade8cb44bcbe9620b26128dbcba92bbdec1"},"cell_type":"markdown","source":"The above data frame and the visualizations summarize the resuts after training different models on the given dataset.  "},{"metadata":{"_uuid":"f1390acbc39adda39a5782fc847220e6b606bda6"},"cell_type":"markdown","source":"<a id=\"content6\"></a>\n## 6) Conclusions\n"},{"metadata":{"_uuid":"f3c26cc441c9137c83d747c851790b50f8063310"},"cell_type":"markdown","source":"###### Hence we have completed the analysis of the data and also made predictions using the various ML models.                                           "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a5d420956e868decbcbbd92532776eea36c6ff58"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f52eb3c6ab3ba0243e2f9edd7e8dde382b7f272","collapsed":true},"cell_type":"code","source":"Image('../input/imagesibm/image-hr.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fec01606a737daa1aa39e54739f1285c4b177e8d"},"cell_type":"code","source":" ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0e3e0798f8ba29705fdd3a38a60411500c07afc"},"cell_type":"markdown","source":"# THE END."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"09048176fa7967ec876408dcf5d27906e3623393"},"cell_type":"markdown","source":"## [Please star/upvote if u found it helpful.]##"},{"metadata":{"trusted":true,"_uuid":"e3fe3b898f7ebafe8dfe1293c349784e513a962f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}