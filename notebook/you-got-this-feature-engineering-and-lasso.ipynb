{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9de0282c-9dd9-28a5-7b8f-e9c5cd4b87aa"},"source":"###This is my first notebook! And rather than going the XGBoost way, I decided it's about time to go back to basics.\n\nWe'll look into \n###1. basic feature engineering and extraction\n###2. use the Lasso model and give our newly developed data a test run!\n###3. Some (minor) visualizations"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d2f05bb-b435-a58a-5f23-cd7afa2a6d58"},"outputs":[],"source":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale\nfrom scipy.stats import skew, skewtest\n%config InlineBackend.figure_format = 'png' \n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4f2eb2f-f945-3097-faa1-91ec20a22b6b"},"outputs":[],"source":"# read in the data\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"8391cd6d-0670-830f-e235-199412467e9e"},"source":"### Proposed feature: '1stFlrSF' + '2ndFlrSF' to give us combined Floor Square Footage\nbut first, let's check how well it fares!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7877b2ce-7fa2-c375-5eb3-d7d6e5f96d2a"},"outputs":[],"source":"feat_trial = (train['1stFlrSF'] + train['2ndFlrSF']).copy()\nprint(\"Skewness of the original intended feature:\",skew(feat_trial))\nprint(\"Skewness of transformed feature\", skew(np.log1p(feat_trial)))\n\n# hence, we'll use the transformed feature thank you very much!\nfeat_trial = np.log1p(feat_trial)\nmatplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n\n# seaborn's regression plot (I liked it a lot. hence it found it's way here!)\nsns.regplot(x=(feat_trial), y=np.log1p(train['SalePrice']), data=train, order=1);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8722ee08-29df-a37a-e146-d2c728db412e"},"outputs":[],"source":"# lets create the feature then\ntrain['1stFlr_2ndFlr_Sf'] = np.log1p(train['1stFlrSF'] + train['2ndFlrSF'])\ntest['1stFlr_2ndFlr_Sf'] = np.log1p(test['1stFlrSF'] + test['2ndFlrSF'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"99196bc4-187f-25a1-33b5-486cbdc18a00"},"source":"### Feature number 2 -> 1stflr+2ndflr+lowqualsf+GrLivArea = All_Liv_Area\n\nlet's see how this fares too"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a14e2cc-1d22-78a0-3bd6-e0aae60ba04d"},"outputs":[],"source":"feat_trial = (train['1stFlr_2ndFlr_Sf'] + train['LowQualFinSF'] + train['GrLivArea']).copy()\nprint(\"Skewness of the original intended feature:\",skew(feat_trial))\nprint(\"Skewness of transformed feature\", skew(np.log1p(feat_trial)))\n\n# hence, we'll use the transformed feature thank you very much!\nfeat_trial = np.log1p(feat_trial)\nmatplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n\n# seaborn's regression plot (I liked it a lot. hence it found it's way here!)\nsns.regplot(x=(feat_trial), y=np.log1p(train['SalePrice']), data=train, order=1);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6941183b-b0c5-b1f4-8d2a-564a9b36ad2e"},"outputs":[],"source":"train['All_Liv_SF'] = np.log1p(train['1stFlr_2ndFlr_Sf'] + train['LowQualFinSF'] + train['GrLivArea'])\ntest['All_Liv_SF'] = np.log1p(test['1stFlr_2ndFlr_Sf'] + test['LowQualFinSF'] + test['GrLivArea'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"2cb1f19e-18bc-10d3-ede8-ee4bc981140b"},"source":"## Those were the two features I added. Let's move further to Step 2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"335cd655-22d8-03f7-7b5a-d3816223abd1"},"outputs":[],"source":"# get all features except Id and SalePrice\nfeats = train.columns.difference(['Id','SalePrice'])\n\n# the most hassle free way of working with data is to concatenate them\n# since there are many features that contain nan/null values in the test set\n# that the train set doesn't\nall_data = pd.concat((train.loc[:,feats],\n                      test.loc[:,feats]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ac29d519-9ac8-3f09-0146-8f077844e6db"},"source":"## The To-Do List\n\n### 1. Transform skewed numeric features using log(p+1) transformation making them more normal\n### 2. Find dummy variables for categorical features\n### 3. Replace nans/null values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ad23db5-c3f1-17b6-0b7f-6e5ba28f3c80"},"outputs":[],"source":"# But first, we log transform the target: (reason well explained in Alexandru's AWESOME Notebook)\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])"},{"cell_type":"markdown","metadata":{"_cell_guid":"953a1ffa-f009-655f-206f-97ba1b6ea25b"},"source":"## 1. Transformations\n\n###PS: log(p+1) transformations are not the available transformations for reducing skewness. I have tried sqrt, which yields better results as compared to log(p+1) (for certain features only. log(1+p) is a better way to go for this dataset at least)*emphasized text*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"118ea210-68d8-9a01-9942-bc0e70fc49af"},"outputs":[],"source":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])"},{"cell_type":"markdown","metadata":{"_cell_guid":"18fb8633-9110-8ac7-79f8-8a6904d37749"},"source":"## 2. Let's get them dummies"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5baa18b-32a3-8bc9-8db1-9c4c7b96a10b"},"outputs":[],"source":"# getting dummies for all features. You can go the LabelEncoder way, but this method\n# is more sound (and easier!!!) in my opinion\nall_data = pd.get_dummies(all_data)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7e558b1b-e2eb-6725-0a7f-441c8f029a2c"},"source":"## 3. Fill them nan's"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36377d36-49dc-0eda-85c1-ff32af34099f"},"outputs":[],"source":"# 3. filling NA's with the mean of the column:\nall_data = all_data.fillna(all_data[:train.shape[0]].mean())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e00add98-4568-4e6e-7705-a911d0fa5939"},"outputs":[],"source":"print(all_data.shape)\n# creating matrices for sklearn:\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.SalePrice"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f485753-8a79-37c3-c585-fcb628b3b0ed"},"outputs":[],"source":"# optional. Save these newly created matrices for later usage\n# new_test = pd.DataFrame(X_test.copy())\n# new_test['Id'] = test['Id'].copy()\n# new_test.to_csv(\"../input/new_test.csv\", index=False)\n\n# new_train = pd.DataFrame(X_train.copy())\n# new_train['SalePrice'] = y.copy()\n# new_train.to_csv(\"../input/new_train.csv\", index=False)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"595b3774-a328-4435-9dce-cb755ce85651"},"source":"## All set. Moving on to incorporate this data\n### But first, Let's devise a cross-validation methodology once and for all (thanks again, Alexandru)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"138ee94e-b76e-db10-30b7-e8ec8e4554b3"},"outputs":[],"source":"from sklearn.cross_validation import cross_val_score\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return(rmse)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5cbd1e5-5fd5-4786-20a1-90d8f0130cab"},"outputs":[],"source":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, LinearRegression"},{"cell_type":"markdown","metadata":{"_cell_guid":"bb6f8617-e69e-130f-ebe8-72ac494b4eec"},"source":"### Let's get to work using LassoCV"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d3048a7-f1d7-0f87-99c6-bbff8634c0e1"},"outputs":[],"source":"model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005], selection='random', max_iter=15000).fit(X_train, y)\nres = rmse_cv(model_lasso)\nprint(\"Mean:\",res.mean())\nprint(\"Min: \",res.min())"},{"cell_type":"markdown","metadata":{"_cell_guid":"5b17ca15-fd6a-401d-d0cd-82e1d74ae7dd"},"source":"### The above model yields a lb score of 0.12102* (some results I've gained through kaggle nb's and my local system have been different)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32c89a2c-1842-09ef-1b49-cb4ac256e749"},"outputs":[],"source":"coef = pd.Series(model_lasso.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c77c4ad-f7d6-f95e-9487-6721daa42bb5"},"outputs":[],"source":"# plotting feature importances!\nimp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"f49e0191-346a-e88a-2e77-6573b0757a71"},"source":"### Woah. The first feature we engineered did end up being pretty important!!!! Way To go!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2287d547-32d3-02b3-2ab3-f09ae79ebe82"},"outputs":[],"source":"# Let's make some predictions and submit it to the lb\ntest_preds = np.expm1(model_lasso.predict(X_test))\nsubmission = pd.DataFrame()\nsubmission['Id'] = test['Id']\nsubmission[\"SalePrice\"] = test_preds\nsubmission.to_csv(\"lasso_by_Sarthak.csv\", index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03d12841-ea57-18b1-aca3-594272882963"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}