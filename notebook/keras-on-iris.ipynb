{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"322a1248-71b5-106b-201b-cc8dbca89bf9"},"source":"Keras on Iris (Quick and Dirty)\n-------------------------------\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0fb46161-0311-d2b5-fbb1-78f950cc3b56"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\n \nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n# https://www.kaggle.com/uciml/iris/downloads/iris-species.zip\ndata = pd.read_csv('../input/Iris.csv')\ndata = np.array(data)\n\nclass iris:\n    \"\"\" Simple way to hold data\"\"\"\n    dictionary={}\n    \n    \ndef getV(n):\n    mint = int(np.round(n)) \n    if mint in dictionary:\n        return iris.dictionary[mint]\n    else:\n        return -1\n\n        \n# label encode the categorical variables\nfor i in range(data.shape[1]):\n    if i in [5]:  # colums to convert. Just the last one in this case\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(data[:,i])\n        \n        values   = np.unique(data[:,5])\n        keys = lbl.transform(np.unique(data[:,5]))\n        iris.dictionary = dict(zip(keys, values))\n\n        data[:,i] = lbl.transform(data[:,i])\n        \n# See class above\niris.target = data[:,-1]\niris.data   = data[:,1:-1]\n\n# Training on 10% of the data\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, train_size=0.10, random_state=5)\n\nX_train.shape,y_train.shape,X_train.shape[1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19ade35d-9c43-af51-5561-60b9d5388d7a"},"outputs":[],"source":"# Make sure you get kind of an even balance.\n# You want at least one of each.\n\nnp.unique(y_train, return_counts=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b622bf9-c503-028a-91a5-a0a81a986a1f"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\n\n\nfrom sklearn import preprocessing\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\n\n\n# Keras model\ndef findMin(dense=64, drop=[],epoch=33):\n    np.random.seed(seed=23) \n    PYTHONHASHSEED=0\n    model = Sequential()\n    model.add(Dense(dense, input_dim=X_train.shape[1]))\n    model.add(Activation('relu'))\n    model.add(Dropout(drop[0]))\n    model.add(Dense(dense))\n    model.add(Activation('relu'))\n    model.add(Dropout(drop[1]))\n    model.add(Dense(1))\n\n    #model.compile(loss='mse', optimizer='rmsprop')\n    model.compile(loss='mse', optimizer='adam')\n\n    history = model.fit(X_train, y_train, nb_epoch=epoch, batch_size=32, verbose=0)\n    myMin=min(history.history['loss'])\n    return history.history['loss'].index(myMin),myMin,history,model\n\np=[]\nfor dense in [20,32,64,128]:\n    a,b,c,model = findMin(dense,[0,0])\n    print(\"Min loss: {:^9.3f}   dense: {:^9.3f}\".format(b,dense))\n    plt.plot(c.history['loss'])\n    p.append('Dense %s' % dense)\n    \n    \nplt.title(\"Loss\")\nplt.legend(p);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c9dd8b4c-12cf-afca-140e-a42678e30e60"},"outputs":[],"source":"p=[]\nfor dense in [64,128]:\n    a,b,c,model = findMin(dense,[0,0],100)\n    print(\"Min loss: {:^9.3f}   dense: {:^9.2f}\".format(b,dense))\n    plt.plot(c.history['loss'])\n    p.append('Dense %s' % dense)\n    \n    \n\nplt.legend(p);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"caae7faa-3831-69c5-722f-f02e7d6082a5"},"outputs":[],"source":"p=[]\ng=[]\nfor dense in [32]:\n    index,b,c,model = findMin(dense,[0,0],epoch=1500)\n    print(\"Min loss: {:^9.3f}   dense: {:^9.2f}  index: {:d}\".format(b,dense,index))\n    g.append(c.history['loss'])\n    plt.plot(c.history['loss'])\n    p.append('Dense %s' % dense)\n    \n    \n\nplt.legend(p);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3cbc42cc-8f6c-1dc5-2e76-25f43cd75bfc"},"outputs":[],"source":"a,b,c,model=findMin(32,[0,0],1500)\nmodel.summary()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"770b2ab6-2e64-1978-9c00-1dcba042c562"},"outputs":[],"source":"# calculate predictions\npredictions = model.predict(X_test)\n\nstatus = [ int(np.round(i)) == int(np.round(j)) for i,j in zip(predictions.flatten(),y_test)]\nprint(\"correct: \",status.count(True),\"/\",len(status))"},{"cell_type":"markdown","metadata":{"_cell_guid":"c8377d76-4c7a-2786-a4b2-43020b78d75d"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"419f2ed1-90a0-990d-d740-ff5cd98d8ffb"},"outputs":[],"source":"from keras.models import load_model\n\nmodel.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\ndel model  # deletes the existing model\n\n# returns a compiled model\n# identical to the previous one\nmodel = load_model('my_model.h5')\n\n\n# calculate predictions\npredictions = model.predict(X_test)\nstatus = [ int(np.round(i)) == int(np.round(j)) for i,j in zip(predictions.flatten(),y_test)]\nprint(\"correct: \",status.count(True),\"/\",len(status))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}