{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"6fc79172-6e3c-2235-e6e4-475ec06ec402"},"source":"Home values are influenced by many factors. Basically, there are two major aspects:\n\n 1. The environmental information, including location, local economy, school district, air quality, etc.\n 2. The characteristics information of the property, such as lot size, house size and age, the number of rooms, heating / AC systems, garage, and so on.\n\nWhen people consider buying homes, usually the location has been constrained to a certain area such as not too far from the workplace. \n\n**With location factor pretty much fixed, the property characteristics information weights more in the home prices**. \n\nThere are many factors describing the condition of a house, and they do not weigh equally in determining the home value. I present a feature selection process to examine the key features affecting their values.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f09fe65c-d873-bd31-5b59-db7b92b30a34"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"0de4a94c-4c3c-ac57-7f0a-9fb2419f62fd"},"source":"**Combining both the datasets**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7bfa8b4f-fd86-8004-7f51-cc6a229adfbc"},"outputs":[],"source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\n%config InlineBackend.figure_format = 'png' #set 'png' here when working on notebook\nwarnings.filterwarnings('ignore') "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce03b456-628d-d280-2948-20238b48b909"},"outputs":[],"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"e67bffce-e4df-d553-1e12-97349d3283fc"},"source":"**Combining both the datasets**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad49aeb3-8661-08d0-e853-fa76bedd46f6"},"outputs":[],"source":"df = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']), ignore_index=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"fc58bdf1-16fa-9f8f-9d0a-5954e4128c7f"},"source":"**Data Preprocessing**"},{"cell_type":"markdown","metadata":{"_cell_guid":"42ec5d19-1c9f-b03c-e9b1-90a788dce24a"},"source":"**Eliminating samples or features with missing values**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f29cfb6e-a47e-f63d-5f95-b02c8f55cacd"},"outputs":[],"source":"#delete the column without having to reassign df you can do:\ndf.drop('Alley', axis=1, inplace=True)\ndf.drop('PoolQC', axis=1, inplace=True)\ndf.drop('Fence', axis=1, inplace=True)\ndf.drop('MiscFeature', axis=1, inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"91fb09eb-ca60-193d-593d-1f34f22de17e"},"source":"The rest of the data processing is from the kernel by Boris Klyus"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8ce2cb2-599a-7c3a-66af-978e1bd14580"},"outputs":[],"source":"df.loc[df.MasVnrType.isnull(), 'MasVnrType'] = 'None' # no good\ndf.loc[df.MasVnrType == 'None', 'MasVnrArea'] = 0\ndf.loc[df.LotFrontage.isnull(), 'LotFrontage'] = df.LotFrontage.median()\ndf.loc[df.LotArea.isnull(), 'MasVnrType'] = 0\ndf.loc[df.BsmtQual.isnull(), 'BsmtQual'] = 'NoBsmt'\ndf.loc[df.BsmtCond.isnull(), 'BsmtCond'] = 'NoBsmt'\ndf.loc[df.BsmtExposure.isnull(), 'BsmtExposure'] = 'NoBsmt'\ndf.loc[df.BsmtFinType1.isnull(), 'BsmtFinType1'] = 'NoBsmt'\ndf.loc[df.BsmtFinType2.isnull(), 'BsmtFinType2'] = 'NoBsmt'\ndf.loc[df.BsmtFinType1=='NoBsmt', 'BsmtFinSF1'] = 0\ndf.loc[df.BsmtFinType2=='NoBsmt', 'BsmtFinSF2'] = 0\ndf.loc[df.BsmtFinSF1.isnull(), 'BsmtFinSF1'] = df.BsmtFinSF1.median()\ndf.loc[df.BsmtQual=='NoBsmt', 'BsmtUnfSF'] = 0\ndf.loc[df.BsmtUnfSF.isnull(), 'BsmtUnfSF'] = df.BsmtUnfSF.median()\ndf.loc[df.BsmtQual=='NoBsmt', 'TotalBsmtSF'] = 0\ndf.loc[df.FireplaceQu.isnull(), 'FireplaceQu'] = 'NoFireplace'\ndf.loc[df.GarageType.isnull(), 'GarageType'] = 'NoGarage'\ndf.loc[df.GarageFinish.isnull(), 'GarageFinish'] = 'NoGarage'\ndf.loc[df.GarageQual.isnull(), 'GarageQual'] = 'NoGarage'\ndf.loc[df.GarageCond.isnull(), 'GarageCond'] = 'NoGarage'\ndf.loc[df.BsmtFullBath.isnull(), 'BsmtFullBath'] = 0\ndf.loc[df.BsmtHalfBath.isnull(), 'BsmtHalfBath'] = 0\ndf.loc[df.KitchenQual.isnull(), 'KitchenQual'] = 'TA'\ndf.loc[df.MSZoning.isnull(), 'MSZoning'] = 'RL'\ndf.loc[df.Utilities.isnull(), 'Utilities'] = 'AllPub'\ndf.loc[df.Exterior1st.isnull(), 'Exterior1st'] = 'VinylSd'\ndf.loc[df.Exterior2nd.isnull(), 'Exterior2nd'] = 'VinylSd'\ndf.loc[df.Functional.isnull(), 'Functional'] = 'Typ'\ndf.loc[df.SaleCondition.isnull(), 'SaleCondition'] = 'Normal'\ndf.loc[df.SaleCondition.isnull(), 'SaleType'] = 'WD'\ndf.loc[df['Electrical'].isnull(), 'Electrical'] = 'SBrkr'\ndf.loc[df['SaleType'].isnull(), 'SaleType'] = 'NoSale'\n#GarageYrBlt\ndf.loc[df.GarageYrBlt.isnull(), 'GarageYrBlt'] = df.GarageYrBlt.median()\n# only one is null and it has type Detchd\ndf.loc[df['GarageArea'].isnull(), 'GarageArea'] = df.loc[df['GarageType']=='Detchd', 'GarageArea'].mean()\ndf.loc[df['GarageCars'].isnull(), 'GarageCars'] = df.loc[df['GarageType']=='Detchd', 'GarageCars'].median()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4bd2c4a2-5478-2e01-bbf5-a17f59eeea6a"},"source":"**Handling categorical data**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ed75999-fe07-c4b4-d8ab-6dad2aa75f67"},"outputs":[],"source":"size_mapping = {'Y': 1,'N': 0}\ndf['CentralAir'] = df['CentralAir'].map(size_mapping)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54224d2f-211b-95d3-274e-b6376aa5e372"},"outputs":[],"source":"df = pd.get_dummies(df)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f81292b-b968-7472-fecb-ce5d90ec34f8"},"outputs":[],"source":"#log transform the target:\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7ce33c6-9678-e491-de5e-e0c497bc65ba"},"outputs":[],"source":"#creating matrices for sklearn:\nX_train = df[:train.shape[0]]\nX_test = df[train.shape[0]:]\ny = train.SalePrice"},{"cell_type":"markdown","metadata":{"_cell_guid":"327e708c-56ad-99c4-0abc-6fae176ee211"},"source":"**Partitioning a dataset in training and test sets**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b05098b-7cdf-5632-3a05-ca0366da2d8f"},"outputs":[],"source":"from sklearn.cross_validation import train_test_split\nX = df[:train.shape[0]].values\ny = train.SalePrice.values\nX_train, X_test, y_train, y_test = \\\ntrain_test_split(X, y, test_size=0.3, random_state=0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5f4bafe9-f8f2-5613-c05b-bc67a330196f"},"source":"**Bringing features onto the same scale**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"908bc5ff-5f40-993f-17cb-070a48163402"},"outputs":[],"source":"from sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(X_train)\nX_test_std = stdsc.transform(X_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1c6df122-9f0a-23aa-4fcb-c6f5122989ca"},"source":"The issue is that after your scaling step, the labels are float-valued, which is not a valid label-type; you convert to int or str for the y_train and y_test to work"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e04fbfb-d79d-5f06-b43a-5343ef83efe8"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d19f3d4b-089b-2512-400a-3dd5eedf1d29"},"outputs":[],"source":"y_train = y_train.astype(int)\ny_train\n\ny_test = y_test.astype(int)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a3156ab8-a20d-52a3-7a27-c72be025260f"},"source":"**Selecting meaningful features**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e723d6e-c50b-9bb3-ddc3-e291ff471995"},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(penalty='l1', C=0.1)\nlr.fit(X_train_std, y_train)\nprint('Training accuracy:', lr.score(X_train_std, y_train))\nprint('Test accuracy:', lr.score(X_test_std, y_test))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"917f9527-bdfe-0937-6d31-d152b2351510"},"outputs":[],"source":"lr.intercept_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4329ecd1-a999-4731-5a36-9e566556efbc"},"outputs":[],"source":"lr.coef_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f1d9824-f748-0850-cb7a-3abf0054013c"},"outputs":[],"source":"fig = plt.figure()\nax = plt.subplot(111)\n    \ncolors = ['blue', 'green', 'red', 'cyan', \n         'magenta', 'yellow', 'black', \n          'pink', 'lightgreen', 'lightblue', \n          'gray', 'indigo', 'orange']\n\nweights, params = [], []\nfor c in np.arange(-4, 6):\n    lr = LogisticRegression(penalty='l1', C=10**c, random_state=0)\n    lr.fit(X_train_std, y_train)\n    weights.append(lr.coef_[1])\n    params.append(10**c)\n\nweights = np.array(weights)\n\nfor column, color in zip(range(weights.shape[1]), colors):\n    plt.plot(params, weights[:, column],\n             label=df.columns[column+1],\n             color=color)\nplt.axhline(0, color='black', linestyle='--', linewidth=3)\nplt.xlim([10**(-5), 10**5])\nplt.ylabel('weight coefficient')\nplt.xlabel('C')\nplt.xscale('log')\nplt.legend(loc='upper left')\nax.legend(loc='upper center', \n          bbox_to_anchor=(1.38, 1.03),\n          ncol=1, fancybox=True)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"cffa8425-3741-f3fe-6454-5478bae69920"},"source":"**The resulting plot provides us with further insights about the behavior of L1 regularization.**"},{"cell_type":"markdown","metadata":{"_cell_guid":"62deaacd-801e-4585-91dc-153b9714aca9"},"source":"**Assessing feature importance with random forests**\n\nUsing a random forest, we can measure feature importance as the averaged impurity decrease computed from all decision trees in the forest without making any assumptions whether our data is linearly separable or not.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"50d5bb76-c632-87ad-7f89-eeb4d6da3502"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train_std, y_train)\n\nfeature_imp = pd.DataFrame(model.feature_importances_, index=df.columns, columns=[\"importance\"])\nfeat_imp_20 = feature_imp.sort_values(\"importance\", ascending=False).head(20).index\nfeat_imp_20"},{"cell_type":"markdown","metadata":{"_cell_guid":"28d938f2-d8b5-68b4-55e1-6074ec7612a7"},"source":"**Feature selector that removes all low-variance features.**\n\nThis feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9547f62c-b1d0-cfeb-f30d-999c2099fa91"},"outputs":[],"source":"from sklearn.feature_selection import VarianceThreshold, f_regression, SelectKBest\n\n#Find all features with more than 90% variance in values.\nthreshold = 0.90\nvt = VarianceThreshold().fit(X_train_std)\n\n# Find feature names\nfeat_var_threshold = df.columns[vt.variances_ > threshold * (1-threshold)]\n# select the top 20 \n\nfeat_var_threshold[0:20]"},{"cell_type":"markdown","metadata":{"_cell_guid":"5bb5fe6a-2e33-7984-5138-f88566c7ec48"},"source":"**Univariate feature selection**\n\nUnivariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the transform method."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1bbf7532-b028-e127-ed2e-532f52496c5c"},"outputs":[],"source":"X_scored = SelectKBest(score_func=f_regression, k='all').fit(X_train_std, y_train)\nfeature_scoring = pd.DataFrame({\n        'feature': df.columns,\n        'score': X_scored.scores_\n    })\n\nfeat_scored_20 = feature_scoring.sort_values('score', ascending=False).head(20)['feature'].values\nfeat_scored_20"},{"cell_type":"markdown","metadata":{"_cell_guid":"b68346ae-1a20-db06-e2a2-9469598be8be"},"source":"**Recursive Feature Elimination**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aed45b74-c08d-59dc-0110-8bafbffaeeb7"},"outputs":[],"source":"#Select 20 features from using recursive feature elimination (RFE) with logistic regression model.\nfrom sklearn.feature_selection import RFE\nrfe = RFE(LogisticRegression(), 20)\nrfe.fit(X_train_std, y_train)\n\nfeature_rfe_scoring = pd.DataFrame({\n        'feature': df.columns,\n        'score': rfe.ranking_\n    })\n\nfeat_rfe_20 = feature_rfe_scoring[feature_rfe_scoring['score'] == 1]['feature'].values\nfeat_rfe_20"},{"cell_type":"markdown","metadata":{"_cell_guid":"cb552322-a961-b561-4c37-f92fbcc01a8f"},"source":"**Final feature selection**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ceace3c-b1a2-3fa5-6097-26f902ebb0c4"},"outputs":[],"source":"features = np.hstack([\n        feat_var_threshold[0:20], \n        feat_imp_20,\n        feat_scored_20,\n        feat_rfe_20\n    ])\n\nfeatures = np.unique(features)\nprint('Final features set:\\n')\nfor f in features:\n    print(\"\\t-{}\".format(f))"},{"cell_type":"markdown","metadata":{"_cell_guid":"946d1cdb-95cb-9def-25c9-3675e309325b"},"source":"After feature selection, it looks like my hypothesis is true that the property characteristics information weights more location in the home prices.\n\nWhat do you all think ? Please leave a comment for [me][1]. \n\n\n  [1]: http://piushvaish.com\n\nThanks\nPiush"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}