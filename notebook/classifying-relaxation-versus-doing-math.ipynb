{"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","file_extension":".py","version":"3.5.2"},"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":0,"cells":[{"metadata":{"_uuid":"44fdbe840f3248910ff4e62d13a93ef2bf18ab71","_cell_guid":"3837d085-cbe7-ac01-7f60-8d0357653bfb"},"source":"In this notebook, we'll see if we can use some machine learning to classify whether a subject is relaxing, or doing math problems, using data from our dataset.\n\nAlong the way, we'll learn how to get the subject data we want, and how to turn the raw EEG data into useable feature vectors.\n\nReady?\n\nOK, first, we'll perform a few cleaning steps.\n\n- We'll convert all the timestamps from strings into Python datetimes.\n- We'll convert the lists from strings into np arrays of floats.","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"4d5c38aa8bf32d75cd8533e6173b8149efb0a72d","_cell_guid":"90497b9e-9848-053a-fb3b-f222a2020462"},"source":"import json\nimport pandas as pd\n\ndf = pd.read_csv(\"../input/eeg-data.csv\")\n\n# convert to arrays from strings\ndf.raw_values = df.raw_values.map(json.loads)\ndf.eeg_power = df.eeg_power.map(json.loads)","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"54ad943d2bfd6169785fc821653d6b68a97cacf8","_cell_guid":"e7448f6f-c101-cd28-1f5c-388ac8b21b19"},"source":"Next, we'll grab some subject data. We're interested in the \"relax\" and \"math\" tasks, so we'll need to get the readings with those labels.","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"b8b2bdb7c360e955f0a2bdc7a9872e7b1ed7832f","_cell_guid":"703ccfa2-85ab-8790-8ee4-1f87db15efaa"},"source":"\nrelax = df[df.label == 'relax']\nmath = df[(df.label == 'math1') |\n          (df.label == 'math2') |\n          (df.label == 'math3') |\n          (df.label == 'math4') |\n          (df.label == 'math5') |\n          (df.label == 'math6') |\n          (df.label == 'math7') |\n          (df.label == 'math8') |\n          (df.label == 'math9') |\n          (df.label == 'math10') |\n          (df.label == 'math11') |\n          (df.label == 'math12') ]\n\nlen(relax)\nlen(math)","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"0b4834bb0d94159da23291ddd53a051fda87488d","_cell_guid":"6d33d51d-7a99-3c4b-244a-7d96e27c0e85"},"source":"Now that we have our feature vectors, let's try to build a binary classifier!\n\nAn SVM should do the trick for now. We'll make a `cross_val_svm` convenience method for doing n-fold cross-validation on the data.","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"804825eb06f732bb8960e2187f0dc3beecd7984d","_cell_guid":"2940bc77-b6dc-1f46-1ceb-6d55a2192a0f"},"source":"from sklearn.model_selection import cross_val_score\nfrom sklearn import svm\ndef cross_val_svm (X,y,n):\n    clf = svm.SVC()\n    scores = cross_val_score(clf, X, y, cv=n)\n    return scores                                              ","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"dc3a5a09c91379f130eadaa2d72bd8f73f222712","_cell_guid":"37af2c49-9efc-9646-1e0b-d3a24575ca30"},"source":"We'll also make a `vectors_labels` convenience function to produce an `X` list of vectors and a `y` list of labels, given two lists of vectors as input.","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"b9628dd98340d2c10421afa5702a75e408e300cd","_cell_guid":"80c990d8-c872-42ca-ac75-e97aadeeffd2"},"source":"def vectors_labels (list1, list2):\n    def label (l):\n        return lambda x: l\n    X = list1 + list2\n    y = list(map(label(0), list1)) + list(map(label(1), list2))\n    return X, y","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"9412106459c1d23507101ebb25aaf7797f50ac41","_cell_guid":"b88a69a1-f0ba-cd7d-1009-bf2bcc181bcf"},"source":"OK! Now, let's try the simplest feature vectors that could possibly work: the EEG power arrays produced by the Neurosky device.\n\nTo keep things from getting **too** crazy, let's just use data from just one (random) subject for these examples.","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"5646255f98e18abfd8657e26d3257bb64bcc7da6","_cell_guid":"89f34a24-9c7f-11db-344b-0f0bf9b55508"},"source":"one_math = math[math['id']==12]\none_relax = relax[relax['id']==12]\nX, y = vectors_labels(one_math.eeg_power.tolist(), one_relax.eeg_power.tolist())\ncross_val_svm(X,y,7)","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"9e4641aa13bdf4d606ccdcb452dd567e0c7cbcc7","_cell_guid":"98ca765c-9213-90a3-3fbd-b590b3eeb9e4"},"source":"Not so impressive, is it?\n\nLet's build some better feature vectors. Roughly, we take each group of 512 raw values produced by the device, and FFT them to produce a power spectrum. Then, we take groups of 3 power spectra, average them, and logarithmically bin the result to produce feature vectors of 100 values.\n\nI wrote a [blog post](http://blog.cosmopol.is/eeg/2015/06/26/pre-processing-EEG-consumer-devices.html) about this technique, if you're interested in more depth about how these feature vectors work. There'a also a [paper about this](http://people.ischool.berkeley.edu/~chuang/pubs/MMJC15.pdf) if you're into that sort of thing.\n\nOk, feature vector time!","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"fdc0102a718e689f981fa52df8c5ef02352d97d3","_cell_guid":"dc8261d9-4af3-6e6d-6dab-0296c52bdb42"},"source":"from scipy import stats\nfrom scipy.interpolate import interp1d\nimport itertools\nimport numpy as np\n\ndef spectrum (vector):\n    '''get the power spectrum of a vector of raw EEG data'''\n    A = np.fft.fft(vector)\n    ps = np.abs(A)**2\n    ps = ps[:len(ps)//2]\n    return ps\n\ndef binned (pspectra, n):\n    '''compress an array of power spectra into vectors of length n'''\n    l = len(pspectra)\n    array = np.zeros([l,n])\n    for i,ps in enumerate(pspectra):\n        x = np.arange(1,len(ps)+1)\n        f = interp1d(x,ps)#/np.sum(ps))\n        array[i] = f(np.arange(1, n+1))\n    index = np.argwhere(array[:,0]==-1)\n    array = np.delete(array,index,0)\n    return array\n\ndef feature_vector (readings, bins=100): # A function we apply to each group of power spectra\n  '''\n  Create 100, log10-spaced bins for each power spectrum.\n  For more on how this particular implementation works, see:\n  http://coolworld.me/pre-processing-EEG-consumer-devices/\n  '''\n  bins = binned(list(map(spectrum, readings)), bins)\n  return np.log10(np.mean(bins, 0))\n\nex_readings = one_relax.raw_values[:3]\nfeature_vector(ex_readings)\n\ndef grouper(n, iterable, fillvalue=None):\n    \"grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx\"\n    args = [iter(iterable)] * n\n    return itertools.zip_longest(*args, fillvalue=fillvalue)\n\ndef vectors (df):\n    return [feature_vector(group) for group in list(grouper(3, df.raw_values.tolist()))[:-1]]","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"ea2126a497a1544958fe718615ae39fb804fdf8f","_cell_guid":"cc0654d0-3cbe-ff55-bbe2-08992213ad08"},"source":"X,y = vectors_labels(\n    vectors(one_math),\n    vectors(one_relax))\n\ncross_val_svm(X,y,7).mean()","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"efc5131956dd605800b6e6c00cc0fdc701fb2a75","_cell_guid":"f89442b5-9c3f-6228-07ef-3c13f785640d"},"source":"Now that's more like it! I bet we can do even better if we scale the data:","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"242bb8f7198aebbdfb8fd58af52039862e38ab12","_cell_guid":"60613e3f-4631-65bc-9be0-fa10636f33ca"},"source":"from sklearn import preprocessing\nX = preprocessing.scale(X)\ncross_val_svm(X,y,7).mean()","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"57697ff986e01475f8238e81fd5a2ae1f9a029f4","_cell_guid":"58fa934c-f489-5100-914e-18e03f192241"},"source":"Woohoo!\n\nLet's see what kind of accuracy we get classifying relax and math readings for each subject\n in the dataset.","outputs":[],"cell_type":"markdown","execution_count":null},{"metadata":{"_uuid":"7a07d87e969d98aa39cf60fe7c49035468bc1d0c","_cell_guid":"a9da6c26-6333-ce9b-48c6-6bd455a8e5ea"},"source":"def estimated_accuracy (subject):\n    m = math[math['id']==subject]\n    r = relax[relax['id']==subject]\n    X,y = vectors_labels(vectors(m),vectors(r))\n    X=preprocessing.scale(X)\n    return cross_val_svm(X,y,7).mean()\n\n[('subject '+str(subj), estimated_accuracy(subj)) for subj in range(1,31)]","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"fd8165a408a540707fb284b3a7ab9dbaaa558885","_cell_guid":"167d1476-6f70-04a0-69b9-8e0549029a51"},"source":"Not bad! \n\nI wonder why some people are easier to classify than others? Maybe not everyone was paying attention to the math (or not everyone was relaxing during the relax task).\n\nWell, I trust this will be enough to make your own feature vectors, and explore the data yourself. Enjoy!","outputs":[],"cell_type":"markdown","execution_count":null}]}