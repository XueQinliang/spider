{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"65081060-8b1e-fab9-b392-dab5fc0b26b3"},"source":"# This workbook will provide an understanding of how SVM (Support vector machines) work with the iris dataset"},{"cell_type":"markdown","metadata":{"_cell_guid":"ac2bf1b7-53fb-88fc-13dd-f86b4ed52c0c"},"source":"# Import required packages"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c302c84-6423-55bf-32ce-ce0257771137"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"65096f17-c1b9-8baf-14aa-08814a91d65c"},"source":"# Loading dataset\n\n#### We will focus our analysis on 2D datasets. This means that, instead of trying to predict flower classes by using all 4 features, we will analyse separately the sepal and petal information.\n\n#### This is done for visualisation purposes which will enable us to better understand what the algorithm does when performing parameter tuning to it."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80e97848-31aa-be1b-4a0b-f8ad29f17853"},"outputs":[],"source":"from sklearn import datasets\niris = datasets.load_iris()\n\nX1_sepal = iris.data[:,[0,1]]\nX2_petal = iris.data[:,[2,3]]\ny = iris.target\n\nprint(X1_sepal[1:5,:])\nprint(X2_petal[1:5,:])\nprint(y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4a982aec-03d3-08bf-b5ba-876bc859fb84"},"source":"# Visualising the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"372eb1b7-4cc4-1abb-0368-2aa41e1cb14c"},"outputs":[],"source":"plt.figure(figsize=(15, 5))\n\nplt.subplot(1,2,1)\nplt.scatter(X1_sepal[:, 0], X1_sepal[:, 1], c=y)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.subplot(1,2,2)\nplt.scatter(X2_petal[:, 0], X2_petal[:, 1], c=y)\nplt.xlabel('Petal length')\nplt.ylabel('Petal width')"},{"cell_type":"markdown","metadata":{"_cell_guid":"a636ff82-60be-e02a-2a9c-068b22111439"},"source":"#### Create function to plot decision regions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d27b0434-7ca0-9be4-6684-6511a9f8af61"},"outputs":[],"source":"from matplotlib.colors import ListedColormap\n\ndef plot_decision_regions(X,y,classifier,test_idx=None,resolution=0.02):\n    \n    # Initialise the marker types and colors\n    markers = ('s','x','o','^','v')\n    colors = ('red','blue','lightgreen','gray','cyan')\n    color_Map = ListedColormap(colors[:len(np.unique(y))]) #we take the color mapping correspoding to the \n                                                            #amount of classes in the target data\n    \n    # Parameters for the graph and decision surface\n    x1_min = X[:,0].min() - 1\n    x1_max = X[:,0].max() + 1\n    x2_min = X[:,1].min() - 1\n    x2_max = X[:,1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),\n                           np.arange(x2_min,x2_max,resolution))\n    \n    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    \n    plt.contour(xx1,xx2,Z,alpha=0.4,cmap = color_Map)\n    plt.xlim(xx1.min(),xx1.max())\n    plt.ylim(xx2.min(),xx2.max())\n    \n    # Plot samples\n    X_test, Y_test = X[test_idx,:], y[test_idx]\n    \n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\n                    alpha = 0.8, c = color_Map(idx),\n                    marker = markers[idx], label = cl\n                   )"},{"cell_type":"markdown","metadata":{"_cell_guid":"02cfcfd1-75c2-8f6b-08a9-06e91fb7481c"},"source":"# Splitting and scaling the dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc62cf26-0d64-e529-9056-7af5d2ff5a6d"},"outputs":[],"source":"from sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n#######################################################################\n## SPLITTING\n\n\nX_train_sepal, X_test_sepal, y_train_sepal, y_test_sepal = train_test_split(X1_sepal,y,test_size=0.3,random_state=0)\n\nprint(\"# training samples sepal: \", len(X_train_sepal))\nprint(\"# testing samples sepal: \", len(X_test_sepal))\n\nX_train_petal, X_test_petal, y_train_petal, y_test_petal = train_test_split(X2_petal,y,test_size=0.3,random_state=0)\n\nprint(\"# training samples petal: \", len(X_train_petal))\nprint(\"# testing samples petal: \", len(X_test_petal))\n\n#####################################################################\n## SCALING\n\nsc = StandardScaler()\nX_train_sepal_std = sc.fit_transform(X_train_sepal)\nX_test_sepal_std = sc.transform(X_test_sepal)\n\nsc = StandardScaler()\nX_train_petal_std = sc.fit_transform(X_train_petal)\nX_test_petal_std = sc.transform(X_test_petal)\n\n#####################################################################\n## COMBINING FOR FUTURE PLOTTING\n\nX_combined_sepal_standard = np.vstack((X_train_sepal_std,X_test_sepal_std))\nY_combined_sepal = np.hstack((y_train_sepal, y_test_sepal))\n\nX_combined_petal_standard = np.vstack((X_train_petal_std,X_test_petal_std))\nY_combined_petal = np.hstack((y_train_petal, y_test_petal))"},{"cell_type":"markdown","metadata":{"_cell_guid":"78a46baf-8d09-a126-d776-b3d4be0a863b"},"source":"# 1. SVM with LINEAR kernel\n\n#### SVMs using linear kernel have one important parameter that can be tuned, and this is the inverse of the regularization parameter, which corresponds to C. \n\n#### Let's plot the decision regions by using linear kernels and changing the C parameter."},{"cell_type":"markdown","metadata":{"_cell_guid":"cc7aaf67-e5d4-17ce-56cc-b9d9a02654c9"},"source":"### 1.1. Sepal decision regions with linear kernel"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e03ac22-d1c2-d662-db22-f3d8a3a2fd29"},"outputs":[],"source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\nC_param_range = [0.01,0.1,1,10,100]\n\nsepal_acc_table = pd.DataFrame(columns = ['C_parameter','Accuracy'])\nsepal_acc_table['C_parameter'] = C_param_range\n\nplt.figure(figsize=(10, 10))\n\nj = 0\n\nfor i in C_param_range:\n    \n    # Apply SVM model to training data\n    svm_linear =  SVC(kernel = 'linear', C = i, random_state = 0)\n    svm_linear.fit(X_train_sepal_std,y_train_sepal)\n    \n    # Predict using model\n    y_pred_sepal = svm_linear.predict(X_test_sepal_std)\n    \n    # Saving accuracy score in table\n    sepal_acc_table.iloc[j,1] = accuracy_score(y_test_sepal,y_pred_sepal)\n    j += 1\n    \n    # Printing decision regions\n    plt.subplot(3,2,j)\n    plt.subplots_adjust(hspace = 0.4)\n    plot_decision_regions(X = X_combined_sepal_standard\n                      , y = Y_combined_sepal\n                      , classifier = svm_linear\n                      , test_idx = range(105,150))\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.title('Linear Kernel using C = %s'%i)\n    \nprint(sepal_acc_table)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4ef43663-900f-892b-498a-f46253f0fb85"},"source":"### 1.2. Petal decision regions with linear kernel"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c55c3bd-cd20-c03a-09a3-a17104fa79be"},"outputs":[],"source":"petal_acc_table = pd.DataFrame(columns = ['C_parameter','Accuracy'])\npetal_acc_table['C_parameter'] = C_param_range\n\nplt.figure(figsize=(10, 10))\n\nj = 0\n\nfor i in C_param_range:\n    \n    # Apply SVM model to training data\n    svm_linear =  SVC(kernel = 'linear', C = i, random_state = 0)\n    svm_linear.fit(X_train_petal_std,y_train_petal)\n    \n    # Predict using model\n    y_pred_petal = svm_linear.predict(X_test_petal_std)\n    \n    # Saving accuracy score in table\n    petal_acc_table.iloc[j,1] = accuracy_score(y_test_petal,y_pred_petal)\n    j += 1\n    \n    # Printing decision regions\n    plt.subplot(3,2,j)\n    plt.subplots_adjust(hspace = 0.4)\n    plot_decision_regions(X = X_combined_petal_standard\n                      , y = Y_combined_petal\n                      , classifier = svm_linear\n                      , test_idx = range(105,150))\n    plt.xlabel('Petal length')\n    plt.ylabel('Petal width')\n    plt.title('Linear Kernel using C = %s'%i)\n    \nprint(petal_acc_table)"},{"cell_type":"markdown","metadata":{"_cell_guid":"51c4aff6-10cd-924d-eccb-2868613c979d"},"source":"### 1.3. Validation curves for both datasets"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"619dcc80-b74f-dc6b-ac9e-1c585bcaf069"},"outputs":[],"source":"from sklearn.learning_curve import validation_curve\n\nC_param_range = [0.01,0.1,1,10,100,1000]\n\nplt.figure(figsize=(15, 10))\n\n# SEPAL Plot validation curve\ntrain_sepal_scores, test_sepal_scores = validation_curve(estimator=svm_linear\n                                                            ,X=X_combined_sepal_standard\n                                                            ,y=Y_combined_sepal\n                                                            ,param_name='C'\n                                                            ,param_range=C_param_range\n                                                            ,scoring='accuracy'\n                                                            )\n\n\ntrain_sepal_mean = np.mean(train_sepal_scores,axis=1)\ntrain_sepal_std = np.std(train_sepal_scores,axis=1)\ntest_sepal_mean = np.mean(test_sepal_scores,axis=1)\ntest_sepal_std = np.std(test_sepal_scores,axis=1)\n\nbest_C_table_sepal = pd.DataFrame(columns = ['C_parameter','Train_scores','Test_scores','Difference'])\nbest_C_table_sepal['C_parameter'] = C_param_range\nbest_C_table_sepal['Train_scores'] = train_sepal_mean\nbest_C_table_sepal['Test_scores'] = test_sepal_mean\nbest_C_table_sepal['Difference'] = best_C_table_sepal['Train_scores'] - best_C_table_sepal['Test_scores']\n\nprint(best_C_table_sepal)\n\nplt.subplot(2,2,1)\nplt.plot(C_param_range\n            ,train_sepal_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\n    \nplt.plot(C_param_range\n            ,test_sepal_mean\n            ,color='green'\n            ,marker='o'\n            ,markersize=5\n            ,label='test accuracy') \n    \nplt.xlabel('C_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0,1])\n\n# PETAL Plot validation curve\ntrain_petal_scores, test_petal_scores = validation_curve(estimator=svm_linear\n                                                            ,X=X_combined_petal_standard\n                                                            ,y=Y_combined_petal\n                                                            ,param_name='C'\n                                                            ,param_range=C_param_range\n                                                            ,scoring='accuracy'\n                                                            )\n\n\ntrain_petal_mean = np.mean(train_petal_scores,axis=1)\ntrain_petal_std = np.std(train_petal_scores,axis=1)\ntest_petal_mean = np.mean(test_petal_scores,axis=1)\ntest_petal_std = np.std(test_petal_scores,axis=1)\n\nbest_C_table_petal = pd.DataFrame(columns = ['C_parameter','Train_scores','Test_scores','Difference'])\nbest_C_table_petal['C_parameter'] = C_param_range\nbest_C_table_petal['Train_scores'] = train_petal_mean\nbest_C_table_petal['Test_scores'] = test_petal_mean\nbest_C_table_petal['Difference'] = best_C_table_petal['Train_scores'] - best_C_table_petal['Test_scores']\n\nprint(best_C_table_petal)\n\nplt.subplot(2,2,2)\nplt.plot(C_param_range\n            ,train_petal_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\n    \nplt.plot(C_param_range\n            ,test_petal_mean\n            ,color='green'\n            ,marker='o'\n            ,markersize=5\n            ,label='test accuracy') \n    \nplt.xlabel('C_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0,1])"},{"cell_type":"markdown","metadata":{"_cell_guid":"a03d9b0c-a1cf-34aa-7eb1-eaa9b1690b84"},"source":"# 2. SVM with POLYNOMIAL kernel\n\n#### In this case we can tune both the C parameter and the polynomial order of the kernel. Let's start simple, and test how the polynomial order affects the classification by keeeping the C parameter constant and = 1 (no regularization applied)."},{"cell_type":"markdown","metadata":{"_cell_guid":"1efac8b0-60ac-962a-af45-5ef72f36ee58"},"source":"### 2.1 Sepal decision regions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a28c776d-0c20-7bbe-60f1-b2ce47dab1f5"},"outputs":[],"source":"polynomial_degree_range = [1,2,3,4,5,6] \n\nsepal_acc_table = pd.DataFrame(columns = ['degree','Accuracy'])\nsepal_acc_table['degree'] = polynomial_degree_range\n\nplt.figure(figsize=(10, 10))\n\nj = 0\n\nfor i in polynomial_degree_range:\n    \n    # Apply SVM model to training data\n    svm_poly =  SVC(kernel = 'poly', degree = i, C = 1, random_state = 0)\n    svm_poly.fit(X_train_sepal_std,y_train_sepal)\n    \n    # Predict using model\n    y_pred_sepal = svm_poly.predict(X_test_sepal_std)\n    \n    # Saving accuracy score in table\n    sepal_acc_table.iloc[j,1] = accuracy_score(y_test_sepal,y_pred_sepal)\n    j += 1\n    \n    # Printing decision regions\n    plt.subplot(3,2,j)\n    plt.subplots_adjust(hspace = 0.4)\n    plot_decision_regions(X = X_combined_sepal_standard\n                      , y = Y_combined_sepal\n                      , classifier = svm_poly\n                      )\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.title('Polynomial Kernel using degree = %s'%i)\n    \nprint(sepal_acc_table)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e8b1f251-2264-8e79-3e85-01eb89a81a6f"},"source":"### 2.2 Petal decision regions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bfb60b5b-724a-9a99-b4a4-11364af7e104"},"outputs":[],"source":"polynomial_degree_range = [1,2,3,4,5,6] \n\npetal_acc_table = pd.DataFrame(columns = ['degree','Accuracy'])\npetal_acc_table['degree'] = polynomial_degree_range\n\nplt.figure(figsize=(10, 10))\n\nj = 0\n\nfor i in polynomial_degree_range:\n    \n    # Apply SVM model to training data\n    svm_poly =  SVC(kernel = 'poly', degree = i, C = 1, random_state = 0)\n    svm_poly.fit(X_train_petal_std,y_train_petal)\n    \n    # Predict using model\n    y_pred_petal = svm_poly.predict(X_test_petal_std)\n    \n    # Saving accuracy score in table\n    petal_acc_table.iloc[j,1] = accuracy_score(y_test_petal,y_pred_petal)\n    j += 1\n    \n    # Printing decision regions\n    plt.subplot(3,2,j)\n    plt.subplots_adjust(hspace = 0.4)\n    plot_decision_regions(X = X_combined_petal_standard\n                      , y = Y_combined_petal\n                      , classifier = svm_poly\n                      , test_idx = range(105,150))\n    plt.xlabel('Petal length')\n    plt.ylabel('Petal width')\n    plt.title('Polynomial Kernel using degree = %s'%i)\n    \nprint(petal_acc_table)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c53666a6-178f-893f-0e17-2a046217039c"},"source":"# 3. SVM with RADIAL BASIS FUNCTION kernel"},{"cell_type":"markdown","metadata":{"_cell_guid":"7f7a6c82-e536-ff57-a2b0-ab5d81aaa1bd"},"source":"### 3.1 Sepal decision regions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a52f5382-3e0d-23e8-2f37-57ea9bfebce1"},"outputs":[],"source":"rbf_degree_range = [1,2,3,4,5,6]\n\nsepal_acc_table = pd.DataFrame(columns = ['degree','Accuracy'])\nsepal_acc_table['degree'] = rbf_degree_range\n\nplt.figure(figsize=(10, 10))\n\nj = 0\n\nfor i in rbf_degree_range:\n    \n    # Apply SVM model to training data\n    svm_rbf =  SVC(kernel = 'rbf', degree = i, C = 1, random_state = 0)\n    svm_rbf.fit(X_train_sepal_std,y_train_sepal)\n    \n    # Predict using model\n    y_pred_sepal = svm_rbf.predict(X_test_sepal_std)\n    \n    # Saving accuracy score in table\n    sepal_acc_table.iloc[j,1] = accuracy_score(y_test_sepal,y_pred_sepal)\n    j += 1\n    \n    # Printing decision regions\n    plt.subplot(3,2,j)\n    plt.subplots_adjust(hspace = 0.4)\n    plot_decision_regions(X = X_combined_sepal_standard\n                      , y = Y_combined_sepal\n                      , classifier = svm_rbf\n                      )\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.title('rbf Kernel using degree = %s'%i)\n    \nprint(sepal_acc_table)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a1674be5-9d43-5c6c-c9b1-276abb542be1"},"source":"### 3.2. Petal decision regions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"afec08d0-e3e9-1f21-2bec-8dc890c59398"},"outputs":[],"source":"rbf_degree_range = [1,2,3,4,5,6] \n\npetal_acc_table = pd.DataFrame(columns = ['degree','Accuracy'])\npetal_acc_table['degree'] = rbf_degree_range\n\n\n\nplt.figure(figsize=(10, 10))\n\nj = 0\n\nfor i in rbf_degree_range:\n    \n    # Apply SVM model to training data\n    svm_rbf =  SVC(kernel = 'rbf', degree = i, C = 1, random_state = 0)\n    svm_rbf.fit(X_train_petal_std,y_train_petal)\n    \n    # Predict using model\n    y_pred_petal = svm_rbf.predict(X_test_petal_std)\n    \n    # Saving accuracy score in table\n    petal_acc_table.iloc[j,1] = accuracy_score(y_test_petal,y_pred_petal)\n    j += 1\n    \n    # Printing decision regions\n    plt.subplot(3,2,j)\n    plt.subplots_adjust(hspace = 0.4)\n    plot_decision_regions(X = X_combined_petal_standard\n                      , y = Y_combined_petal\n                      , classifier = svm_rbf\n                      , test_idx = range(105,150))\n    plt.xlabel('Petal length')\n    plt.ylabel('Petal width')\n    plt.title('RBF Kernel using degree = %s'%i)\n    \nprint(petal_acc_table)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"924ead01-5d46-5ab4-314d-ca16885075f5"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}