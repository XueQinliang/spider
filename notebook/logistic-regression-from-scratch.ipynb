{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# reading the csv file, del 2 columns from the file, checking first few rows of the file\ndata = pd.read_csv('../input/Social_Network_Ads.csv')\ndata.drop(columns=['User ID','Gender',],axis=1,inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cb45e28344e7e245ab398e9f4f5272ef21d2129"},"cell_type":"code","source":"#Declare label as last column in the source file\ny = data.iloc[:,-1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e7a145fa49435ad9578ec2827f76a70cc99f2e1"},"cell_type":"code","source":"#Declaring X as all columns excluding last\nX = data.iloc[:,:-1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dffb1f3e19e19964995ac827bf55108b5815ff67"},"cell_type":"code","source":"# Splitting data\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d4ed14782e114ae3282f20d3754121398e6d232"},"cell_type":"code","source":"# Sacaling data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ff7415e3e0e0673d59051cfe6154c63d3312a32"},"cell_type":"code","source":"#Variabes to calculate sigmoid function\ny_pred = []\nlen_x = len(X_train[0])\nw = []\nb = 0.2\nprint(len_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a228174207f4631be4f26a0cc05e379f3f58aa56"},"cell_type":"code","source":"entries = len(X_train[:,0])\nentries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d4d6e47ee65c9c7404e60fcf8f05c11708546b3"},"cell_type":"code","source":"for weights in range(len_x):\n    w.append(0)\nw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18dbd2196d72527a82d30ab88ed2aa8d10bd01ce"},"cell_type":"code","source":"# Sigmoid function\ndef sigmoid(z):\n    return (1/(1+np.exp(-z)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daa0f87fdbf98591cb9f51b8dc7157dc399ca827"},"cell_type":"code","source":"\ndef predict(inputs):\n    z = np.dot(w,inputs)+b\n    a = sigmoid(z)\n    return a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4126f842d072ccd40019cc283b767a014e2ee074"},"cell_type":"code","source":"#Loss function\ndef loss_func(y,a):\n    J = -(y*np.log(a) + (1-y)*np.log(1-a))\n    return J         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc0ceb65c69f4ee0c3f28e050744229dc90c621b"},"cell_type":"code","source":"dw = []\ndb = 0\nJ = 0\nalpha = 0.1\nfor x in range(len_x):\n    dw.append(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4be38e9b500ae0c5a7134296a3055675c4fb2d8"},"cell_type":"code","source":"#Repeating this process 1000 times\nfor iterations in range(1000):\n    for i in range(entries):\n        localx = X_train[i]\n        a = predict(localx)   \n        dz = a - y_train[i]\n        J += loss_func(y_train[i],a)\n        for j in range(len_x):\n            dw[j] = dw[j]+(localx[j]*dz)\n        db += dz\n    J = J/entries\n    db = db/entries\n    for x in range(len_x):\n        dw[x]=dw[x]/entries\n    for x in range(len_x):\n        w[x] = w[x]-(alpha*dw[x])\n    b = b-(alpha*db)         \n    J=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5479ccb6073ed1ea310ef7de01b2935fc3ec400e"},"cell_type":"code","source":"#Print weight\nprint(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a939c247b8a092f74c9843975612daa85c423621"},"cell_type":"code","source":"#print intercept\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7ae24169a21c7ac8ea0787f4a38a0de3e07a6b5"},"cell_type":"code","source":"#predicting the label\nfor x in range(len(y_test)):\n    y_pred.append(predict(X_test[x]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"967ad1b72305ad792a5d50e4d8b8a07632f7b241","scrolled":false},"cell_type":"code","source":"#print actual and predicted values in a table\nfor x in range(len(y_pred)):\n    print('Actual ',y_test[x],' Predicted ',y_pred[x])\n    if y_pred[x]>=0.5:\n        y_pred[x]=1\n    else:\n        y_pred[x]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a59807150900082ab876ef0200c6c7f8f93e098c"},"cell_type":"code","source":"# Calculating accuracy of prediction\ncount = 0\nfor x in range(len(y_pred)):\n    if(y_pred[x]==y_test[x]):\n        count=count+1\nprint('Accuracy:',(count/(len(y_pred)))*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"64fc1557c2b8f2e76e66a84d0b1e5419a692e395"},"cell_type":"code","source":"#Adding the code for confusion matrix which is the best model performace measure for Classification Algorithms\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9aaade066015e04f20dd7eb1d37339be75ca3836","_kg_hide-output":true},"cell_type":"code","source":"# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifierLR = LogisticRegression(random_state = 0)\nclassifierLR.fit(X_train, y_train)\n\n#predicting the test label with LR. Predict always takes X as input\ny_predLR=classifierLR.predict(X_test)\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifierLR.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifierLR.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n#Comparing logistic regression and KNN results\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nknn.fit(X_train, y_train)\n\n#predicting the test label with KNN. Predict always takes X as input\ny_predknn=knn.predict(X_test)\n\n# Making the Confusion Matrix for KNN\nfrom sklearn.metrics import confusion_matrix\ncmknn = confusion_matrix(y_test, y_predknn)\n\n# Visualising the Training set results for KNN\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('K-NN (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results for KNN\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('K-NN (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Fitting SVM to the Training set\nfrom sklearn.svm import SVC\nclassifiersvc = SVC(kernel = 'linear', random_state = 0)\nclassifiersvc.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_predsvc = classifiersvc.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncmsvc = confusion_matrix(y_test, y_predsvc)\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifiersvc.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('SVM (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifiersvc.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('SVM (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Fitting Kernel SVM to the Training set\nfrom sklearn.svm import SVC\nclassifierksvm = SVC(kernel = 'rbf', random_state = 0)\nclassifierksvm.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_predsvm = classifierksvm.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncmksvm = confusion_matrix(y_test, y_pred)\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifierksvm.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Kernel SVM (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifierksvm.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Kernel SVM (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifierNB = GaussianNB()\nclassifierNB.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_predNB = classifierNB.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncmNB = confusion_matrix(y_test, y_pred)\n\n#Alternative for confusion matrix\nTruePositive = 0  #True results correctly classified(Will buy) \nTrueNegative = 0  #False results correctly classified(Will not buy)\nfalsePositive = 0 #False Positive\nfalseNegative = 0 #False Negative\nfor x in range(len(y_pred)):\n    if(y_pred[x]==1 and y_test[x]==1):\n        TruePositive += 1\n    elif (y_pred[x]==0 and y_test[x]==0):\n        TrueNegative += 1\n    elif(y_pred[x]==0 and y_test[x]==1):\n        falsePositive += 1\n    else: \n        falseNegative += 1\nprint('Count of correctly predicted results :',TruePositive+TrueNegative)\nprint('Count of false positives :',falsePositive)\nprint('Count of false negatives :',falseNegative)\n# We can calculate same values using confusion matrix as above\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifierNB.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Naive Bayes (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifierNB.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Naive Bayes (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Fitting Decision Tree Classification to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifierDT = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifierDT.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_predDT = classifierDT.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncmDT = confusion_matrix(y_test, y_predDT)\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifierDT.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Decision Tree Classification (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifierDT.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Decision Tree Classification (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifierRF = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifierRF.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_predRF = classifierRF.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncmRF = confusion_matrix(y_test, y_pred)\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifierRF.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifierRF.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}