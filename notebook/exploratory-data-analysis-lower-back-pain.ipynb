{"cells":[{"metadata":{"_uuid":"00354d39c00e866951b46b7be6bb4f02602f6277"},"cell_type":"markdown","source":"# Lower Back Pain\n\n[Lower back pain](https://www.healthline.com/health/back-pain), also called __lumbago__, is not a disorder. It¡¯s a symptom of several different types of medical problems. It usually results from a problem with one or more parts of the lower back, such as:\n* ligaments\n* muscles\n* nerves\n* the bony structures that make up the spine, called vertebral bodies or vertebrae\n\nLower back pain can also be due to a problem with nearby organs, such as the kidneys.\n\nAccording to the American Association of Neurological Surgeons, 75 to 85 percent of Americans will experience back pain in their lifetime. Of those, 50 percent will have more than one episode within a year. In 90 percent of all cases, the pain gets better without surgery. Talk to your doctor if you¡¯re experiencing back pain. \n\nIn this [Exploratory Data Analysis (EDA)](https://en.wikipedia.org/wiki/Exploratory_data_analysis) I am going to use the Lower Back Pain Symptoms Dataset and try to find out interesting insights of this dataset. \n\n\n## Dataset Description\nThis dataset contains:\n* 310 Observations\n* 12 Features \n* 1 Lebel\n\n|__ col. no.__| __Attribute name__| __type__| \n|-------------|---------------------|-----------|\n| Col1 | pelvic_incidence | numeric, float64|\n|Col2|pelvic_tilt |numeric, float64 |\n|Col3| lumbar_lordosis_angle |numeric, float64|\n|Col4|sacral_slope|numeric, float64|\n|Col5| pelvic_radius  |numeric, float64|\n|Col6|degree_spondylolisthesis   |numeric, float64|\n|Col7| pelvic_slope |numeric, float64|\n|Col8|Direct_tilt  |numeric, float64|\n|Col9| thoracic_slope |numeric, float64|\n|Col10|  cervical_tilt |numeric, float64|\n|Col11| sacrum_angle |numeric, float64|\n|Col12| scoliosis_slope |numeric, float64|\n|Class_att| Attribute Class | categorical, object|\n"},{"metadata":{"_uuid":"9e76b6c543abb2dd71fcab593121406b4e40b064"},"cell_type":"markdown","source":"## EDA on Lower Back Pain Symptoms Dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier, plot_importance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/Dataset_spine.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b4171a6ce72ef0e39b01b6091a1e2ced5765024"},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"662ff00acdff2b50e66a5a69e58ff51cd3dfb777"},"cell_type":"code","source":"# Unnecessary column\ndataset.iloc[:,-1:].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87e2e68452f9dbfe7b9ae8731d1c6ee471f74ee0"},"cell_type":"code","source":"# removing Unnecessary column\ndel dataset[\"Unnamed: 13\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31f1fbc27759c7d02fe05501cbc873aca208dc85"},"cell_type":"markdown","source":"## Full Dataset Summary  \n[DataFrame.describe()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html) method generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset¡¯s distribution, excluding `NaN` values.  This method tells us a lot of things about a dataset. One important thing is that the `describe()` method deals only with numeric values. It doesn't work with any categorical values. \n\nNow, let's understand the statistics that are generated by the `describe()` method:\n\n* `count` tells us the number of `NoN-empty` rows in a feature.\n\n* `mean` tells us the mean value of that feature.\n\n* `std` tells us the Standard Deviation Value of that feature.\n\n* `min` tells us the minimum value of that feature.\n\n* `25%`, `50%`, and `75%` are the percentile/quartile of each features. This quartile information helps us to detect [Outliers](https://machinelearningmastery.com/how-to-identify-outliers-in-your-data/).\n\n* `max` tells us the maximum value of that feature.\n"},{"metadata":{"trusted":true,"_uuid":"6ebbb549ee783346a38dd7951f3bf37c84aabf35"},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6465e30c34b06f2eb2c83f666359aadcebab31c4"},"cell_type":"code","source":"# Change the Column names\ndataset.rename(columns = {\n    \"Col1\" : \"pelvic_incidence\", \n    \"Col2\" : \"pelvic_tilt\",\n    \"Col3\" : \"lumbar_lordosis_angle\",\n    \"Col4\" : \"sacral_slope\", \n    \"Col5\" : \"pelvic_radius\",\n    \"Col6\" : \"degree_spondylolisthesis\", \n    \"Col7\" : \"pelvic_slope\",\n    \"Col8\" : \"direct_tilt\",\n    \"Col9\" : \"thoracic_slope\", \n    \"Col10\" :\"cervical_tilt\", \n    \"Col11\" : \"sacrum_angle\",\n    \"Col12\" : \"scoliosis_slope\", \n    \"Class_att\" : \"class\"}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"478b1e1f1ca9061a0e22a9de067f794b782ab11a"},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b883fba2abdefd2d82c0dddd17e0ca4b39f0a691"},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6759910f63d39ca9df0d49e42446cb437cc72c8"},"cell_type":"markdown","source":"[DataFrame.info()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html) prints information about a DataFrame including the `index` dtype and `column` dtypes, `non-null` values and memory usage. We can use the `info()` to know whether a dataset contains any missing value or not."},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"a5bfb433a248f43dc9ba78299f1ca916738e9d94"},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e07d5c08277d3dc845451de3bf1d0cd9b7d73978"},"cell_type":"markdown","source":"### Visualize the number of abnormal and normal cases \nThe tendency of `abnormal` cases is 2 times higher than the `normal` cases."},{"metadata":{"trusted":true,"_uuid":"dec287948fdc30c45aaa1d8df6ce1dabd79d218b"},"cell_type":"code","source":"dataset[\"class\"].value_counts().sort_index().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b2ef681daf92e9a9ed61cd0fd4f51e9b0744109"},"cell_type":"markdown","source":"## Correlation between features\nA [correlation coefficient](https://en.wikipedia.org/wiki/Correlation_coefficient) is a numerical measure of some type of correlation, meaning a statistical relationship between two variables."},{"metadata":{"trusted":true,"_uuid":"1e1ada2ce9eb83d6b4b94dd5b44945795db4a9f6"},"cell_type":"code","source":"plt.subplots(figsize=(12,8))\nsns.heatmap(dataset.corr())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"623bba71328d682c6ad633bd2b9df9c61f121d66"},"cell_type":"markdown","source":"## Custom correlogram\nA [pair plot](https://seaborn.pydata.org/generated/seaborn.pairplot.html) allows us to see both distribution of single variables and relationships between two variables.\n\nLots of things are going on in the below pair plot. Let¡¯s try to understand the __pair plot__. In __pair plot__, there are mainly two things that we need to understand. One is the __distribution of a feature__ and another is the __relationship between one feature to all others__. If we look at the diagonal we can see the distribution of each feature. Let¡¯s consider the `first row X first column`, this diagonal shows us the distribution of `pelvic_incidence`. Similarly, if we look at the `second row X second column` diagonal we can see the distribution of `pelvic_tilt`. All the cells except the diagonals show the relationship between one feature to another. Let¡¯s consider the `first row X second column`, here we can the relationship between `pelvic_incidence` and `pelvic_tilt`.\n\n"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"420273defce48266377e3148e164960d0f924368"},"cell_type":"code","source":"sns.pairplot(dataset, hue=\"class\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"767fb02ab08e2cd4307efb87a1f4310ec937c268"},"cell_type":"markdown","source":"## Histogram of Each Feature\nA [Histogram](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html) is the most commonly used graph to show frequency distributions."},{"metadata":{"trusted":true,"_uuid":"2732a9012cfa7a2e041f8fed0346403f2bed42a8"},"cell_type":"code","source":"dataset.hist(figsize=(15,12),bins = 20, color=\"#007959AA\")\nplt.title(\"Features Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b455f2a2fcca39ecf4f33053cfc67efeff996275"},"cell_type":"markdown","source":"## Detecting and Removing Outliers"},{"metadata":{"trusted":true,"_uuid":"960c43c1b31391d2cff40d1e915719586d4a85e7"},"cell_type":"code","source":"plt.subplots(figsize=(15,6))\ndataset.boxplot(patch_artist=True, sym=\"k.\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c24e4503a214106a88825d906b2e26bdd8f8a0cf"},"cell_type":"markdown","source":"### Detect and Remove Outliers by hand"},{"metadata":{"trusted":true,"_uuid":"6d7a55bd4ee2822bddd84ac1c1f476080939eac0"},"cell_type":"code","source":"# detecting Outlier\n# Inter Quartile Range is the distance between the 3rd Quartile and the first Qartile\n\nminimum = 0\nmaximum = 0\n\ndef detect_outlier(feature):\n    first_q = np.percentile(feature, 25)\n    third_q = np.percentile(feature, 75) \n    IQR = third_q - first_q\n    IQR *= 1.5\n    minimum = first_q - IQR \n    maximum = third_q + IQR\n    flag = False\n    \n    if(minimum > np.min(feature)):\n        flag = True\n    if(maximum < np.max(feature)):\n        flag = True\n    \n    return flag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32acbbb39c7e5badf69d4100b5c543304ac33b84"},"cell_type":"code","source":"# we use tukey method to remove outliers.\n# whiskers are set at 1.5 times Interquartile Range (IQR\n\ndef  remove_outlier(feature):\n    first_q = np.percentile(X[feature], 25)\n    third_q = np.percentile(X[feature], 75)\n    IQR = third_q - first_q\n    IQR *= 1.5\n    \n    minimum = first_q - IQR # the acceptable minimum value\n    maximum = third_q + IQR # the acceptable maximum value\n    \n    median = X[feature].median()\n    \n    \"\"\"\n    # any value beyond the acceptance range are considered\n    as outliers. \n    # we replace the outliers with the median value of that \n      feature.\n    \"\"\"\n    \n    X.loc[X[feature] < minimum, feature] = median \n    X.loc[X[feature] > maximum, feature] = median\n\n# taking all the columns except the last one\n# last column is the label\n\nX = dataset.iloc[:, :-1]\nfor i in range(len(X.columns)): \n        remove_outlier(X.columns[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d630412cdd33e3cde8e14d61044df3b861f65fc"},"cell_type":"code","source":"X = dataset.iloc[:, :-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fb4d5bd66d7f922a848b94022d3a045dcc6df88"},"cell_type":"code","source":"for i in range(len(X.columns)):\n    if(detect_outlier(X[X.columns[i]])):\n        print(X.columns[i], \"Contains Outlier\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fa0073b75dce44eeee51d3f84625c7ab7d912ea"},"cell_type":"code","source":"for i in range (3):\n    for i in range(len(X.columns)):\n        remove_outlier(X.columns[i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bd767a52f7e61aef4ed2c4f15e58038c8d8ff7a"},"cell_type":"markdown","source":"### After removing Outliers"},{"metadata":{"trusted":true,"_uuid":"5d77b61588476cd4d9ec9f11855cd0642bb1cde2"},"cell_type":"code","source":"plt.subplots(figsize=(15,6))\nX.boxplot(patch_artist=True, sym=\"k.\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d12d15dbe9171db6041b10990a988a9f9a4d5b31"},"cell_type":"markdown","source":"## Feature Scaling\n[Feature scaling](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html) though standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Our dataset contain features with highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this will create a problem. To avoid this effect, we need to bring all features to the same level of magnitudes. This can be acheived by [feature scaling](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e)."},{"metadata":{"trusted":true,"_uuid":"bf19eb6434ce6fd376d7756255cc014a6ea5800a"},"cell_type":"code","source":"scaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(X)\nscaled_df = pd.DataFrame(data = scaled_data, columns = X.columns)\nscaled_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a68a17824237e79aea287fc713923e41f32c454"},"cell_type":"markdown","source":"## Label Encoding\nCertain algorithms like XGBoost can only have numerical values as their predictor variables. Hence  we need encode our categorical values. \n[LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) from `sklearn.preprocessing` package encode labels with value between 0 and n_classes-1."},{"metadata":{"trusted":true,"_uuid":"e9c45fc3f6e8e33874d7adcfc286b0e2e15c9f0b"},"cell_type":"code","source":"label = dataset[\"class\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abd49bbbb4c46f2eee5faa75852d5632e39c5cdb"},"cell_type":"code","source":"encoder = LabelEncoder()\nlabel = encoder.fit_transform(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0339eff4be8c014327cc2923d8ab67dd10cbe4fd"},"cell_type":"code","source":"X = scaled_df\ny = label \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e097b9e3ccde85035956db8acb8e0a4c0d8085a"},"cell_type":"code","source":"clf_gnb = GaussianNB()\npred_gnb = clf_gnb.fit(X_train, y_train).predict(X_test)\naccuracy_score(pred_gnb, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97036a725ad265c56c45dc63ba2a2387e8320ada"},"cell_type":"code","source":"clf_svc = SVC(kernel=\"linear\")\npred_svc = clf_svc.fit(X_train, y_train).predict(X_test)\naccuracy_score(pred_svc, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be6a1060561fdeb5d72be5df507c45c2c74aa1a9"},"cell_type":"code","source":"clf_xgb =  XGBClassifier()\npred_xgb = clf_xgb.fit(X_train, y_train).predict(X_test)\naccuracy_score(pred_xgb, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"284fd632f98395af65aafb4e533ddc4bbd9b7547"},"cell_type":"code","source":"confusion_matrix(pred_xgb, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6b064edf4a5087e263f173522f219cf53bfd247"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 6))\nplot_importance(clf_xgb, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4351c0490502b789b99dbb46efdcf59da07f3cc4"},"cell_type":"markdown","source":"## Marginal plot\nA [marginal plot](https://python-graph-gallery.com/82-marginal-plot-with-seaborn/) allows to study the relationship between 2 numeric variables. The central chart display their correlation.\n\nLets visualize the relationship between `degree_spondylolisthesis` and `class`."},{"metadata":{"trusted":true,"_uuid":"162a62d5a40a9aaf2c254f5e453165af19fc9084"},"cell_type":"code","source":"sns.set(style=\"white\", color_codes=True)\nsns.jointplot(x=X[\"degree_spondylolisthesis\"], y=label, kind='kde', color=\"skyblue\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d84a43d7383902f861cc5ae4f01a5b27cc595e8b"},"cell_type":"markdown","source":"__Thats all. If you think the kernel is useful, then give a upvote. Cheers :) __"},{"metadata":{"trusted":true,"_uuid":"50ef7df9aef797581e2a674186642d9ad897ca2b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}