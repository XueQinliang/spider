{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"fa1a9421-50b6-7ae4-0b03-212f533f8fcb"},"source":"## TF-DNNRegressor - AllState Claims Severity (v0.2)\n\nThis script show a simple example of using [tf.contrib.learn][1] library to create our model.\n\nThe code is divided in following steps:\n\n - Load CSVs data\n - Filtering Categorical and Continuous features\n - Converting Data into Tensors\n - Selecting and Engineering Features for the Model\n - Defining The Regression Model\n - Training and Evaluating Our Model\n - Predicting output for test data\n\n*v0.1: Added code for data loading, modeling and  prediction model.*\n\n*v0.2: Removed unnecessary output logs.*\n\n*PS: I was able to get a score of 1295.07972 using this script with 70% (of train.csv) data used for training and rest for evaluation. Script took 2hrs for training and 3000 steps were used.*\n\n[1]: https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5259819c-f765-e649-5e67-7bcde8665463"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"0e7f9a40-2b80-bdea-d5aa-d52a87577e8c"},"source":"## Load CSVs data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb7edeab-c13e-8a29-34dc-2e9d0bd7059b"},"outputs":[],"source":"df_train_ori = pd.read_csv('../input/train.csv')\ndf_test_ori = pd.read_csv('../input/test.csv')"},{"cell_type":"markdown","metadata":{"_cell_guid":"8745286a-f9f6-67db-4c2a-e38bc6673045"},"source":"We only take first 1000 rows for training/testing and last 500 row for evaluation.\n\n\nThis done so that this script does not consume a lot of kaggle system resources."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95a34f94-b934-2e8f-fbd7-6e5c05425557"},"outputs":[],"source":"train_df = df_train_ori.head(1000)\nevaluate_df = df_train_ori.tail(500)\n\ntest_df = df_test_ori.head(1000)\n\nMODEL_DIR = \"tf_model_full\"\n\nprint(\"train_df.shape = \", train_df.shape)\nprint(\"evaluate_df.shape = \", evaluate_df.shape)\nprint(\"test_df.shape = \", test_df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3c9feb25-7e41-33b9-eb92-543e925b0214"},"source":"## Filtering Categorical and Continuous features\n\nWe store Categorical, Continuous and Target features names in different variables. This will be helpful in later steps."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3bfe4dd5-5b24-b897-b1b1-952e6944b308"},"outputs":[],"source":"features = train_df.columns\ncategorical_features = [feature for feature in features if 'cat' in feature]\ncontinuous_features = [feature for feature in features if 'cont' in feature]\nLABEL_COLUMN = 'loss'"},{"cell_type":"markdown","metadata":{"_cell_guid":"5f408c4f-60b3-1e75-281b-ace7ae729e43"},"source":"## Converting Data into Tensors\n\n> When building a TF.Learn model, the input data is specified by means of an Input Builder function. This builder function will not be called until it is later passed to TF.Learn methods such as fit and evaluate. The purpose of this function is to construct the input data, which is represented in the form of Tensors or SparseTensors.\n\n> Note that input_fn will be called while constructing the TensorFlow graph, not while running the graph. What it is returning is a representation of the input data as the fundamental unit of TensorFlow computations, a Tensor (or SparseTensor).\n\n[More detail][2] on input_fn.\n\n[2]: https://www.tensorflow.org/versions/r0.11/tutorials/input_fn/index.html#building-input-functions-with-tf-contrib-learn"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"04601f78-9447-20e9-20ce-3adcfd9a433c"},"outputs":[],"source":"# Converting Data into Tensors\ndef input_fn(df, training = True):\n    # Creates a dictionary mapping from each continuous feature column name (k) to\n    # the values of that column stored in a constant Tensor.\n    continuous_cols = {k: tf.constant(df[k].values)\n                       for k in continuous_features}\n\n    # Creates a dictionary mapping from each categorical feature column name (k)\n    # to the values of that column stored in a tf.SparseTensor.\n    categorical_cols = {k: tf.SparseTensor(\n        indices=[[i, 0] for i in range(df[k].size)],\n        values=df[k].values,\n        shape=[df[k].size, 1])\n        for k in categorical_features}\n\n    # Merges the two dictionaries into one.\n    feature_cols = dict(list(continuous_cols.items()) +\n                        list(categorical_cols.items()))\n\n    if training:\n        # Converts the label column into a constant Tensor.\n        label = tf.constant(df[LABEL_COLUMN].values)\n\n        # Returns the feature columns and the label.\n        return feature_cols, label\n    \n    # Returns the feature columns    \n    return feature_cols\n\ndef train_input_fn():\n    return input_fn(train_df)\n\ndef eval_input_fn():\n    return input_fn(evaluate_df)\n\ndef test_input_fn():\n    return input_fn(test_df, False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9f53238a-876b-bf8b-a5f9-77cdfafe67d9"},"source":"## Selecting and Engineering Features for the Model\n\nWe use tf.learn's concept of [FeatureColumn][FeatureColumn] which help in transforming raw data into suitable input features. \n\nThese engineered features will be used when we construct our model.\n\n[FeatureColumn]: https://www.tensorflow.org/versions/r0.11/tutorials/linear/overview.html#feature-columns-and-transformations"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87b90027-c671-d7aa-88b2-4e2856a83f52"},"outputs":[],"source":"engineered_features = []\n\nfor continuous_feature in continuous_features:\n    engineered_features.append(\n        tf.contrib.layers.real_valued_column(continuous_feature))\n\n\nfor categorical_feature in categorical_features:\n    sparse_column = tf.contrib.layers.sparse_column_with_hash_bucket(\n        categorical_feature, hash_bucket_size=1000)\n\n    engineered_features.append(tf.contrib.layers.embedding_column(sparse_id_column=sparse_column, dimension=16,\n                                                                  combiner=\"sum\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ca73de97-e020-4cf3-934f-44dde67523ae"},"source":"## Defining The Regression Model\n\nFollowing is the simple DNNRegressor model. More detail about hidden_units, etc can be found [here][123].\n\n**model_dir** is used to save and restore our model. This is because once we have trained the model we don't want to train it again, if we only want to predict on new data-set.\n\n[123]: https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.learn.html#DNNRegressor"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c5b7254-0e26-88d2-a65b-17f81c223ab4"},"outputs":[],"source":"regressor = tf.contrib.learn.DNNRegressor(\n    feature_columns=engineered_features, hidden_units=[10, 10], model_dir=MODEL_DIR)"},{"cell_type":"markdown","metadata":{"_cell_guid":"09e6b3b1-0c96-df80-85e7-31e958018309"},"source":"## Training and Evaluating Our Model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"109a3d46-d5b6-93f6-d792-14765b531b8d"},"outputs":[],"source":"# Training Our Model\nwrap = regressor.fit(input_fn=train_input_fn, steps=500)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7655bb62-0183-1778-4c86-a69ec5d3a3cf"},"outputs":[],"source":"# Evaluating Our Model\nprint('Evaluating ...')\nresults = regressor.evaluate(input_fn=eval_input_fn, steps=1)\nfor key in sorted(results):\n    print(\"%s: %s\" % (key, results[key]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"1c418c90-cbda-b772-f3be-bc9107bca2f8"},"source":"## Predicting output for test data\n\nMost of the time prediction script would be separate from training script (we need not to train on same data again) but I am providing both in same script here; as I am not sure if we can create multiple notebook and somehow share data between them in Kaggle."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4e45f26-f03e-9079-65a6-b13871b24a46"},"outputs":[],"source":"predicted_output = regressor.predict(input_fn=test_input_fn)\nprint(predicted_output[:10])"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}