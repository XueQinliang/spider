{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"0f31f0d3-831a-88b7-b3ac-8017e27107fc"},"source":"I'm doing this analysis in order to pick out facts directly from the data in an attempt to avoid spin from reporting and social media 'viralizing' in the 2016 Presidential Debates. <br> Also, I'm using this analysis to get more practice with text modeling and try some new approaches I haven't tried often before. <br><br>**I'm also super curious to compare these with social media and debates from past years. So many people are finding this year's election to be heinous---- does the data actually support that the content is more extreme, or is the prevalence of media in more places (i.e., mobile, streaming, etc) and the further proliferation of social media amplifying bad portions of content from this election to make it seem worse than it may actually be?  Or, is the case that this phenomenon is a mix of multiple causes?** <br><br>New on 10-22: Added heat map of medians of LDA model topic weights and subjectivity/polarity at bottom."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be4e7433-ca42-1cbe-e17c-c08df0bd3724"},"outputs":[],"source":"# Import needed python packages\nimport pandas as pd\nimport numpy as np\nimport gensim\nimport seaborn as sns\nimport textblob\nfrom gensim.parsing.preprocessing import preprocess_documents, preprocess_string\nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim.models import ldamodel, LdaModel\nfrom gensim import corpora, models\nimport nltk.data\nfrom nltk.corpus import stopwords\nimport re\n#import pyldavis\n#import pattern\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dde78c8b-6f8a-1a41-42de-dce2fe6bbc59"},"outputs":[],"source":"# Making a function to identify non-text lines in the transcripts\ndef identify_nontext(text):\n    '''\n    Identifies non-text text column rows. \n    text = Text item\n    returns:\n      1: If text is non-text and is contained entirely in parentheses\n      0: Text is text. \n    '''\n    if text.startswith('(') and text.endswith(')'):\n        return 1\n    else:\n        return 0"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"85802557-8a9a-931d-aec4-5af9a8bf2d59"},"outputs":[],"source":"# Define a function to indicate the candidates vs speakers\ndef speaker_type(speaker):\n    '''\n    Returns a label for speaker type. Deliberate excluding candidates' crosstalk\n    items since the transcript seems to try to pick out what they said anyway. \n    '''\n    if (speaker == 'Trump') | (speaker == 'Clinton'):\n        return 'Candidate'\n    if (speaker == 'Kaine') | (speaker == 'Pence'):\n        return 'VP Candidate'\n    if (speaker == 'Holt') | (speaker == 'Quijano') | (speaker == 'Cooper') | (speaker == 'Raddatz') | (speaker == 'Wallace'):\n        return 'Moderator'\n    if (speaker == 'QUESTION') | (speaker == 'Audience'):\n        return 'Audience'\n    else:\n        return 'Unlabeled'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5622aa2f-b3f9-66a3-8eab-58077385fb1b"},"outputs":[],"source":"# Define a function to indicate the candidates vs speakers\ndef general_speaker_type(speaker):\n    '''\n    Returns a label for speaker type. Deliberate excluding candidates' crosstalk\n    items since the transcript seems to try to pick out what they said anyway. \n    Combines VP's with presidential candidates for percentage calculation\n    '''\n    if (speaker == 'Trump') | (speaker == 'Clinton') | (speaker == 'Kaine') | (speaker == 'Pence'):\n        return 'Candidate'\n    if (speaker == 'Holt') | (speaker == 'Quijano') | (speaker == 'Cooper') | (speaker == 'Raddatz') | (speaker == 'Wallace'):\n        return 'Moderator'\n    if (speaker == 'QUESTION') | (speaker == 'Audience'):\n        return 'Audience'\n    else:\n        return 'Unlabeled'"},{"cell_type":"markdown","metadata":{"_cell_guid":"ff79969c-ffe8-bee3-464d-d9a33b2ea487"},"source":"Truth be told, I'm still looking into precisely what is meant by text blob's sentiment measures, namely polarity and subjectivity. If I'm not wrong, it looks like it's taking the sentiment measures from the pattern.en package <br>http://www.clips.ua.ac.be/pages/pattern-en#sentiment<br><br>pattern.en mentions that the metric comes from a lexicon of adjectives that occur frequently in reviews, and that are rated on a polarity and a subjectivity scale. I'm still looking up how that determination is made, but it looks like pattern.en has paper citations that will probably explain this. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"995f42ae-bb2a-c2e5-2ce0-112a0b4bce76"},"outputs":[],"source":"# Define a function to indicate an utterance's polarity score based upon textblob\ndef utterance_polarity(utterance):\n    '''\n    Returns a textblob polarity score for text passed to this function\n    '''\n    # Turn the string into a text blob\n    blob = textblob.TextBlob(utterance)\n    \n    # Return the polarity metric\n    return blob.sentiment.polarity"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20e6b120-9725-5ac5-1ba4-d16a5f1c1ad1"},"outputs":[],"source":"# Define a function to indicate an utterance's subjectivity score based upon textblob\ndef utterance_subjectivity(utterance):\n    '''\n    Returns a textblob polarity score for text passed to this function\n    '''\n    # Turn the string into a text blob\n    blob = textblob.TextBlob(utterance)\n    \n    # Return the polarity metric\n    return blob.sentiment.subjectivity"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"016d98fe-32a0-803f-0e4d-943756f7a485"},"outputs":[],"source":"# Convert text to lower-case and strip punctuation/symbols from words\n# Borrowed from doc2vec tutorial: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb\ndef normalize_text(text):\n    norm_text = text.lower()\n\n    # Replace breaks with spaces\n    norm_text = norm_text.replace('<br />', ' ')\n\n    # Pad punctuation with spaces on both sides\n    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n        norm_text = norm_text.replace(char, ' ' + char + ' ')\n\n    return norm_text"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5833584-3f7b-a8d9-1988-c965eb771d1f"},"outputs":[],"source":"# Making a function to identify non-text lines in the transcripts\ndef identify_nontext(text):\n    '''\n    Identifies non-text text column rows. \n    text = Text item\n    returns:\n      1: If text is non-text and is contained entirely in parentheses\n      0: Text is text. \n    '''\n    if text.startswith('(') and text.endswith(')'):\n        return 1\n    else:\n        return 0"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82daa2c8-66ce-cc0b-7aa5-088fdac3477e"},"outputs":[],"source":"def utterance_to_wordlist(utterance, remove_stopwords=False ):\n    '''\n    Derived from the Kaggle Bag-of-Words-Meets-Bags-Of-Popcorn Tutorial: \n    https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n    Function to convert a document to a sequence of words,\n    optionally removing stop words.  Returns a list of words.\n    '''\n    # 0. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z0-9]\",\" \", utterance)\n    #\n    # 1. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 4. Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    #\n    # 5. Return a list of words\n    return(words)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e48e4e1-84ef-524f-44f7-14f3cf146232"},"outputs":[],"source":"# Define a function to split a review into parsed sentences\ndef utterance_to_sentences( utterance, tokenizer, remove_stopwords=False ):\n    '''\n    Derived from the Kaggle Bag-of-Words-Meets-Bags-Of-Popcorn Tutorial: \n    https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n    Function to split a review into parsed sentences. Returns a \n    list of sentences, where each sentence is a list of words\n    '''\n    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(utterance.strip())\n    #\n    # 2. Loop over each sentence\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            # Otherwise, call review_to_wordlist to get a list of words\n            sentences.append( utterance_to_wordlist( raw_sentence, \\\n              remove_stopwords=True))\n    #\n    # Return the list of sentences (each sentence is a list of words,\n    # so this returns a list of lists\n    return sentences"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60c096d6-c69d-191d-9a9b-ddb8cdffaa4d"},"outputs":[],"source":"# Define a function to get the list of topic weights for each 'document', which is each utterance in this case\ndef get_utterance_topic_weights(bagofwords, ldamodel):\n    '''\n    Use an lda model to get the list of topic weights for a bag-of-words object\n    '''\n    return ldamodel.get_document_topics(bagofwords)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a074874-c046-076e-9fd6-63dee2c0a37b"},"outputs":[],"source":"# Define a function to get the different topic weights separated into their own columns\ndef get_individual_topic_weights(topic_weights, topic):\n    '''\n    Inputs:\n        topic_weights: List of topics\n        k: Number of topics\n    '''\n    # Make the list into a dict for easier lookup\n    topic_dict = dict(topic_weights)\n    \n    # Return the value for the specified topic\n    return topic_dict.get(topic)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ccc434b-c620-0e54-3550-b844754bacac"},"outputs":[],"source":"# Import the dataset\ndf = pd.read_csv('../input/debate.csv',encoding = 'iso-8859-1')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8336e06e-9bcf-0a74-05cd-01b00696af3a"},"outputs":[],"source":"# Insert needed categorical indicators, and also the polarity and subjectivity metrics from TextBlob\ndf.insert(df.shape[1], 'nontext_ind', df.Text.apply(identify_nontext))\ndf.insert(df.shape[1], 'speaker_type', df.Speaker.apply(speaker_type))\ndf.insert(df.shape[1], 'general_speaker_type', df.Speaker.apply(general_speaker_type))\ndf.insert(df.shape[1], 'polarity', df.Text.apply(utterance_polarity))\ndf.insert(df.shape[1], 'subjectivity', df.Text.apply(utterance_subjectivity))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7feac178-6db3-ed8e-89a6-71bfcb360697"},"outputs":[],"source":"# Check what the first lines of the dataframe are, to make sure it loaded\ndf.head(15)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"25ca3c0e-2e49-0d94-075b-50a68c9c41c4"},"outputs":[],"source":"# Check the shape of the dataframe--- also to assess whether loaded accurately--- number of rows x number of columns\ndf.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de861a9d-dd4f-82a9-f548-ab0b3c3e4ac9"},"outputs":[],"source":"# Check how many rows there are per the different dates present in the 'Date' column\ndf.Date.value_counts().plot(kind='bar')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c956937-5730-0263-4ff5-e33d0c3b9c46"},"outputs":[],"source":"# Show candidates' utterances compared with one another\ng = sns.factorplot(\"Speaker\", col=\"Date\", col_wrap=3, palette='Blues_r',\n                  data = df[(df['Speaker'] == 'Trump') | (df['Speaker'] == 'Clinton')], \n                   kind=\"count\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"383c56ac-4574-7f87-aa64-810cd22468b0"},"outputs":[],"source":"# Show the moderators' overall utterances\ng = sns.countplot(x='Date', data=df[df['speaker_type'] == 'Moderator'], palette='Reds_r')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cfa052ff-8981-2460-d91d-f98fe0b35266"},"outputs":[],"source":"# Show the VPs' overall utterances\ng = sns.countplot(x='Speaker', data=df[df['speaker_type'] == 'VP Candidate'], palette='Blues_r')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d1c25ae-c0fc-fba3-a429-8715e90ef407"},"outputs":[],"source":"# Compare the different speaker types over the different dates\ng = sns.factorplot(\"general_speaker_type\", col=\"Date\", col_wrap=3, palette='Reds_r',\n                  data = df[df['general_speaker_type'] != 'Unlabeled'], kind=\"count\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74dae40e-b36b-9d70-39da-adf4c4d3a218"},"outputs":[],"source":"df2 = df[df['general_speaker_type'] != 'Unlabeled']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b777c0e8-6ca6-8e48-fcf8-721beb5d6a3c"},"outputs":[],"source":"df2_ct = pd.crosstab(df2['Date'], df2['general_speaker_type'], margins=True, normalize='index')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ea4cd70-f0c6-74af-d19a-80655be5c154"},"outputs":[],"source":"df2_ct"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"14a52779-6ec1-bb8e-69a1-808f0a73a31b"},"outputs":[],"source":"# This graph shows the percentages--- the candidates spoke the most during the vice presidential debate,\n# but had the least audience input. The moderators had their highest percentage of input on the 2nd\n# presidential debate. \ndf2_ct.plot.bar(stacked=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5cca40cf-8feb-5d01-594b-f70e05f15d59"},"outputs":[],"source":"# Histograms of polarity\ndf[df['Speaker'] == 'Trump'].polarity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1336c404-b7af-92d8-47cb-0d5839ea1e9b"},"outputs":[],"source":"# Histograms of polarity\ndf[df['Speaker'] == 'Clinton'].polarity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1612e2ba-1889-57d3-1f78-cb26b1e29c40"},"outputs":[],"source":"# Histograms of polarity\ndf[df['Speaker'] == 'Pence'].polarity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b17057b-cfe1-551b-b79e-5f1d182e3676"},"outputs":[],"source":"# Histograms of polarity\ndf[df['Speaker'] == 'Kaine'].polarity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11b2eaa2-2e6f-19a2-00b6-9340cfdf2f70"},"outputs":[],"source":"# Histograms of polarity ---- all moderators together\ndf[df['general_speaker_type'] == 'Moderator'].polarity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9a85b97-02d2-691d-e930-0e5e5e5e29ee"},"outputs":[],"source":"# Histograms of polarity ---- all moderators together\ndf[df['Speaker'] == 'Audience'].polarity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"429b864f-4f93-9b40-b27d-cb0319d0f048"},"outputs":[],"source":"# Histograms of subjectivity ---- Trump\ndf[df['Speaker'] == 'Trump'].subjectivity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"92fe368b-cba7-6d57-5c00-5991d85e57a8"},"outputs":[],"source":"# Histograms of subjectivity ---- Clinton\ndf[df['Speaker'] == 'Clinton'].subjectivity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"13db324e-27a4-ec9a-254c-dc49f172b9f2"},"outputs":[],"source":"# Histograms of subjectivity ---- Pence\ndf[df['Speaker'] == 'Pence'].subjectivity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4a4c736-6446-68c1-8f4e-d7373c538ffd"},"outputs":[],"source":"# Histograms of subjectivity ---- Kaine\ndf[df['Speaker'] == 'Kaine'].subjectivity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad86c916-ba2e-a09f-c393-a2a1ef75119b"},"outputs":[],"source":"# Histograms of subjectivity ---- Moderators\ndf[df['general_speaker_type'] == 'Moderator'].subjectivity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e77fb4dc-7248-fdf3-9d62-0bb2a9e38eda"},"outputs":[],"source":"# Histograms of subjectivity ---- Audience\ndf[df['Speaker'] == 'Audience'].subjectivity.hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"894b1c4a-6fe9-f096-e1ea-70bef839f6a6"},"outputs":[],"source":"df[df['Speaker'] == 'Audience'].subjectivity.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"34d2c898-39da-6004-b0d0-18a5d86647f8"},"outputs":[],"source":"df[df['Speaker'] == 'Audience'].polarity.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6686f37c-1019-659a-5c56-0e8e0213e71f"},"outputs":[],"source":"# Lol, and the audience is the least subjective and polar group in the lot. They didn't get to \n# speak very much though. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a7c9bc8-a708-48e5-2bff-2ad9266af6f8"},"outputs":[],"source":"# How many utterances did each party end up making for all dates?\n# In this case and at this early stage of analysis, I'm going to define 'utterance' as one line.\ndf.Speaker.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"96f193e2-11aa-1df5-53c9-f764bfcae55f"},"outputs":[],"source":"df.Speaker.value_counts().plot(kind='bar')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6e41143-8f60-4d50-cebe-3079fa5aecba"},"outputs":[],"source":"# Ok, so without any other preprocessing, Trump has 224 and Clinton has 158. Percentage-wise,\n# How many more utterances does he have?\n# After running once, dividing comes out to 142%, so about 42% more than Clinton.\n224/158"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58da74db-1232-4019-9de7-c8fd8b1ef2b3"},"outputs":[],"source":"# So--- how do both presidential candidates rate for utterance count in both debates?\n# This line of code excludes (!=) the 10-04 VP debate lines\ndf[df['Date'] != '2016-10-04'].Speaker.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf2bdb4e-9fcf-494c-0ba6-fce516d4d805"},"outputs":[],"source":"df[df['Date'] != '2016-10-04'].Speaker.value_counts().plot(kind='bar')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31a3ee61-8969-c98c-8d2e-4140fdec9c06"},"outputs":[],"source":"# How does this break down by the individual debate?\n# Since Trump complained of problems with his microphone later, my expectation before\n# running this code is that in the first debate, he would not have spoken as frequently as Clinton\n# due to the microphone issues, and thus made up the utterance discrepancy in the 2nd debate\ndf[df['Date'] == '9/26/16'].Speaker.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea2584af-a922-4fa9-9f1a-94d4678326b7"},"outputs":[],"source":"df[df['Date'] == '9/26/16'].Speaker.value_counts().plot(kind='bar')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f3439b1-8319-58c1-5736-86004cd1325a"},"outputs":[],"source":"df.Date.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5964ae97-056b-eb17-5150-9f0f872a1cc4"},"outputs":[],"source":"# Note--- the expectation did not pan out--- Trump actually talked the most of anyone in the first debate\n# Now--- let's check out the utterance count in the 2nd debate\ndf[df['Date'] == '10/9/16'].Speaker.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99b144fa-5cff-7f9f-fbdf-922d1abd785a"},"outputs":[],"source":"df[df['Date'] == '10/9/16'].Speaker.value_counts().plot(kind='bar')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bebe3db3-3d87-d903-ace2-02d9315b1695"},"outputs":[],"source":"# Utterance counts in the 2nd debate\ndf[df['Date'] == '10/19/2016'].Speaker.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"364e168b-5b9b-ae40-759e-d90820d47087"},"outputs":[],"source":"df[df['Date'] == '10/19/2016'].Speaker.value_counts().plot(kind='bar')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec589dde-1e78-60bc-b6d0-ecb860127cc9"},"outputs":[],"source":"df.Speaker.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"538d490e-48cd-452f-35fb-7c3db0359a5d"},"outputs":[],"source":"'''\nSo, not only did Clinton make over 30% fewer utterances than Trump in all debates\nconsidered together, she also typically made fewer utterances than the moderators. \n'''"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6d772e81-9c71-c8cd-7919-cb6cd67e0dfc"},"outputs":[],"source":"# Let's see who had the most utterances in the VP debate\ndf[df['Date'] == '10/4/16'].Speaker.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af8ae795-724a-6fd0-4075-da3e57975c02"},"outputs":[],"source":"df[df['Date'] == '10/4/16'].Speaker.value_counts().plot(kind='bar')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f70aedfd-f85e-c5e4-6b1b-d67f865e9e8e"},"outputs":[],"source":"# Both VP candidates had more utterances than the moderator. Interesting.\n# In this case, Trump's VP candidate, Pence, also did most of the talking, but\n# only had about 8% more utterances than Kaine did. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe2890b7-087e-7ed0-7b7a-3b22a316bb36"},"outputs":[],"source":"# Just to illustrate what the polarity and subjectivity measures will and won't do, let's see how the\n# infamous 'bad hombres' quote from Trump rated\ndf[df['Text'].str.contains('hombre')]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"953679d9-a95a-299e-54ca-163a4722ce09"},"outputs":[],"source":"# However, looking over the dataframe, when some of the speech was spoken in a manner\n# that was disjointed or heavily  repeated in audio recordings, the transcription \n# tends to consider these statements as two separate utterances... may need to go back\n# and correct the utterance counts for this, but on first glance it doesn't look like\n# this happened a lot. But first, the text itself, since the content is more value \n# than the basic utterance count. "},{"cell_type":"markdown","metadata":{"_cell_guid":"f093f3c4-5488-7b3f-8051-0828c5f8337e"},"source":"## LDA Model\nTry it from this tutorial, since it's a really good one: https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c358b82-322b-e1f4-2600-47f9a6314d38"},"outputs":[],"source":"x"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b2e2525-a2f9-9a22-8ab5-4a9c05e6c2dc"},"outputs":[],"source":"# Make another dataframe entirely out of just the text fields--- I may want to look\n# at the non-text fields again, so I'll write these into a new variable.\ndf_text = df[df['nontext_ind'] == 0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b34fb65-540f-13c1-11cb-62975e914a8f"},"outputs":[],"source":"# Check the first 5 lines of this new one to make sure it looks right\ndf_text.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63a590bd-0110-6bb5-bfa5-9338fd7afe4b"},"outputs":[],"source":"# Add another column to the dataframe in which the text is pre-processed by the\n# gensim package's 'preprocess_string' function. \n# This converts everything to lowercase, removes non-informative 'stopwords' (words\n# that are necessary for English language but don't lend any meaning for analysis),\n# 'tokenizes' the sentences by splitting them out into individual words,\n# and 'stems' the words--- i.e., takes the 'stem' of a word only and removing any\n# endings... this allows things like 'grand' and 'grandly', which typically denote\n# the same or very similar things via their root word to be counted as the same\n# word in order to more clearly pick out relative topics without accidentally splitting\n# useful information. \ndf_text.insert(df_text.shape[1], 'PreprocessedText', df_text['Text'].apply(preprocess_string))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0843016e-ce7f-48c4-3a6f-e3b50cc49d2f"},"outputs":[],"source":"# Check the first 5 lines, or the 'head', to see if that worked as expected\ndf_text['PreprocessedText'].head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"124653a1-814d-c65e-51d9-2801cb4a7653"},"outputs":[],"source":"# Use gensim to tag the text. Might re-do this later using some sort of \n# categrory label based on a sentiment analysis from text blob, but for now,\n# I'm just going to label with the speakers, since the doc2vec tutorial\n# Labeling with speakers may allow analysis of seeing if you can tell which statements\n# were uttered by whom. \n# I'm following says the tagging is required---- note: starting with LDA Model instead\n# Tutorial: https://linanqiu.github.io/2015/10/07/word2vec-sentiment/\ndf_text.insert(df_text.shape[1], 'TaggedText', df_text['PreprocessedText'].apply(TaggedDocument, args=(df_text['Speaker'],)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"417e5f89-5af0-eed5-ff2d-f890aee3759c"},"outputs":[],"source":"# RANDOM CODE BLOCK! Started playing with textblob and got sidetracked :-)\n# Maybe won't use this quite yet, but it's neat. \n# What this does is tags the part of speech. PRP = preposition, VB = verb, etc.\n# I'm sure I will use this later on, but for now, I want to finish out the doc2vec stuff first. \nzz = textblob.taggers.NLTKTagger()\nzzz = zz.tag(df_text['Text'][8])\nzzz[0:5] # Equivalent to a 'head' statement--- this object is a list and therefore doesn't have the \n# Head function like a pandas dataframe does. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77b71ba1-19ff-e40e-ec7d-1ce6907cb8b6"},"outputs":[],"source":"# Sort the text\ntext = df_text['Text'].sort_values()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9ec4e61-7b81-ec2c-818b-06150bd91f44"},"outputs":[],"source":"text.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"560cd13c-ee25-feec-777f-f1a3fd1ee696"},"outputs":[],"source":"sentences = []"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6656f6d-7285-dff0-75eb-868de9c6c9bc"},"outputs":[],"source":"import nltk.data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2038176d-2212-16ec-ce8d-677520b3e64b"},"outputs":[],"source":"tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf1d207d-f13e-3fb9-aac7-f811f4715d31"},"outputs":[],"source":"# Convert the text to sentences and tokenize\nprint(\"Parsing sentences from unlabeled set\")\nfor utterance in text:\n    sentences += utterance_to_sentences(utterance, tokenizer)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ce42a0f-1b79-70fd-4800-ed4c6fe29363"},"outputs":[],"source":"len(sentences)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3550acdc-a0b1-ad06-c169-ecf24c3e8c52"},"outputs":[],"source":"# Create lDA's vocab dictionary\ndictionary = corpora.Dictionary(sentences)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"81f2c140-6d19-eb57-1dc5-686cdeb58586"},"outputs":[],"source":"# Make a bag-of-words corpus from the sentences object\ncorpus = [dictionary.doc2bow(text) for text in sentences]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"514190c4-b3ef-2253-7f01-88e5e92fa65f"},"outputs":[],"source":"# Make a model object - start with a default number of topics of 10\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=20)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9cab8cbd-0c11-6412-9e7f-0dede193445f"},"outputs":[],"source":"# Show the top 10 words in each of the ten topics\ndict(ldamodel.print_topics(num_topics=10, num_words=10))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"207541f2-76c2-6120-1b48-0200486daa09"},"outputs":[],"source":"# Interesting topics. I have requested pyLDAvis for the kaggle docker container and will complete the \n# visualization when it has been installed. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3782c03f-fbe2-294c-1e52-afe5552c8ca5"},"outputs":[],"source":"# Ok... figure out how to get topic distributions for the 10 topics. \ndf_text.insert(df_text.shape[1],'bow_column', df_text.PreprocessedText.apply(dictionary.doc2bow))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fb886dd-a02a-450b-0a8f-bcbcf2dc541e"},"outputs":[],"source":"# Insert the list of document topics into the 'master' text dataframe\ndf_text.insert(df_text.shape[1], 'topic_weights', df_text['bow_column'].apply(get_utterance_topic_weights, args=(ldamodel,)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"83a9c4eb-c670-4abd-b8fe-1d0782b2cc4a"},"outputs":[],"source":"# Loop through the topics, and make a new column for each topic\nfor topic in range(0, 10):\n    col_name = 'topic_weight_' + str(topic)\n    df_text.insert(df_text.shape[1], col_name, df_text['topic_weights'].apply(get_individual_topic_weights, args=(topic,)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b38cf34-254a-eaa5-e3e1-992edf80a35a"},"outputs":[],"source":"# Loop through the topics again, and this time, fillna\nfor topic in range(0, 10):\n    col_name = 'topic_weight_' + str(topic)\n    df_text[col_name].fillna(0.0, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"429ca9e0-e00b-a233-da8c-701d20308d8e"},"outputs":[],"source":"# On this, to explore, I just went through all of the topic dists. Looks like a gamma dist for each, just eyeballing it.\ndf_text['topic_weight_0'].hist()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df2df5cc-4999-8a11-9f73-605a24f1a26a"},"outputs":[],"source":"# Set up a groupby\ndf_text_heat = df_text.groupby('Speaker')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4ede092-7e6b-a1be-5717-040e4e217c05"},"outputs":[],"source":"df_text_heat_med = df_text_heat.aggregate({'topic_weight_0':'median',\n                                          'topic_weight_1':'median',\n                                          'topic_weight_2':'median',\n                                          'topic_weight_3':'median',\n                                          'topic_weight_4':'median',\n                                          'topic_weight_5':'median',\n                                          'topic_weight_6':'median',\n                                          'topic_weight_7':'median',\n                                          'topic_weight_8':'median',\n                                          'topic_weight_9':'median',\n                                          'polarity':'median',\n                                          'subjectivity':'median'})"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c774b05b-576f-07e0-42a3-3f22cec8151b"},"outputs":[],"source":"df_heat_med = speak = df_text_heat_med.T"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f1d74f4-cd61-e914-c407-92c9cc0ac1ad"},"outputs":[],"source":"df_heat_med.sort_index(axis=0, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fafba54a-b45b-9ee6-12ae-93cdaa2a5c5e"},"outputs":[],"source":"g = sns.heatmap(df_heat_med, annot=True, linewidths=.5, cmap='RdBu_r')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94ebb0d7-31b1-ff41-9d43-1b2776818180"},"outputs":[],"source":"# So, how far IS Trump from Clinton on subjectivity --- not terribly different, since\n# the difference is quite a bit less than one standard deviation. \ndf_text.subjectivity.std()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6312b3a5-7792-7476-d131-bdfbcf115c5c"},"outputs":[],"source":"# May play around with this a bit more\n# To demonstrate though, I noticed that topic 7 might have a bi-modal distribution, \n# so let's see what that looks like relative to subjectivity\n\n'''\nTopic Six Words:\n6: '0.030*taxes,\n    0.029*trump,\n    0.028*clinton,\n    0.025*mr,\n    0.023*tax,\n    0.023*let,\n    0.021*two,\n    0.019*thank,\n    0.019*want,\n    0.017*secretary'\n'''\nsns.jointplot('topic_weight_7', 'subjectivity', data=df_text[df_text['topic_weight_7'] > 0.1], \n              kind=\"kde\", color=\"Blue\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4800d9e9-9876-c563-d38a-4809f7ee7889"},"outputs":[],"source":"# Actually, maxes by speaker of the aggregates may be more useful\ndf_text_heat_max = df_text_heat.aggregate({'topic_weight_0':'max',\n                                          'topic_weight_1':'max',\n                                          'topic_weight_2':'max',\n                                          'topic_weight_3':'max',\n                                          'topic_weight_4':'max',\n                                          'topic_weight_5':'max',\n                                          'topic_weight_6':'max',\n                                          'topic_weight_7':'max',\n                                          'topic_weight_8':'max',\n                                          'topic_weight_9':'max',\n                                          'polarity':'max',\n                                          'subjectivity':'max'})\ndf_heat_max = speak = df_text_heat_max.T\ndf_heat_max.sort_index(axis=0, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e23e63b5-2c5d-cfd7-7776-93863b386bb0"},"outputs":[],"source":"g = sns.heatmap(df_heat_max, annot=True, linewidths=.5, cmap='RdBu_r')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5652bfe4-e688-9d79-dc68-8597d7a69cd8"},"outputs":[],"source":"# Show the top 10 words in each of the ten topics\ndict(ldamodel.print_topics(num_topics=10, num_words=10))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0d6659b-6860-67a4-54e0-661cf7fc1d84"},"outputs":[],"source":"# Let's try mins --- the topics and subjectivity were a big bunch of 0's, so I'll just do polarity\ndf_text_heat_min = df_text_heat.aggregate({'polarity':'min'})\ndf_heat_min = df_text_heat_min.T\ndf_heat_min.sort_index(axis=0, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a84d4b1-4de7-9b91-5b92-9e600c834e12"},"outputs":[],"source":"g = sns.heatmap(df_heat_min, annot=True, linewidths=.5, cmap='RdBu_r')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28cea082-5805-d875-f3e3-4b73860c0c97"},"outputs":[],"source":"# On the minimum heatmap, the polarity line is pretty interesting. \n# The extremes kind of re-illustrate how the VP running-mates balance the candidates on polarity,\n# but the candidates and VPs match better along party lines in terms of subjectivity extremes.\n# I was kind of surprised that moderators are ranking as low as -.5 though... my\n# assumption before doing any analysis was that they would be closer to neutral than\n# they ended up being.\n# On that bent too... on subjectivity, everyone ran the gamut from 0 to 1, except Cooper. He\n# was the only one that didn't hit a full 1 on subjectivity, even including the questions. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84bf3206-129f-b68a-8fa1-5759b32bbb8a"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}