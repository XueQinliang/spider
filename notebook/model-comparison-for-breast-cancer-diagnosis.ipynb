{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b39fbdf9-de00-7b31-2ab0-c1a9c9fcecb8"},"source":"# Comparison of ML Models at Predicting Breast Cancer"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0726d96-550b-56e4-e7c3-b603c4e4b7bd"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom scipy.stats import randint as sp_randint\n\n%matplotlib inline\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32be4d69-776a-fec2-e451-5ae20eae64cc"},"outputs":[],"source":"df = pd.read_csv(\"../input/data.csv\",header = 0)\ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"aa108f3e-becb-072c-6c27-3967b55786fa"},"source":"# Prepare Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6321644-c419-67e8-86e2-97e247f41cf3"},"outputs":[],"source":"# Remove unnecessary columns\ndf.drop('id',axis=1,inplace=True)\ndf.drop('Unnamed: 32',axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa74da90-0f91-ecad-d9fb-1ffd3123bb92"},"outputs":[],"source":"# Encode diagnosis as numerical values(B=0, M=1)\nle = preprocessing.LabelEncoder()\nle.fit(['M', 'B'])\n\ndf['diagnosis'] = le.transform(df['diagnosis'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90f55c80-b8d7-6dc8-4575-2f1304bf0a4f"},"outputs":[],"source":"df.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c7c3d1f4-6110-3f35-8f24-8d5054cfae53"},"source":"# Visualize Data"},{"cell_type":"markdown","metadata":{"_cell_guid":"1b835cbc-e9a3-c1e6-9a39-739acfdfc569"},"source":"One of the main goals of visualizing the data here is to observe which features are most helpful in predicting malignant or benign cancer. The other is to see general trends that may aid us in model selection and hyper parameter selection."},{"cell_type":"markdown","metadata":{"_cell_guid":"2f6aedea-6a4a-03d6-43e3-d868bb19e79c"},"source":"## Principal Component Analysis\n\nThe purpose for doing principal component analysis on the labeled data here is to observe the variance explained by each of the components and the associated weights assigned to each feature. The resulting output will aid in deciding on which features to drop."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6cd4249a-0615-28e0-0f27-7b51d037dd84"},"outputs":[],"source":"from sklearn.decomposition import PCA\n\n# observables = df.loc[:,observe]\nobservables = df.iloc[:,1:]\npca = PCA(n_components=3)\npca.fit(observables)\n\n# Dimension indexing\ndimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\n# Individual PCA Components\ncomponents = pd.DataFrame(np.round(pca.components_, 4), columns = observables.keys())\ncomponents.index = dimensions\n\n# Explained variance in PCA\nratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\nvariance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\nvariance_ratios.index = dimensions\n\nprint(pd.concat([variance_ratios, components], axis = 1))"},{"cell_type":"markdown","metadata":{"_cell_guid":"fc3ac4a4-c942-767f-27cf-f734ed5f917d"},"source":"## Observations\n\nIt can be observed that **98.20%** of the variance is explained in dimension 1. This means that nearly all of the variance in the data can be described by one dimension. The remaining two dimensions describe a much smaller amount of variance. \n\nIn dimension 1, most of the feature weight is associated with the **area_mean** and **area_worst** dimension. This was a surprise. My assumption was that the mean values would describe most of the variance in the data. Due to this observation, in the next step, I will visualize how well each of the **mean** features as well as **area_worst** and **perimeter_worst** explain the resulting diagnosis.\n\nWhile I will not be using PCA in the actual machine learning phase, this describes the data well and helps understand which features should be further investigated for their importance in the final prediction."},{"cell_type":"markdown","metadata":{"_cell_guid":"e39a7890-77fe-3f6c-f047-80d470a3643b"},"source":"# Feature Selection\nAlong with my initial hypothesis that the *mean value* features were important in predicting cancer type, **area_worst** and **perimeter_worst** will also be investigated due to their weighted importance in the previous PCA observation step."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd32736d-cba7-794d-7e85-f65fb491147f"},"outputs":[],"source":"# Observe correlation to the diagnosis\ntst = df.corr()['diagnosis'].copy()\ntst = tst.drop('diagnosis')\ntst.sort_values(inplace=True)\ntst.plot(kind='bar', alpha=0.6)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3e03657-17dd-ec80-56c8-17866fb2a951"},"outputs":[],"source":"# Separate out malignant and benign data for graphing\nmalignant = df[df['diagnosis'] ==1]\nbenign = df[df['diagnosis'] ==0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20f6aae9-629b-07ef-2f62-b6ec22800f6e"},"outputs":[],"source":"# Column names to observe in following graphs - mean values only\nobserve = list(df.columns[1:11]) + ['area_worst'] + ['perimeter_worst']\nobservables = df.loc[:,observe]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9697effb-5409-c589-fa97-7cf19c22563f"},"outputs":[],"source":"plt.rcParams.update({'font.size': 8})\nplot, graphs = plt.subplots(nrows=6, ncols=2, figsize=(8,10))\ngraphs = graphs.flatten()\nfor idx, graph in enumerate(graphs):\n    graph.figure\n    \n    binwidth= (max(df[observe[idx]]) - min(df[observe[idx]]))/50\n    bins = np.arange(min(df[observe[idx]]), max(df[observe[idx]]) + binwidth, binwidth)\n    graph.hist([malignant[observe[idx]],benign[observe[idx]]], bins=bins, alpha=0.6, normed=True, label=['Malignant','Benign'], color=['red','blue'])\n    graph.legend(loc='upper right')\n    graph.set_title(observe[idx])\nplt.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"94270f57-c2d5-c8ee-fb86-5051bc625fa6"},"source":"## Observations\nFrom the graphs, we can see that **radius_mean, perimeter_mean, area_mean, concavity_mean** and **concave_points_mean** are useful in predicting cancer type due to the distinct grouping between malignant and benign cancer types in these features. We can also see that **area_worst** and **perimeter_worst** are also quite useful.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c1bcd34a-e297-7e98-afb5-3ef235f68c09"},"outputs":[],"source":"color_wheel = {0: \"blue\", 1: \"red\"}\ncolors = df[\"diagnosis\"].map(lambda x: color_wheel.get(x))\npd.scatter_matrix(observables, c=colors, alpha = 0.5, figsize = (15, 15), diagonal = 'kde');"},{"cell_type":"markdown","metadata":{"_cell_guid":"0a04dfb7-54ed-f7b0-293c-0772200cdd8d"},"source":"## Observations\n\nThe scatter matrix clarifies a few more points. The **perimeter_mean, area_mean** and **radius mean** have a strong, positive, linear correlation. Most other data also has a more rough linear correlation to other features with the exception of **fractal_dimension_mean, symmetry_mean** and **smoothness_mean**.\n\nWithin these three features we can see quite a bit of mixing between malignant and benign cancer in the scatter matrix. This suggests that our assumption above, that they do not aid in predicting cancer type, is likely correct. There is less correlation and separability between the two diagnoses.\n\nDue to the lack of clear separability and lack of variance explained I feel comfortable dropping them."},{"cell_type":"markdown","metadata":{"_cell_guid":"b5bb80ad-5b63-2652-0695-cde784d0f593"},"source":"### Trimming Data\nFrom observing the graphs and PCA data above: fractal_dimension_mean, smoothness_mean and symmetry_mean are not very useful in predicting the type of cancer. To aid in the learning process and remove noise, these columns will be dropped."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a0a9d5bd-f380-43d1-f0fa-44df513ab8c3"},"outputs":[],"source":"# Drop columns that do not aid in predicting type of cancer\nobservables.drop(['fractal_dimension_mean', 'smoothness_mean', 'symmetry_mean'],axis=1,inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"814b32ba-d6f3-d8bc-7e79-0085bf5f2c6d"},"source":"# Classification"},{"cell_type":"markdown","metadata":{"_cell_guid":"e209fc40-3cce-611e-f313-fbbe0abe3bc5"},"source":"Here a comparison will be made between the different types of learning algorithms. At the end a breakdown of the data and explanation of the algorithm's performance will be made."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2064964f-4844-6dc6-7460-1bb6458a12a2"},"outputs":[],"source":"# Split data appropriately\nX = observables\ny = df['diagnosis']"},{"cell_type":"markdown","metadata":{"_cell_guid":"d7a80e85-9536-1600-6cb6-b3f47dd72030"},"source":"## Naive Bayes"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b364cddf-7aa2-e8cd-6113-6d4cc269cd41"},"outputs":[],"source":"gnb = GaussianNB()\ngnb_scores = cross_val_score(gnb, X, y, cv=10, scoring='accuracy')\nprint(gnb_scores.mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"8202db30-77cb-07bf-8b0e-34133cadda50"},"source":"### Gaussian Naive Bayes Findings\nGaussian Naive Bayes had an accuracy score of **0.92**. While this is not ideal, it is not a terrible score to attain using an algorithm as simple as Naive Bayes. NB performed well because, as seen above, much of the data is linearly separable. "},{"cell_type":"markdown","metadata":{"_cell_guid":"c1f60f1a-bae5-b54f-bee7-429f4aaa05cf"},"source":"## KNN"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"513b5672-72fa-d229-8937-755339a76bad"},"outputs":[],"source":"# Decide what k should be for KNN\nknn = KNeighborsClassifier()\n\nk_range = list(range(1, 30))\nleaf_size = list(range(1,30))\nweight_options = ['uniform', 'distance']\nalgorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\nparam_grid = {'n_neighbors': k_range, 'leaf_size': leaf_size, 'weights': weight_options, 'algorithm': algorithm}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"97f537c9-de65-790c-4875-25c6e8d246f8"},"outputs":[],"source":"rand_knn = RandomizedSearchCV(knn, param_grid, cv=10, scoring=\"accuracy\", n_iter=100, random_state=42)\nrand_knn.fit(X,y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b6f3e607-01d6-a963-76e7-632c52216545"},"source":"It looks as though any value for K past 20 would work well but the simpler the better."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74e8e4ca-9a4b-0d4c-3e2e-bec05cec2b1b"},"outputs":[],"source":"print(rand_knn.best_score_)\nprint(rand_knn.best_params_)\nprint(rand_knn.best_estimator_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2df18d2c-5f53-b467-03f9-4a95afa2acdb"},"source":"### KNN Findings\n\nUtilizing Randomized hyper parameter search along with cross validation resulted in a KNN model with an accuracy score of **0.93**. The model that was chosen by *RandomizedSearchCV* is as follows: {'weights': 'uniform', 'n_neighbors': 14, 'leaf_size': 22, 'algorithm': 'ball_tree'}.\n\nI do not believe KNN is optimal for this problem so a more involved comparison of results will be made after several more tests."},{"cell_type":"markdown","metadata":{"_cell_guid":"53c2f041-bfd1-2b9d-db8a-290a9e178f8e"},"source":"## Decision Tree Classifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e14c3d0f-3709-6331-447e-dc0315e60e9e"},"outputs":[],"source":"dt_clf = DecisionTreeClassifier(random_state=42)\n\nparam_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n              'min_samples_split': sp_randint(2, 11), \n              'min_samples_leaf': sp_randint(1, 11)}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a810f503-d8f5-a2d3-cacb-f7a211aba4a7"},"outputs":[],"source":"rand_dt = RandomizedSearchCV(dt_clf, param_grid, cv=10, scoring=\"accuracy\", n_iter=100, random_state=42)\nrand_dt.fit(X,y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b72222c-f6bf-5f2d-1af5-5f14af08c533"},"outputs":[],"source":"print(rand_dt.best_score_)\nprint(rand_dt.best_params_)\nprint(rand_dt.best_estimator_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7331530f-4160-64c4-a150-5f4b010e574d"},"source":"### Decision Tree Findings\n\nUtilizing Randomized hyper parameter search along with cross validation resulted in a Decision Tree Classification model with an accuracy score of **0.95**. The model that was chosen by *RandomizedSearchCV* is as follows: {'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 3}."},{"cell_type":"markdown","metadata":{"_cell_guid":"899bb7e4-621d-6db7-4e5d-c89c50a393c3"},"source":"## Support Vector Machine Classifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ddad3cbb-903c-144b-9dd9-96b71e0cffe4"},"outputs":[],"source":"sv_clf = SVC(random_state=42)\n\nparam_grid = [\n              {'C': [1, 10, 100, 1000], \n               'kernel': ['linear']\n              },\n              {'C': [1, 10, 100, 1000], \n               'gamma': [0.001, 0.0001], \n               'kernel': ['rbf']\n              },\n ]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"48d73fd9-b994-2271-c4ce-3f75fd41d4bd"},"outputs":[],"source":"grid_sv = GridSearchCV(sv_clf, param_grid, cv=10, scoring=\"accuracy\")\ngrid_sv.fit(X,y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d64d446-b306-ec04-a0e5-6fc39b7f546c"},"outputs":[],"source":"print(grid_sv.best_score_)\nprint(grid_sv.best_params_)\nprint(grid_sv.best_estimator_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d60d0b9e-1d52-5ed1-a8d9-1fbb0d05ad46"},"source":"### SVM Findings\n\nUtilizing Randomized hyper parameter search along with cross validation resulted in a SVM model with an accuracy score of **0.96** which is quite good. The model that was chosen by *RandomizedSearchCV* is as follows: {'C': 100, 'kernel': 'linear'}.\n\nI found it interesting how the linear kernel performed significantly better than the RBF. It is worth taking more time to look into the exactly how RBF and linear kernels behave. An interesting side note was the execution time on linear kernels which was much longer than RBF."},{"cell_type":"markdown","metadata":{"_cell_guid":"a1d35d89-4120-a2c1-dd0e-601db13584f7"},"source":"## Random Forest Classification"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5f7798f-fcf1-bfab-11da-52dde0e84532"},"outputs":[],"source":"rf_clf = RandomForestClassifier(random_state=42)\n\nparam_grid = {\"max_depth\": [3, None],\n              \"max_features\":  sp_randint(1, 8),\n              \"min_samples_split\": sp_randint(2, 11),\n              \"min_samples_leaf\": sp_randint(1, 11),\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a674b1eb-418d-add5-bfc3-643675879a9e"},"outputs":[],"source":"rand_rf = RandomizedSearchCV(rf_clf, param_distributions=param_grid, n_iter=100, random_state=42)\nrand_rf.fit(X,y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2729adfa-9446-741f-4fb1-af9ce3c1c801"},"outputs":[],"source":"print(rand_rf.best_score_)\nprint(rand_rf.best_params_)\nprint(rand_rf.best_estimator_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"01e19c84-9e40-325a-b12a-c284771077ea"},"source":"### Random Forest Findings\n\nHere we continue to see relatively accurate predictions. The classification accuracy is **0.95** which is pretty good given the size of the dataset."},{"cell_type":"markdown","metadata":{"_cell_guid":"7587291c-0827-ce51-c4f3-dd2570ee8475"},"source":"## AdaBoost Classifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da5917ae-70a3-fe33-b112-ee7a492f9330"},"outputs":[],"source":"# Using decision stumps due to size of sample.\n# Attempting to prevent over-fitting\nstump_clf =  DecisionTreeClassifier(random_state=42, max_depth=1)\n\nparam_grid = {\n              \"base_estimator__max_features\": ['auto', 'sqrt', 'log2'],\n              \"n_estimators\": list(range(1,500)),\n              \"learning_rate\": np.linspace(0.01, 1, num=20),\n             }"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"742afd3c-252d-f9b1-4123-7c90cfee74ac"},"outputs":[],"source":"ada_clf = AdaBoostClassifier(base_estimator = stump_clf)\n\nrand_ada = RandomizedSearchCV(ada_clf, param_grid, scoring = 'accuracy', n_iter=100, random_state=42)\nrand_ada.fit(X,y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3a77d18-75e9-cbd6-ab9c-1dc19b766ba2"},"outputs":[],"source":"print(rand_ada.best_score_)\nprint(rand_ada.best_params_)\nprint(rand_ada.best_estimator_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b15427d6-4e47-d822-ba53-70b9611385e9"},"source":"### AdaBoost Findings\n\nAs I expected, AdaBoost performed quite well. It has an accuracy of **0.97**. I decided to use a decision stump as the base estimator for a few reasons. Due to the size of the dataset I wanted to reduce the possibility of overfitting by using a very simple model. I could have achieved better results by swapping out a randomforest for the decision stump but I feel that there is a lack of generalization in that case. I'm much more confident in the generalization of this model."},{"cell_type":"markdown","metadata":{"_cell_guid":"2895951f-7c15-e8ec-18df-f0f4094b7ac2"},"source":"# Conclusion \n\nThe best model used to diagnose breast cancer from my comparative analysis is **AdaBoost** using a **Decision Stump** as its base estimator. Adaboost had a prediction accuracy of **0.97** with Support Vector Machine (**0.96**) and Random Forest (**0.95**) in close second and third.\n\nI believe that with further analysis of data, especially misclassified data, I could improve these scores further. There are also many more algorithms that I could attempt as well, but with such a small dataset I wanted to keep it relatively simple.\n\nThe largest change in performance that I found was in the feature selection phase. There are huge tradeoffs for selecting to keep certain features and it is not always very obvious. Visualizing the data the ways I did above, as well as analyzing principal component analysis aided in selecting the most useful features. \n\n\n## Notes\n\nIt is important to note that each model is chosen using K-Fold (10-Fold) cross validation with hyperparameter optimization using RandomSearchCV. I chose random search as opposed to the exhaustive solution of grid search simply for saving time."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}