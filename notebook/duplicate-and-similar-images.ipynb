{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"71e003f5-c5d7-6f03-8fa9-653093c76be2"},"source":"In this notebook I want to explore duplicate images in the dataset\n\nBy duplicate I do not mean byte-by-byte nor pixel-by-pixel comparison but rather images that cannot be visually distinguished by human\n\nTo do that I will hash the images using [pHash][1] from [imagehash][2] library\nThen I will perform an nearest neighbours search on image hashes\n\n  [1]: http://www.hackerfactor.com/blog/index.php?/archives/432-Looks-Like-It.html\n  [2]: https://pypi.python.org/pypi/ImageHash"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a29a73a8-efaf-9c15-aed4-045df41a004c"},"outputs":[],"source":"import os\nimport pandas as pd\nimport numpy as np"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe3fa7dc-2ed2-870c-2cc8-14a68ff83e79"},"outputs":[],"source":"import imagehash\nfrom PIL import Image\ndef img_hash(file, hash_size):    \n    return imagehash.phash(Image.open(file),hash_size=hash_size)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b4b2065-3b93-fa3e-e980-c4c4533a9f23"},"outputs":[],"source":"def get_hashes(directory, hash_size):\n    hash_file = 'img_hashes_%s.csv' % hash_size\n    if not os.path.isfile(hash_file):\n        hashes = pd.DataFrame()\n    else:\n        hashes = pd.read_csv(hash_file)\n    new_hashes_calculated = 0\n    num_of_files=len(os.listdir(directory))\n    for file in os.listdir(directory):\n        if 'file' not in hashes.columns or file not in list(hashes['file']):                                               \n            new_hashes_calculated = new_hashes_calculated + 1\n            result = {'file': file,'hash':img_hash(directory + '/' + file,hash_size)}\n            hashes = hashes.append(result,ignore_index=True)\n            if (new_hashes_calculated % 200 == 199):\n                hashes[['file','hash']].to_csv(hash_file,index=False) \n    if new_hashes_calculated:\n        hashes[['file','hash']].to_csv(hash_file,index=False)    \n    return read_hashes(hash_size)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2fe6958-a9ca-e648-2aad-1d854de412d0","collapsed":true},"outputs":[],"source":"def read_hashes(hash_size):\n    hash_file = 'img_hashes_%s.csv' % hash_size\n    hashes = pd.read_csv(hash_file)[['file','hash']]\n    lambdafunc = lambda x: pd.Series([int(i,16) for key,i in zip(range(0,len(x['hash'])),x['hash'])])\n    newcols = hashes.apply(lambdafunc, axis=1)\n    newcols.columns = [str(i) for i in range(0,len(hashes.iloc[0]['hash']))]\n    return hashes.join(newcols) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c094c544-41f6-a2b6-ea87-ee931972d5d5"},"outputs":[],"source":"hashes_4 = get_hashes('../input/test_stg1',4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bee352c4-f899-2fe5-3da6-d0420cf0c1f4"},"outputs":[],"source":"hashes_4.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09ad1fcd-79b7-223b-23a8-0a2317acbcff"},"outputs":[],"source":"#are there any duplicates in terms of hashes of size 4?\nprint(\"%s out of %s\" % (len(hashes_4[hashes_4.duplicated(subset='hash',keep=False)]),len(hashes_4)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"97ec717e-9218-91cf-c465-a671d8b52962"},"source":"quite a lot! as expected this hash size is probably too small. it actually can only represent 16^4 values\nlet's try with a bigger hash"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b38186a2-20ba-8b59-4aae-efacb22234f1"},"outputs":[],"source":"hashes_16_lag = get_hashes('../input/train/LAG/',16)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e02d8f5d-be70-7630-e378-77ba4280504e"},"outputs":[],"source":"hashes_16_lag.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f8cb5d8-6362-9f97-7a30-121e01c20213"},"outputs":[],"source":"#are there any duplicates in terms of hashes of size 16?\nlen(hashes_16_lag[hashes_16_lag.duplicated(subset='hash',keep=False)])"},{"cell_type":"markdown","metadata":{"_cell_guid":"dac99941-27bf-5642-6da2-206126cae02a"},"source":"nope/ok - let's find the pair of images that is closest to each other:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6d002c6-b589-cb01-297a-d1eb8dc86d79"},"outputs":[],"source":"from sklearn.neighbors import KDTree\nt = KDTree(hashes_16_lag[[str(i) for i in range(0,64)]],metric='manhattan')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc8b7348-0413-9c18-9d4f-a14bd00f8443"},"outputs":[],"source":"distances, indices = t.query(hashes_16_lag[[str(i) for i in range(0,64)]],k=2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52b2e0c4-3303-76ee-c6c6-76431adefd67"},"outputs":[],"source":"distances[:,1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5593168-6351-7754-8e1a-517613e5701d"},"outputs":[],"source":"index_of_closest_distances = np.argsort(distances[:,1])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb876e61-fed9-7b38-3fa3-f539618ab32c"},"outputs":[],"source":"distances[index_of_closest_distances[:10]]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5659e276-7b51-3686-352f-0a4ab254f780"},"outputs":[],"source":"indices_pairs_of_closest_distance = indices[index_of_closest_distances]\nindices_pairs_of_closest_distance[:10]"},{"cell_type":"markdown","metadata":{"_cell_guid":"12f778ba-b3d7-ee10-af34-cee86da9c543"},"source":"since distance is symmetric we get pair-duplicates, let's get rid of them"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4bdd5776-5dcc-e22a-7e17-509bae8bb503"},"outputs":[],"source":"unique_pairs = [pair for pair in indices_pairs_of_closest_distance if (pair == np.sort(pair)).all()]\nunique_pairs[:5]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd8a97ac-73b8-52ac-61c4-ef2b8ee4ace1"},"outputs":[],"source":"hashes_16_lag.iloc[unique_pairs[0]]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad7aab2d-c1c1-b8ef-e0a0-3958ad23ab6b"},"outputs":[],"source":"len(unique_pairs)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76d12960-19d9-873b-c55d-073c34fa556d"},"outputs":[],"source":"def read_image_bytes(filename):\n    with open(filename, mode='rb') as file:\n        return file.read()\n    \ndef read_image_numpy(filename, w, h):\n    from PIL import Image\n    from numpy import array\n    img = Image.open(filename).resize((w,h))\n    img = img.convert('RGB')\n    return array(img)\n\ndef scale(arr):\n    return arr / 255.0\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndef mosaic_images(images_tensor, ncols, grayscale=False):\n    img_size = images_tensor.shape[1]\n    col_size = ncols*(img_size +1) -1\n    nrows = int(np.ceil(images_tensor.shape[0] / ncols))\n    row_size = nrows*(img_size +1)-1\n    if grayscale:\n        final = np.ones((row_size,col_size))\n    else:\n        final = np.ones((row_size,col_size,3))\n    \n    for i in range(images_tensor.shape[0]):\n        row = int(np.floor(i / ncols))\n        col = i % ncols\n        kernel = images_tensor[i]\n        x = col * (img_size+1)\n        y = row * (img_size+1)\n        final[y:y+img_size,x:x+img_size] = kernel\n    return final"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"730d3b52-f3ab-316c-763d-a15d2fc8d19d"},"outputs":[],"source":"fromm = 0\ntoo = 10\nfile_names = hashes_16_lag.iloc[np.ndarray.flatten(np.array(unique_pairs[fromm:too]))]\nfiles_to_show = [scale(read_image_numpy('../input/train/LAG/%s' % f,400,400)) for f in list(file_names['file'])]\nlen(files_to_show)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9257e51-8952-f40f-f0b6-fe16dddf3460"},"outputs":[],"source":"## plt.figure(figsize=(10,how_many*5))\n## plt.imshow(mosaic_images(np.asarray(files_to_show),2))\n\n# You must fork this notebook to the private space since we cannot show images from the competition in public kernels"},{"cell_type":"markdown","metadata":{"_cell_guid":"0727fc23-29b1-e17e-5e17-9927a146af24"},"source":"I looked at every pair of images here and only the last pair was a pair of images different in an important way.\nThis means that for every image in train/LAG except from one - there is a very similar image\nso the hypothesis is that we could get rid of the duplicates for training\n\nLet's take another look by taking a few pictures and viewing images similar to the base one with increasing distance"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"13de5b5a-66c8-9c58-b3e1-dbeafca867a7"},"outputs":[],"source":"hashes_sample = hashes_16_lag.sample(n=10,random_state = 124)\ndistances_10, indices_10 = t.query(hashes_sample[[str(i) for i in range(0,64)]],k=18)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe650663-13b4-88a3-b070-c1861ff6a6cb"},"outputs":[],"source":"distances_10"},{"cell_type":"markdown","metadata":{"_cell_guid":"250c76b0-3273-caa5-cc04-2fc4777c961e"},"source":"we can see that there are images that have at least 8 other images within distance 100 \nand there are images which do not have even one nieghbour within distance 200\n\nlet's first look at neighbours of close distance"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ac1abad-0522-db7a-20a3-00b01bf9b381"},"outputs":[],"source":"print(distances_10[0])\nother_images = list(hashes_16_lag.iloc[indices_10[4]]['file'])\nimages = [scale(read_image_numpy('../input/train/LAG/%s' % file,500,500)) for file in other_images]\n## plt.figure(figsize=(10,20))\n## plt.imshow(mosaic_images(np.array(images),3))\n# base image is in the top left corner"},{"cell_type":"markdown","metadata":{"_cell_guid":"2796ddc7-56a9-d30a-2c2d-948749d18e09"},"source":"we can now look and maybe find the value of distance that is visually identical?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"959ef227-7f5b-19e5-28d6-c14b7cfc8b7b"},"outputs":[],"source":"# now let's look at some image without so close neighbours\nprint(distances_10[5])\nother_images = list(hashes_16_lag.iloc[indices_10[5]]['file'])\nimages = [scale(read_image_numpy('../input/train/LAG/%s' % file,500,500)) for file in other_images]\n## plt.figure(figsize=(10,20))\n## plt.imshow(mosaic_images(np.array(images),3))\n\n# base image is in the top left corner"},{"cell_type":"markdown","metadata":{"_cell_guid":"28ec0311-186b-ce76-20cc-11526c365ec7"},"source":"I guess we should be aware of the dataset but given my experiment here - \nit is hard to find a good distance metric for near-duplicate removal. maybe the hash should be longer?\nor maybe it is enough to put a small threshold on distance and get just the real obvvious duplicates removed?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b636a3a-5a68-bd24-927b-7c7e7a1ade9b"},"outputs":[],"source":"from sklearn.neighbors import KDTree\n\ndef duplicates_in_dir(directory, hash_size,threshold, return_original=None):\n    hashes = get_hashes(directory,hash_size)\n    hash_str_len = len(hashes.get_value(0,'hash'))\n    files_in_dir = os.listdir(directory)\n    hashes = hashes[hashes['file'].isin(files_in_dir)]    \n    print('calculating distances')\n    t = KDTree(hashes[[str(i) for i in range(0,hash_str_len)]],metric='manhattan')\n    distances, indices = t.query(hashes[[str(i) for i in range(0,hash_str_len)]],k=5)\n    above_threshold_idx = np.argwhere((distances<=threshold) & (distances>0))\n    pairs_of_indexes_ofduplicate_images = set([tuple(sorted([indices[idx[0],0],indices[idx[0],idx[1]]])) for idx in above_threshold_idx])\n    to_remove = [t[1] for t in pairs_of_indexes_ofduplicate_images]\n    files_to_remove = [os.path.join(directory,f) for f in list(hashes.iloc[to_remove]['file'])]\n    if return_original:\n        to_keep = [t[0] for t in pairs_of_indexes_ofduplicate_images]\n        files_to_keep = [os.path.join(directory,f) for f in list(hashes.iloc[to_keep]['file'])]\n        return (files_to_keep, files_to_remove)\n    else:\n        return files_to_remove"},{"cell_type":"markdown","metadata":{"_cell_guid":"e7e04a40-c80d-36ca-5e80-3b61846b220c"},"source":"we will now generate a list of pairs (to_keep, to_remove) from each of the class\nthe function doesn't correctly handle transitive duplicates. e .g\nif there are images A and B with distance 10 and B and C with distance 10\nand A and C with distance 20 - We will find pairs: A->B and B->C\nand as a result we will remove both B and C while C should be kept \nsince after deletion of B there is no image with distance 10 or more for C"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2470797-00c7-6b89-12dd-e85fa509b759"},"outputs":[],"source":"def get_duplicate_report():\n    for clazz in os.listdir('../input/train'):\n        if clazz != '.DS_Store':\n            base_dir = '../input/train'\n            to_keep, to_remove = duplicates_in_dir('%s/%s' % (base_dir,clazz),16,10,True)\n            yield list(zip(to_keep, to_remove))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"97380bf2-8917-2414-abc4-4d5ad4026afb"},"outputs":[],"source":"r = list(get_duplicate_report())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9aee531f-c4ba-b380-a268-23cdc6384a3f"},"outputs":[],"source":"def flatten(lists):\n        return [elem for lis in lists for elem in lis]\ntotal_report = flatten(r)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79552861-732a-5a5a-13fd-09bd518ad830"},"outputs":[],"source":"print('we have found %s duplicates in train set' % len(total_report))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0f27635-f735-e5cb-b81c-831645caa548"},"outputs":[],"source":"for i in range(0,12):\n    chunk = total_report[i*10:(i+1)*10]\n    images = [scale(read_image_numpy(c[j],400,400)) for c in chunk for j in [0,1]]\n    ## plt.figure(figsize=(8,40))\n    ## plt.imshow(mosaic_images(np.array(images),2))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3c4e692-3cf1-eb9f-beeb-d68ed92d3bb0"},"outputs":[],"source":"dups_df = pd.DataFrame(total_report)\ndups_df.columns = ['keep','remove']\ndups_df['hash_size'] = 16\ndups_df['threshold'] = 10\ndups_df.to_csv('dups_hash16_dist10.csv',index=False)\ndups_df.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"e706b84d-c141-97d2-fed5-bd815fda6e9c"},"source":"a long story it is... but let's have a look at the test duplicates as well:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c1bf9c9f-35de-1463-c951-9d65d2fb90d8"},"outputs":[],"source":"to_keep, to_remove = duplicates_in_dir('../input/test_stg1/',16,10,True)\ntest_dups = list(zip(to_keep,to_remove))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e937aa50-778c-3a37-c2c7-2d8d68c45117"},"outputs":[],"source":"dups_test_df = pd.DataFrame(test_dups)\ndups_test_df.columns = ['keep','remove']\ndups_test_df.to_csv('dups_test_hash16_dist10.csv',index=False)\ndups_test_df.head()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}