{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"## Is artificial intelligence less than our intelligence? ¡ªSpike Jonze\n\nThis kernel has the been divided into following section\n<br/>\n* Introduction\n* Data Loading in PyTorch\n* Architecture of Capsule Network\n* Loss function for Capsule Network\n* Training the model\n\n\n### Introduction - Capsule Networks\nThis kernel is a very elementary primer on Capsule network and implementing it using PyTorch. At the end of this tutorial, you will have hands on grash on capsule networks and pytorch. Capsnets reject the pooling layer strategy of conventional CNNs that reduces the amount of detail to be processed at the next higher layer.\n**A capsule network takes input an image and finds ouoto what kind of object is present and what are the instatiation parameters(rotation, thickness, etc).**\n* **Why capsule networks**\n<br/>\nPooling allows a degree of translational invariance (it can recognize the same object in a somewhat different location) and allows a larger number of feature types to be represented. The main disadvantage of Pooling is that pooling can't explore the spatial relationship and provides invariance instead of equivariance i.e translation of input features results in an equivalent translation of outputs. In brief, CNN contains pooling layer which can't capture spatial relationship like poses of an entity. But capsule is designed in such a way that it can easily capture these features. The mathematical interpretation of invariance and equivariance is described below:\n\nWith a structure of a group G, g being one specific translation operator, A function or feature f is ***invariant*** under G if for all images in a class, and for any g\n$$\nf(g(I)) = f(I)\n$$\nwhereas\nA function or feature is ***equivariant*** under G is for all the images a class, there exists a unique **G'**\n$$\nf(g(I)) = g'(f(I))\n$$\n\n![](https://image.slidesharecdn.com/equivarianceinvariance-180201075728/95/brief-intro-invariance-and-equivariance-4-638.jpg?cb=1517472411 | width=50) \n\n![](https://image.slidesharecdn.com/equivarianceinvariance-180201075728/95/brief-intro-invariance-and-equivariance-6-638.jpg?cb=1517472411 | width=50)\n\n**A capsule network is composed of many capsules. Let's peep into them.**\n### CAPSULE\nA capsule is a group of neurons whose activity vectors represent various instantiation parameters of an object. For classifying an entity to class, there should be an agreement between multiple capsules. Basically, this process goes like this:\n* Active capsules at the lower level makes prediction for the instantiation parameters of higher level capsules.Tthe lower level capsules represents lower level features like presence of rectangle or triangle whereas the higher level features represents boat or house.\n* When multiple capsules of a lower level come to an agreement, the capsules at higher level become active. Hence, capsule nets are able to establish a hierarchy of features between various levels of capsules. For eg, from an image of boat, capsnet can capture the presence of a triangle over a rectangle entity as a whole and as we move down the lower levels, it can capture features like presence of rectangle and a triangle.\n* The probability of presence of entity is determined by the length of the activity vectors and the vector's orientation tells about the properties of an entity.\n\n\nThe length of the activity vector is fed into a non-linear transformation called the ***squashing function*** so that the length of output vector becomes less than 1 because it is used to represent probability .\n\n<img src=\"https://cdn-images-1.medium.com/max/2000/1*RB03q9MINVgRPK_VBcBnsg.png\" data-canonical-src=\"https://cdn-images-1.medium.com/max/2000/1*RB03q9MINVgRPK_VBcBnsg.png\" width=\"500\" height=\"400\" />\n\nThe main component of capsule network is the **routing by agreement** strategy by which capsule of different levels interact with each other about which we will discuss in later sections."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch.nn.functional as F\nimport pdb\nfrom torch import nn\nimport math\nfrom torch.optim import lr_scheduler\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport torch\nimport itertools\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets.mnist import MNIST\nfrom tqdm import tqdm\nfrom torchvision import models\nimport torch.optim as optim\nimport torchvision\nfrom matplotlib.ticker import MaxNLocator\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom torch.nn import MaxPool2d\n!pip install torchsummary\nfrom torchsummary import summary\nimport chainer.links as L\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.ion()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68c0b7a0f07b23ef7c288d3ca84d2b30d64d5142"},"cell_type":"code","source":"df = pd.read_csv('../input/fashion-mnist_train.csv')\nY = df['label'].values\nX = df.drop('label', axis = 1).values.reshape(df.shape[0], 28, 28)\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = .25, random_state = 1)\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nfig = plt.figure(figsize=(12, 9))\nsns.set_style(\"white\")\nax = sns.barplot(x=classes, y=pd.value_counts(df['label'].values), linewidth=5,edgecolor=sns.color_palette(\"dark\", 10))\nplt.title(\"Frequency of classes\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"019235818cce37d27c13f1bb5666d458bb68fd29"},"cell_type":"markdown","source":"\n\n# Data Loading in PyTorch\nFor those new to pytorch can head [here](https://www.kaggle.com/ankitjha/the-ultimate-pytorch-cookbook) for quick tutorial.\n<br/>\n*Just for a basic description*\n<br/>\nData loading in PyTorch can be separated in 2 parts: \n\n* Data must be wrapped on a Dataset parent class where the methods getitem and len must be overrided. Not that, the data is not loaded on memory by now.\n* The Dataloader reads the data and puts it into memory\n\n#### torchvision\nIt is used to load and prepare dataset. Using it you can create transformations on the input data. \n\n#### transforms\nIt is used for preprocessing images and performing operations sequentially. \n\n#### num_workers\nIt is used for multiprocessing.Normally, $$num workers = 4 * (number of gpus)$$ works well."},{"metadata":{"trusted":true,"_uuid":"11cd4e953dbdd86f3d0bbfb13dd101d1252d2807"},"cell_type":"code","source":"class DatasetProcessing(Dataset):\n    def __init__(self, data, target, transform=None): #used to initialise the class variables - transform, data, target\n        self.transform = transform\n        self.data = data.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n        self.target = torch.from_numpy(target).long() # needs to be in torch.LongTensor dtype\n    def __getitem__(self, index): #used to retrieve the X and y index value and return it\n        return self.transform(self.data[index]), self.target[index]\n    def __len__(self): #returns the length of the data\n        return len(list(self.data))\ntransform = transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])\ndset_train = DatasetProcessing(X_train, y_train, transform)\ntrain_loader = torch.utils.data.DataLoader(dset_train, batch_size=32,\n                                          shuffle=True, num_workers=4)\ndset_test = DatasetProcessing(X_test, y_test, transform)\ntest_loader = torch.utils.data.DataLoader(dset_test, batch_size=32,\n                                          shuffle=True, num_workers=4)\nclasses = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\nfig = plt.figure(figsize=(12,6))\nfor num, x in enumerate(X_train[0:5]):\n    plt.subplot(1,5,num+1)\n    plt.axis('off')\n    plt.imshow(x)\n    plt.title(classes[y_train[num]])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d8be288fda78667e37fb7237415adf1d624de1f"},"cell_type":"markdown","source":"# Architecture\n<img src=\"https://cdn-images-1.medium.com/max/1600/1*IqMwp2BJzuviCuZ8r-pO6w.png\" data-canonical-src=\"https://cdn-images-1.medium.com/max/1600/1*IqMwp2BJzuviCuZ8r-pO6w.png\" width=\"700\" height=\"700\" />\n\nThe levels of Capsule network can be divided as follows:\n* Level 1 :  **Input layer** <br/>\nAt this level, the input image is fed into the net.\n* Level 2 : **CNN layer** <br/>\nAt this level,  *9 x 9 dim filter* are applied on entire image *stride of 2* and *256 layers* followed by relu activation function. The output dimension becomes ***20 x 20 x 256*** \n* Level 3 : **Primary Capsules** <br/>\nThe primary capsules represent entities of the lowest level. The primary capsules are connected to the digit capsules. Therefore the total capsule units are 32 * 6 * 6 with 6* 6 sharing the same weight vector $W_{ij}$ as shown in the above figure. Each capsule unit is an 8D vector.\n* Level 4 : **Digit Capsules** <br/>\nThe Digit capsules represent entities of higher level and become active by agreement of primary capsules. *There are 10 caspsules in this layer each having 16 dimensions*.\n<br/>\n\nThe  entire control flow take this form\n![](https://blog.keras.io/img/ae/autoencoder_schema.jpg)\n\nwhere ***the encoder structure takes the image as an input and follows the above mentioned pipeline*** and  ***the decoder structure is used to construct the input image from the digit caps level***. This reconstruction of original image ensures that the model is not overfitted and thus acts as a **regularizer**."},{"metadata":{"_uuid":"b8f3f299d2bb03169ebfe8160e1bbb4147a70cd8"},"cell_type":"markdown","source":"# Routing By agreement\n\nThis is the pivotal part of capsule network\n<br/>\nInitially when the feature maps have been produced by the CNN layer, the capsules at the lower level try to predict the output of the capsules at the higher level. When the capsules at the lowel level come upon an agreement over the presence of an entity at higher level, the capsule corresponding to that entity becomes active. Hence, other capsules are rejected and this is instrumental in removing noise  from the model. Therefore, when there is a strong agreement between the capsules at the lower levels for the activity of the capsules at the higher level, the capsules at the lower level are **routed** to the capsules at higher level.\n\n\n**Routing pseudocode**\n( $\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}$ : *predictions made by i*, $r$ : *number of iterations*, $l$: *num_layer*)\n>procedure( $\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}$, $r$, $l$)\n\n> for all capsule $i$ in layer $l$ and capsule $j$ in layer $l+1$: $b_{ij} := 0$ \n\n> for $r$ iterations do:\n\n>> for all capsules $i$ in the layer $l$: $c_{ij} = softmax(b_{ij})$            ->  **eqn {$i$}**\n>>> ***This is the coupling coefficient found by calculating the probability of the routing weights of the layer.***\n\n>>for all capsules $j$ in the layer $l+1$: $s_{ij} = \\sum_{i=1}^{m}{c_{ij}\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}}$         ->  **eqn {$ii$}**\n>>> ***This step calculates the weighted sum of all the predictions by layer $i$ for each capsule in layer j.***\n\n>>for all capsules $j$ in the layer $l+1$: $\\textbf{v}_j = \\textbf{squash}(\\textbf{s}_{ij})$\n>>> ***This non-linear activation function known as squashing function makes the value in range {$0, 1$}.***        ->  **eqn {$iii$}**\n\n>>for all capsule $i$ in layer $l$ and capsule $j$ in layer $l+1$: $b_{ij} = b_{ij} + \\boldsymbol{\\hat{\\textbf{u}}}_{j|i} \\cdot \\textbf{v}_j$\n>>>***This step is used to modify the routing weights according to the predictions made by capsules in layer $i$. ***     ->  **eqn {$iv$}**\n\n>return $\\textbf{v}_j$\n\nLet us now implement this code step by step to get the intuition and understand it properly. Please fork this notebook if you want to execute the code below."},{"metadata":{"trusted":true,"_uuid":"d63101bb2ad0fa4a2afd18ff02205f74dd4e7b59"},"cell_type":"code","source":"class CapsuleLevel(nn.Module):\n    def __init__(self, in_channels, out_channels, capsule_dimension = 8, num_capsules = 10, num_capsule_units = 6 * 6 * 32, routing = False, iterations=3):\n        super(CapsuleLevel, self).__init__()\n        self.routing = routing\n        self.iterations = iterations\n        self.num_capsules = num_capsules\n        self.num_capsule_units = num_capsule_units\n        if routing == True:\n            self.route_weights = nn.Parameter(torch.randn(num_capsule_units, num_capsules, out_channels, in_channels))\n        else:\n            self.capsules = nn.ModuleList(\n                [nn.Conv2d(in_channels, out_channels, kernel_size=(9, 9), stride=(2, 2), padding=0) for _ in\n                 range(capsule_dimension)])\n\n            \n    # The squash function\n    def squash(self, s, dim=-1):\n        norm = torch.sum(s**2, dim=dim, keepdim=True)\n        return norm / (1 + norm) * s / (torch.sqrt(norm) + 1e-8)\n\n    def forward(self, x):\n        if self.routing == True:          # Routing by Agreement\n            batch_size = x.size(0)\n            \n            route_weights = torch.stack([self.route_weights] * batch_size, dim = 0) #dim:([1152, 10, 16, 8])\n            x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4) #dim:([128, 1152, 10, 8, 1])\n            u_hat = torch.matmul(self.route_weights, x)  #dim:([128, 1152, 10, 16, 1])\n            u_hat = u_hat.squeeze(-1)   #dim([128, 1152, 10, 16])\n            temp_u_hat = u_hat.detach()\n            \n            #b_ij dim:([128, 1152, 10, 1])\n            b_ij = Variable(torch.zeros(batch_size, self.num_capsule_units, self.num_capsules, 1).cuda()) \n            for iteration in range(self.iterations):\n                c_ij = F.softmax(b_ij, dim=1)   # Equation 1 # c_ij dim:([128, 1152, 10, 1])\n                s_ij = (c_ij * temp_u_hat).sum(dim=1)  # Equation 2 # s_ij dim:([128, 10, 16])\n                temp_u_hat = temp_u_hat.unsqueeze(3)  # temp_u_hat dim:([128, 1152, 10, 16, 1])\n                v_j = self.squash(s_ij, dim=2) # Equation 3  # v_j dim:([128, 10, 16])\n                v_j_i = torch.stack([v_j] * self.num_capsule_units, dim = 1).unsqueeze(-1) # v_j_i dim:([128, 10, 16, 1])\n                v_j_i = torch.matmul(temp_u_hat, v_j_i).squeeze(3)  \n                temp_u_hat = temp_u_hat.squeeze(3)\n                b_ij = b_ij + v_j_i # Equation 4\n        else:\n            v_j = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]   \n            v_j = torch.cat(v_j, dim=-1)\n            v_j = self.squash(v_j)\n\n        return v_j","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da915a2281b41abba6502b811b53fb078828ae8b"},"cell_type":"code","source":"class CapsuleNetwork(nn.Module):  \n    def __init__(self):\n        super(CapsuleNetwork, self).__init__()\n        self.batch_size = 32\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=(9, 9), stride=(1, 1), padding=0)\n        self.primaryCaps = CapsuleLevel(in_channels=256, out_channels=32, capsule_dimension=8)\n        self.digitCaps   = CapsuleLevel(in_channels=8, out_channels=16, num_capsules=10, routing=True)\n        self.decoder     = nn.Sequential(\n            nn.Linear(16 * 10, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, 784),\n            nn.Sigmoid()\n        )\n    def forward(self, x, y=None):\n        x = F.relu(self.conv1(x), inplace=True)\n        x = self.primaryCaps(x)\n        x = self.digitCaps(x)\n        classes = (x ** 2).sum(dim=-1) ** 0.5\n        classes = F.softmax(classes, dim=-1)\n        if y is None:\n            # Get most active capsule\n            _, max_length_indices = classes.max(dim=1)\n            y = Variable(torch.eye(10)).cuda().index_select(dim=0, index=Variable(max_length_indices.data))\n        reconstructions = self.decoder((x * y[:, :, None]).view(x.size(0), -1))\n        return classes, reconstructions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"836dee27061d0d0d4cce0aed9e6f0fe78c5f4969"},"cell_type":"markdown","source":"# Margin Loss for capsule networks\n\nThe loss function for capsule network is defined as\n\n![](https://cdn-images-1.medium.com/max/1493/1*y-bVFuiLReqSSdmdZ6wAmA.png)\n\nThe loss function which is used for Capsule networks is **linear combination of margin loss function and reconstruction loss**. The first part of the margin loss is calculated for correct prediction i.e. it is 0 in case of output with > 0.9 probability, non-zero otherwise. The second part is calculated for incorrect prediction i.e. 0 when incorrect prediction with probability < 0.1 is prediction, non-zero otherwise. The reconstruction loss is the maximum likelihood estimation loss between the reconstruction of the image using encoder-decoder structure and the original image."},{"metadata":{"trusted":true,"_uuid":"ace5ceeef4683f9a56db1a9076f67d7d623527aa"},"cell_type":"code","source":"class LossFunction(nn.Module):\n    def __init__(self):\n        super(LossFunction, self).__init__()\n        self.reconstruction_loss = nn.MSELoss(size_average=False)\n\n    def forward(self, img, target, classes, reconstructions):\n        fn_1 = F.relu(0.9 - classes, inplace=True) ** 2  # Calculated for correct digit cap\n        fn_2 = F.relu(classes - 0.1, inplace=True) ** 2  # Calculated for incorrect digit cap\n        margin_loss = target * fn_1 + 0.5 * (1. - target) * fn_2\n        margin_loss = margin_loss.sum()\n        img = img.view(reconstructions.size()[0], -1)\n        reconstruction_loss = self.reconstruction_loss(reconstructions, img)\n        return (margin_loss + 0.0005 * reconstruction_loss) / img.size(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61c3d1574b697c04e77d08dd8dcc41c50c4f693a"},"cell_type":"markdown","source":"# Training Capsule Network"},{"metadata":{"trusted":true,"_uuid":"4c6ee74c82494432ec00451db60dd2ee4cda6fc3"},"cell_type":"code","source":"train_loss = []\ntest_loss = []\ndef train(train_loader, epoch):\n    global model\n    model.train()\n    if torch.cuda.is_available():\n        model = model.cuda()\n    tr_loss = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        batch_size = data.size(0)\n        labels = target\n        data, target = Variable(data), Variable(target)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        optimizer.zero_grad()\n        capsule_loss = LossFunction()\n        labels = torch.LongTensor(labels)\n        labels = torch.eye(10).index_select(dim=0, index=labels)\n        labels = Variable(labels).cuda()\n        optimizer.zero_grad()\n        classes, reconstructions = model(data, labels)\n        loss = capsule_loss(data, labels, classes, reconstructions)\n        loss.backward()\n        optimizer.step()\n        tr_loss += loss.item()\n        pred = classes.data.max(1, keepdim=True)[1]\n        if (batch_idx + 1)% 500 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n                100. * (batch_idx + 1) / len(train_loader), loss.item()))\n    train_loss.append(tr_loss / len(train_loader))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad96cbe8bedcd2c0d722396ff47a6b94acecd3ea"},"cell_type":"markdown","source":"## Evaluating the training"},{"metadata":{"trusted":true,"_uuid":"d2f1774ab74c9f95d41ee0f3f89e530383fe84d1"},"cell_type":"code","source":"\ndef evaluate(data_loader):\n    global model\n    model.eval()\n    loss = 0\n    \n    for data, target in data_loader:\n        labels = target\n        data, target = Variable(data, volatile=True), Variable(target)\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        labels = torch.LongTensor(labels)\n        labels = torch.eye(10).index_select(dim=0, index=labels)\n        labels = Variable(labels).cuda()\n        classes, reconstructions = model(data)\n        capsule_loss = LossFunction()\n        loss += capsule_loss(data, labels, classes, reconstructions).item()\n        pred = classes.data.max(1, keepdim=True)[1]\n        \n    loss /= len(data_loader.dataset)\n    test_loss.append(loss)\n    print('\\nAverage Validation loss: {:.6f}\\n'.format(loss))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"849e6dcd5d5010e767fc494a68f22ef9377d14be"},"cell_type":"markdown","source":"**One limitation of capsule networks is that it takes a lot of time to train. Therefore, I have limited this kernel to 5 epochs so as to be in the kernel time limit. You can copy this code in your local system and run for many epochs. Thus the accuracy is  low initially but you will see better accuracy after 100 epochs. Note that this also depends on the learning rate and optimizer.**\n\nLet's have a look at the parameters and shape of the layers."},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(CapsuleNetwork().cuda(), input_size=(1, 28, 28))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dca3f5f0a1ac1aa63532a6eac46752c662483f49"},"cell_type":"code","source":"model = CapsuleNetwork()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nn_epochs = 15\nfor epoch in range(n_epochs):\n    train(train_loader, epoch)  #Training the model\n    evaluate(test_loader)  #evaluating the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff12af1618c4ba1cad517f53ba86348b9b960585"},"cell_type":"code","source":"plt.style.use('ggplot')\nfig = plt.figure(figsize=(20,4))\nax = fig.add_subplot(1, 2, 1)\nplt.title(\"Train Loss\")\nplt.plot(list(np.arange(15) + 1) , train_loss, label='train')\nplt.xlabel('num_epochs', fontsize=12)\nplt.ylabel('train_loss', fontsize=12)\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.legend(loc='best')\nax = fig.add_subplot(1, 2, 2)\nplt.title(\"Validation Loss\")\nplt.plot(list(np.arange(15) + 1), test_loss, label='test')\nplt.xlabel('num_epochs', fontsize=12)\nplt.ylabel('vaidation _loss', fontsize=12)\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b1c1b415fc00e558b44c52f3ae373f160dc6211b"},"cell_type":"markdown","source":"## References\n\n* [Dynamic Routing between Capules](https://arxiv.org/pdf/1710.09829.pdf)\n* [capsule-networks](https://github.com/gram-ai/capsule-networks)"},{"metadata":{"_uuid":"5d16db1ee0082dee37df909cb68af283c9f4f4fb"},"cell_type":"markdown","source":"* **Please upvote if you liked it and feel free to share your feedback**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}