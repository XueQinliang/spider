{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Spotify Music - EDA \nIn continuation of previous kernel about spotify music data extraction -Part 1 \nhttps://www.kaggle.com/pavansanagapati/spotify-music-api-data-extraction-part1\n\nWe now will use the data extracted from Spotify to perform two steps as follows\n\n#### 1. Explore the Audio Features and analyze\n#### 2. Build a Machine Learning Model \n\n## 1. Explore the Audio Features and analyze","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import Libraries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import metrics\nfrom sklearn import svm\n%matplotlib inline\nimport pandas_profiling ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let us first analyse at high level the data in the spotify music dataframe that we build by accessing the spotify data as shown in part 1 of this kernel https://www.kaggle.com/pavansanagapati/spotify-music-api-data-extraction-part1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_df = pd.read_csv('../input/spotify-music-data/spotify_music.csv')\nspotify_music_df.profile_report()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now add few more dataframes available datasets in kaggle for our deeper analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_other_df = pd.read_csv('../input/spotifyclassification/data.csv')\nspotify_music_other_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_other_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_other_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **Important Note**: Considered only those columns which are related to audio features as follows :\n\n**Acousticness :** A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n\n**Danceability** : Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n\n**Energy** : Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n**Instrumentalness**: Predicts whether a track contains no vocals. ¡°Ooh¡± and ¡°aah¡± sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly ¡°vocal¡±. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n\n**Liveness**: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n\n**Loudness**: he overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n\n**Speechiness**: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n\n**Valence**: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n**Tempo**: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create data frame with features\ndef features(df,who):\n    if who == 1:\n         features = df.loc[: ,['acousticness', 'danceability','energy','instrumentalness','liveness', 'loudness','speechiness', 'tempo','valence']]         \n    elif who == 0 :   \n          features = df.loc[:,['acousticness', 'danceability', 'energy', 'instrumentalness','liveness', 'loudness', 'speechiness', 'tempo', 'valence','popularity']]           \n    else:\n        return 'Error'\n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_other_audio_features_df = features(spotify_music_other_df, 1)\nspotify_music_other_audio_features_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_audio_features_df = features(spotify_music_df,0)\nspotify_music_audio_features_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us find the no of records for both datasets with respect to artist\nspotify_music_other_df.artist.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_df.album.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let create a dictionary in which the keys are the artists of both dataframes and the values are the total of songs for each singer or group.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_df['album'].value_counts().head(50).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_other_df['artist'].value_counts().head(100).plot(kind='barh', figsize=(20,20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualise the data:\nWe will plot a Bar chart and a Radar Chart showing the means of the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of features\nN = len(spotify_music_audio_features_df.mean())\n# Array with the number of features\nind = np.arange(N) \nwidth = 0.35  \n\n#Bar plot with Micheal Jackson data\nplt.barh(ind, spotify_music_audio_features_df.mean() , width, label='Spotify Music Data - Micheal Jackson', color = 'blue')\n#X- label\nplt.xlabel('Mean', fontsize = 12)\n# Title\nplt.title('Mean Values of the Audio Features for Micheal Jackson')\n#Vertical ticks\nplt.yticks(ind + width / 2, (list(spotify_music_audio_features_df)[:]), fontsize = 12)\n#legend\nplt.legend(loc='best')\n# Figure size\nplt.rcParams['figure.figsize'] =(8,8)\n# Set style\nstyle.use(\"ggplot\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of features for other artists\nN = len(spotify_music_other_audio_features_df.mean())\n# Array with the number of features\nind = np.arange(N) \nwidth = 0.35  \n\n#Bar plot with Other artists data\nplt.barh(ind, spotify_music_other_audio_features_df.mean() , width, label='Spotify Music Data - Other Artists', color = 'red')\n#X- label\nplt.xlabel('Mean', fontsize = 12)\n# Title\nplt.title('Mean Values of the Audio Features for Other Artists')\n#Vertical ticks\nplt.yticks(ind + width / 2, (list(spotify_music_other_audio_features_df)[:]), fontsize = 12)\n#legend\nplt.legend(loc='best')\n# Figure size\nplt.rcParams['figure.figsize'] =(8,8)\n# Set style\nstyle.use(\"ggplot\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels= list(spotify_music_audio_features_df)[:]\nstats= spotify_music_audio_features_df.mean().tolist()\n\n\nangles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n\n# close the plot\nstats=np.concatenate((stats,[stats[0]]))\n\nangles=np.concatenate((angles,[angles[0]]))\n\n#Size of the figure\nfig=plt.figure(figsize = (18,18))\n\nax = fig.add_subplot(221, polar=True)\nax.plot(angles, stats, 'o-', linewidth=2, label = \"Micheal Jackson\", color= 'blue')\nax.fill(angles, stats, alpha=0.25, facecolor='blue')\nax.set_thetagrids(angles * 180/np.pi, labels , fontsize = 13)\n\n\nax.set_rlabel_position(250)\nplt.yticks([0.2 , 0.4 , 0.6 , 0.8  ], [\"0.2\",'0.4', \"0.6\", \"0.8\"], color=\"blue\", size=12)\nplt.ylim(0,1)\n\n\nax.set_title('Mean Values of the audio features for Micheal Jackson')\nax.grid(True)\n\nplt.legend(loc='best', bbox_to_anchor=(0.1, 0.1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels= list(spotify_music_other_audio_features_df)[:]\nstats= spotify_music_other_audio_features_df.mean().tolist()\n\n\nangles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n\n# close the plot\nstats=np.concatenate((stats,[stats[0]]))\n\nangles=np.concatenate((angles,[angles[0]]))\n\n#Size of the figure\nfig=plt.figure(figsize = (18,18))\n\nax = fig.add_subplot(221, polar=True)\nax.plot(angles, stats, 'o-', linewidth=2, label = \"Other Artists\", color= 'red')\nax.fill(angles, stats, alpha=0.25, facecolor='red')\nax.set_thetagrids(angles * 180/np.pi, labels , fontsize = 13)\n\n\nax.set_rlabel_position(250)\nplt.yticks([0.2 , 0.4 , 0.6 , 0.8  ], [\"0.2\",'0.4', \"0.6\", \"0.8\"], color=\"red\", size=12)\nplt.ylim(0,1)\n\n\nax.set_title('Mean Values of the audio features for other artists')\nax.grid(True)\n\nplt.legend(loc='best', bbox_to_anchor=(0.1, 0.1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The standard deviation of the audio features themselves do not give us much information ( as we can see in the plots below), we can sum them up and calculate the mean of the standard deviation of the lists.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(221)\n\nspotify_music_audio_features_df.std().sort_values(ascending= False).plot(kind = 'bar', color = 'lightslategray')\n\nplt.xlabel('Features', fontsize = 14)\nplt.ylabel('Standard Deviation', fontsize = 14)\nplt.title(\"Standard Deviation of Micheal Jackson Audio Features\")\n\nplt.subplot(222)\n\nspotify_music_other_audio_features_df.std().sort_values(ascending= False).plot(kind = 'bar', color = 'mediumvioletred')\n\nplt.xlabel('Features', fontsize = 14)\nplt.ylabel('Standard Deviation', fontsize = 14)\nplt.title(\"Standard Deviation of Other Artist Audio Features\")\nplt.rcParams['figure.figsize'] =(20,20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Correlation Between Variables\n\nWe will correlate the feature **valence** which describes the musical positiveness with **danceability** and **energy**.\n\n\n#### Valence and Energy\nThe correlation between valence and energy shows us that there is a conglomeration of songs with high energy and a low level of valence. This means that many of my energetic songs sound more negative with feelings of sadness, anger and depression ( NF takes special place here haha). whereas when we look at the grays dots we can see that as the level of valence - positive feelings increase, the energy of the songs also increases. Although her data is split , we can identify this pattern which indicates a kind of 'linear' correlation between the variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nstyle.use('seaborn')\nspotify_music_audio_features_df.plot(kind='scatter',x='valence', y='energy',ax = ax ,c='red', colormap = 'Accent_r' ,title=\"Valence x Energy for Micheal Jackson\")\nax.set_xlabel(\"Valence\")\nax.set_ylabel(\"Energy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nstyle.use('seaborn')\nspotify_music_other_audio_features_df.plot(kind='scatter',x='valence', y='energy',ax = ax ,c='red', colormap = 'viridis_r' ,title=\"Valence x Energy for other artists\")\nax.set_xlabel(\"Valence\")\nax.set_ylabel(\"Energy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Valence and Danceability","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots()\nspotify_music_audio_features_df.plot(kind = 'scatter', x = 'valence', y = 'danceability', c = 'red',ax = ax, colormap = 'Accent_r', title = 'Valence x Danceability for Micheal Jackson')\nax.set_xlabel(\"Valence\")\nax.set_ylabel(\"Danceability\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots()\nspotify_music_other_audio_features_df.plot(kind = 'scatter', x = 'valence', y = 'danceability', c = 'red',ax = ax, colormap = 'Accent_r', title = 'Valence x Danceability for other artists')\nax.set_xlabel(\"Valence\")\nax.set_ylabel(\"Danceability\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. The Machine Learning Approach\nI will be using different algorithms as I improve this kernel notebook to improve the model accuracy.So please keep watching this space on a frequent basis.\n\nRemoving Features\nThe first step is to preprocess our data set in order to have a dataframe with numerical values in all of the columns. So let's start off dropping all features which are not relevant to our model such as id, album, name, uri, popularity and track_number and separate the target from other artist dataframe. We can easily do that by building the function feature_elimination which receives a list with the features we want to drop as a parameter.\n\nNotice that after its removal, we still have a categorical feature (artist). So, we'll have to deal with that in the second step. Also, important to mention that we have two slightly balanced classes which indicate whose list the song belongs to.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_elimination(features_list):\n    for i in features_list:\n        spotify_music_other_df.drop(i, axis = 1, inplace = True)\n    return ';)'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_other_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_elimination(['Unnamed: 0', 'song_title', 'duration_ms', 'time_signature', 'mode', 'key'])\nspotify_music_other_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove target column from our data set\ntarget = spotify_music_other_df['target']\nspotify_music_other_df.drop('target', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us observe how the data is ? Is it balanced or not .Let us see.\ntarget.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it is well balanced dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_other_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Label Encoder\nThe second task is to transform all categocal data (artists names) into numeric data. Why do we have to do that? Well, the ML algorithm only accepts numerical data, hence, the reason why we have to use the class LabelEncoder to encode each artist name into a specific number. The encoding process is shown below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Label Encoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# create Label Encoder instance\nlabel_encoder = LabelEncoder()\n\n# Set the artist labels\nartist_labels = label_encoder.fit_transform(spotify_music_other_df.artist)\n\n#Create column containing the labels\nspotify_music_other_df['labels_artists'] = artist_labels\n\n#Remove artist column as it contains categorical data\nfeature_elimination(['artist'])\nspotify_music_other_df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spotify_music_other_df.labels_artists.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6135ad1b7f034266e1f6db3633b021889ea3a27"},"cell_type":"markdown","source":"# Music CLASSIFICATION\n\n\n### Introduction\nWhen we get started with data science, we start with simple projects like Loan Prediction problem or Big Mart Sales Prediction. These problems have structured data arranged neatly in a tabular format i.e we are spoon-fed the hardest part in data science pipeline.The datasets in real life are much more complex and unstructured format like audio/image, collect it from various sources and arrange it in a format which is ready for processing. \n\n\nI have choosen an unstructured data as this problem of urban sound classification as it represents huge under-exploited opportunity. It is closer to how we communicate and interact as humans. It also contains a lot of useful & powerful information. For example, if a person speaks; you not only get what he / she says but also what were the emotions of the person from the voice.Also the body language of the person can show you many more features about a person, because actions speak louder than words! So in short, unstructured data is complex but processing it can reap easy rewards.\n\n\n#### So what is audio data really mean ? \n\nLets understand this with some theory before we actually jump in the real problem and its solution.\n\nDirectly or indirectly, you are always in contact with audio. Your brain is continuously processing and understanding audio data and giving you information about the environment. A simple example can be your conversations with people which you do daily. This speech is discerned by the other person to carry on the discussions. Even when you think you are in a quiet environment, you tend to catch much more subtle sounds, like the rustling of leaves or the splatter of rain. This is the extent of your connection with audio.\n\nSo in order to catch this audio floating around us there are devices which record in computer readable format. Examples of these formats are\n\n- wav (Waveform Audio File) format\n- mp3 (MPEG-1 Audio Layer 3) format\n- WMA (Windows Media Audio) format\n\nAudio typically looks like a wave like format of data, where the amplitude of audio change with respect to time. This can be pictorial represented as follows.\n\n![](sound.png)\n\n\nReal Time Applications of Audio Processing include but not limited\n\n- Indexing music collections according to their audio features.\n- Recommending music for radio channels\n- Similarity search for audio files (aka Shazam)\n- Speech processing and synthesis ¨C generating artificial voice for conversational agents \n\n#### Data Handling in audio domain\n\nAudio data has a couple of preprocessing steps which have to be followed namely,\n\n- Firstly Load the data into a machine understandable format. \n    For this, we simply take values after every specific time steps. For example; in a 2 second audio file, we extract values at half a second. This is called ***sampling of audio data***, and the rate at which it is sampled is called the ***sampling rate***.\n    In this approach we have disadvantage i.e  When we sample an audio data, we require much more data points to represent the whole data and also, the sampling rate should be as high as possible.To offset this we can look at second approach.\n\n- The second approach of representing audio data is by converting it into a different domain of data representation, namely the ***frequency domain*** which require lesser computational space is required. . \n\nNow let us get more idea on this in detail\n\n![](time_freq.png)\n\nHere, we separate one audio signal into 3 different pure signals, which can now be represented as three unique values in frequency domain.\n\nThere are a few more ways in which audio data can be represented, for example. using MFCs (Mel-Frequency cepstrums. PS: We will cover this in the later article). These are nothing but different ways to represent the data.\n\nNow the next step is to extract features from this audio representations, so that our algorithm can work on these features and perform the task it is designed for. Here¡¯s a visual representation of the categories of audio features that can be extracted.\n\n![](audio-features.png)\n\n\nAfter extracting these features, it is then sent to the machine learning model for further analysis.\n\nNow enough theory.Lets jump into solving the Urban Sound Classifcation Problem\n\n### Objective\n\nThe automatic classification of environmental sound is a growing research field with multiple applications to largescale, content-based multimedia indexing and retrieval. In particular, the sonic analysis of urban environments is the subject of increased interest, partly enabled by multimedia sensor networks, as well as by large quantities of online multimedia content depicting urban scenes.\n\nHowever, while there is a large body of research in related areas such as speech, music and bioacoustics, work on the analysis of urban acoustic environments is relatively scarce.Furthermore, when existent, it mostly focuses on the classification of auditory scene type, e.g. street, park, as opposed to the identification of sound sources in those scenes, e.g.car horn, engine idling, bird tweet. \n\n\n\nThere are primarily two major challenges with urban sound research namely\n\n- Lack of labeled audio data. Previous work has focused on audio from carefully produced movies or television tracks from specific environments such as elevators or office spaces and on commercial or proprietary datasets . The large effort involved in manually annotating real-world data means datasets based on field recordings tend to be relatively small (e.g. the event detection dataset of the IEEE AASP Challenge consists of 24 recordings per each of 17 classes).\n\n- Lack of common vocabulary when working on urban sounds.This means the classification of sounds into semantic groups may vary from study to study, making it hard to compare results\n\nso the objective of this notebook is to address the above two mentioned challenges.\n\n\n### Data\n\nThe dataset is called UrbanSound and contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: -\nThe dataset contains 8732 sound excerpts (<=4s) of urban sounds from 10 classes, namely:\n\n- Air Conditioner\n- Car Horn\n- Children Playing\n- Dog bark\n- Drilling\n- Engine Idling\n- Gun Shot\n- Jackhammer\n- Siren\n- Street Music\n\nThe attributes of data are as follows:\n\nID ¨C Unique ID of sound excerpt\n\nClass ¨C type of sound\n\nThe evaluation metric for this problem is \"Accuracy Score\"\n\n#### Source\n\n- Source of the dataset : https://drive.google.com/drive/folders/0By0bAi7hOBAFUHVXd1JCN3MwTEU\n- Source of research document : https://serv.cusp.nyu.edu/projects/urbansounddataset/salamon_urbansound_acmmm14.pdf\n\n\nNow let me look at a glance a sample sound excerpt from the dataset","execution_count":null},{"metadata":{"trusted":true,"_uuid":"f4bcec54397111dc01d2ad8996be458a5a5ece7c"},"cell_type":"code","source":"import IPython.display as ipd\nipd.Audio('../input/ultrasound-dataset/train/Train/2022.wav')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f29de04532240585cb5818c7e39b0b21d35e0934"},"cell_type":"markdown","source":"To load the audio files into the jupyter notebook ass a numpy array I have used 'librosa' library in python by using the pip command as follows\n\n ***pip install librosa***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install librosa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport glob\n%pylab inline\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom sklearn import metrics ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91f98dc661b1d87f9d6ba9c4aa7dff30c88e5303"},"cell_type":"markdown","source":"Now let us load a sample audio file using librosa","execution_count":null},{"metadata":{"trusted":true,"_uuid":"d8306846e5e9bf0e3d9ee4ac4094d1dd6469e1e9"},"cell_type":"code","source":"data,sampling_rate = librosa.load('../input/ultrasound-dataset/train/Train/2010.wav')\nplt.figure(figsize=(12,4))\nlibrosa.display.waveplot(data,sr=sampling_rate)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d44e64d3c5365cbd5462187c4c2a20c35059beb"},"cell_type":"markdown","source":"Now let us visually inspect data and see if we can find patterns in the data","execution_count":null},{"metadata":{"trusted":true,"_uuid":"55f24ebdec94e87f9351a312de8951f5a991c29a"},"cell_type":"code","source":"train = pd.read_csv('../input/ultrasound-dataset/train/train.csv')\ni = random.choice(train.index)\n\naudio_name = train.ID[i]\npath = os.path.join('../input/ultrasound-dataset/train/', 'Train', str(audio_name) + '.wav')\n\nprint('Class: ', train.Class[i])\nx, sr = librosa.load('../input/ultrasound-dataset/train/Train/' + str(train.ID[i]) + '.wav')\n\nplt.figure(figsize=(12, 4))\nlibrosa.display.waveplot(x, sr=sr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"225885357b4a7b5568ab9b28c2661e74e2936e2a"},"cell_type":"markdown","source":"As you can see the air conditioner class is shown as random class and we can see its pattern.Let us again see another class by using the same code to randomly select another class and observe its pattern","execution_count":null},{"metadata":{"trusted":true,"_uuid":"789325dcfde1a02bea2e451faffd7ac303b5c13f"},"cell_type":"code","source":"i = random.choice(train.index)\naudio_name = train.ID[i]\npath = os.path.join('../input/ultrasound-dataset/train/', 'Train', str(audio_name) + '.wav')\nprint('Class: ', train.Class[i])\nx, sr = librosa.load('../input/ultrasound-dataset/train/Train/' + str(train.ID[i]) + '.wav')\nplt.figure(figsize=(12, 4))\nlibrosa.display.waveplot(x, sr=sr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df1042ccc44a8ef8a0dd46593b9cc9abd99e51ba"},"cell_type":"markdown","source":"Let us see the class distributions for this problem","execution_count":null},{"metadata":{"trusted":true,"_uuid":"593203a5c675dcb7050a1cdfa5eb7ed9d0cceec0"},"cell_type":"code","source":"print(train.Class.value_counts(normalize=True)) #distribution of data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a5edc0e3d276f18cd7e8162657bc0cda878bb6a"},"cell_type":"markdown","source":"It appears that jackhammer has more count than any other classes\n\nNow let us see how we can leverage the concepts we learned above to solve the problem. We will follow these steps to solve the problem.\n\n- Step 1: Load audio files & Extract features\n- Step 2: Convert the data to pass it in our deep learning model\n- Step 3: Run a deep learning model and get results\n\n#### Step 1: Load audio files & Extract features\n\nLet us create a function to load audio files and extract features","execution_count":null},{"metadata":{"trusted":true,"_uuid":"6cd0d5eb913efd2dd5a9fdbdca876e721511cb58"},"cell_type":"code","source":"def parser(row):\n    file_name = os.path.join(os.path.abspath('../input/ultrasound-dataset/train/'),'Train',str(row.ID)+'.wav')\n    try:\n        # here kaiser_fast is a technique used for faster extraction\n        X,sample_rate = librosa.load(file_name,res_type='kaiser_fast')\n        # we extract mfcc feature from data\n        mfccs = np.mean(librosa.feature.mfcc(y=X,sr=sample_rate,n_mfcc=40).T,axis=0)\n    except Exception as e:\n        print('Error encountered while parsing the file:',file_name)\n        \n        return 'None', 'None'\n    \n    feature = mfccs\n    \n    label = row.Class\n    #print(file_name)\n    print(feature)\n    print(label)\n    return pd.Series([feature, label],index=['feature','label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"786b607c60748d35391d9d6692867b5d98c31740","_kg_hide-output":true},"cell_type":"code","source":"temp = train.apply(parser,axis =1)\ntemp.columns = ['feature', 'label']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"433d28c32afb1ae9c3e0c8b030ca883671e28fcc"},"cell_type":"markdown","source":"#### Step 2: Convert the data to pass it in our deep learning model\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"f70092010147fe50a5ccdc6e6aad7a3df32138b4"},"cell_type":"code","source":"X = np.array(temp.feature.tolist())\ny = np.array(temp.label.tolist())\n\nlabel_encoder = LabelEncoder()\nprint(temp.label.dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8da4d73ed70585adeb322bb47e5b0bcb53f9917b"},"cell_type":"code","source":"y = np_utils.to_categorical(label_encoder.fit_transform(y))   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## If you like this kernel greatly appreciate to upvote.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}