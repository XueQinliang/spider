{"cells":[{"metadata":{"_uuid":"31de920b8841625394bdabffdf80b436efc56b0d"},"cell_type":"markdown","source":"### load library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"from tensorflow import keras\n# from tensorflow.keras.utils import to_categorical\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n# from tensorflow.keras.losses import categorical_crossentropy\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b016437d3f8a6460a8763129d010e449f1aa607"},"cell_type":"markdown","source":"### Load dataset "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1f4399ac5ab8fb5b48b4d4d944f51bca6814816c"},"cell_type":"code","source":"train_data_path = '../input/emnist-balanced-train.csv'\ntest_data_path = '../input/emnist-balanced-test.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3bba436039a3c87639fee9fb0f7963933450871","collapsed":true},"cell_type":"code","source":"train_data = pd.read_csv(train_data_path, header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98ea8bd7477906d87fff464cf207cbb0d1a86c22","collapsed":true},"cell_type":"code","source":"train_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f2fb5113d990cab102b2e9f657baa3a8da129400"},"cell_type":"code","source":"# The classes of this balanced dataset are as follows. Index into it based on class label\nclass_mapping = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabdefghnqrt'\n# source data: https://arxiv.org/pdf/1702.05373.pdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"580fafe36c21d0f3d3911b2065f9f09d786db370","collapsed":true},"cell_type":"code","source":"class_mapping[34]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd26cc4af46e4a09316da300f8513f1a9f6fa260","collapsed":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d9acbcee4de0ea29f70ac51394b941d41def7b1"},"cell_type":"markdown","source":"## Data is flipped"},{"metadata":{"trusted":true,"_uuid":"08867c5c0faab423312281eea41015ad10648b66","collapsed":true},"cell_type":"code","source":"num_classes = len(train_data[0].unique())\nrow_num = 8\n\nplt.imshow(train_data.values[row_num, 1:].reshape([28, 28]), cmap='Greys_r')\nplt.show()\n\nimg_flip = np.transpose(train_data.values[row_num,1:].reshape(28, 28), axes=[1,0]) # img_size * img_size arrays\nplt.imshow(img_flip, cmap='Greys_r')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4a3d24bf1e6c5527f13a669704cafd4cc84ef4ca"},"cell_type":"code","source":"def show_img(data, row_num):\n    img_flip = np.transpose(data.values[row_num,1:].reshape(28, 28), axes=[1,0]) # img_size * img_size arrays\n    plt.title('Class: ' + str(data.values[row_num,0]) + ', Label: ' + str(class_mapping[data.values[row_num,0]]))\n    plt.imshow(img_flip, cmap='Greys_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7921f478d1d79dcc480f7987ce20340167b894c1","collapsed":true},"cell_type":"code","source":"show_img(train_data, 149)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5385a804fd7eda7ca00cb874be10dcc85e9af19e"},"cell_type":"code","source":"# 10 digits, 26 letters, and 11 capital letters that are different looking from their lowercase counterparts\nnum_classes = 47 \nimg_size = 28\n\ndef img_label_load(data_path, num_classes=None):\n    data = pd.read_csv(data_path, header=None)\n    data_rows = len(data)\n    if not num_classes:\n        num_classes = len(data[0].unique())\n    \n    # this assumes square imgs. Should be 28x28\n    img_size = int(np.sqrt(len(data.iloc[0][1:])))\n    \n    # Images need to be transposed. This line also does the reshaping needed.\n    imgs = np.transpose(data.values[:,1:].reshape(data_rows, img_size, img_size, 1), axes=[0,2,1,3]) # img_size * img_size arrays\n    \n    labels = keras.utils.to_categorical(data.values[:,0], num_classes) # one-hot encoding vectors\n    \n    return imgs/255., labels\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7f3744ebae007d1150e96f6c9d23896bf0cc3ce"},"cell_type":"markdown","source":"### model, compile"},{"metadata":{"trusted":true,"_uuid":"197e457ae115c85fe96db9621fd296978a62502c","collapsed":true},"cell_type":"code","source":"model = keras.models.Sequential()\n\n# model.add(keras.layers.Reshape((img_size,img_size,1), input_shape=(784,)))\nmodel.add(keras.layers.Conv2D(filters=12, kernel_size=(5,5), strides=2, activation='relu', \n                              input_shape=(img_size,img_size,1)))\n# model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(keras.layers.Dropout(.5))\n\nmodel.add(keras.layers.Conv2D(filters=18, kernel_size=(3,3) , strides=2, activation='relu'))\n# model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(keras.layers.Dropout(.5))\n\nmodel.add(keras.layers.Conv2D(filters=24, kernel_size=(2,2), activation='relu'))\n# model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n\n# model.add(keras.layers.Conv2D(filters=30, kernel_size=(3,3), activation='relu'))\n\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(units=150, activation='relu'))\nmodel.add(keras.layers.Dense(units=num_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35693aac7a868fce047dffa15a5c2c62a5943496","collapsed":true},"cell_type":"code","source":"for layer in model.layers:\n    print(layer.get_output_at(0).get_shape().as_list())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fbd7a6b3c2f3ed6fc9d4e96ddecc7106d31b410"},"cell_type":"markdown","source":"### Train"},{"metadata":{"trusted":true,"_uuid":"f77ca5ebdb6a2080248702f109bef0ca04ca4e9e","collapsed":true},"cell_type":"code","source":"X, y = img_label_load(train_data_path)\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a10897314dbbcef2233cab83b2cdb981533ccf3d","scrolled":false,"collapsed":true},"cell_type":"code","source":"data_generator = keras.preprocessing.image.ImageDataGenerator(validation_split=.2)\n## consider using this for more variety\ndata_generator_with_aug = keras.preprocessing.image.ImageDataGenerator(validation_split=.2,\n                                            width_shift_range=.2, height_shift_range=.2,\n                                            rotation_range=60, zoom_range=.2, shear_range=.3)\n\n# if already ran this above, no need to do it again\n# X, y = img_label_load(train_data_path)\n# print(\"X.shape: \", X.shape)\n\ntraining_data_generator = data_generator.flow(X, y, subset='training')\nvalidation_data_generator = data_generator.flow(X, y, subset='validation')\nhistory = model.fit_generator(training_data_generator, \n                              steps_per_epoch=500, epochs=5, # can change epochs to 10\n                              validation_data=validation_data_generator)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14fb22d26d96914ca1c8111dd8f056915db9cf19","collapsed":true},"cell_type":"code","source":"test_X, test_y = img_label_load(test_data_path)\ntest_data_generator = data_generator.flow(X, y)\n\nmodel.evaluate_generator(test_data_generator)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3aa759dd7aa31e37dc48bc9a65b8c815ce273019"},"cell_type":"markdown","source":"## Look at some predictions\n"},{"metadata":{"trusted":true,"_uuid":"b2267b783e2b08ef23ec5d7bc68c6b0e14b1d2fd"},"cell_type":"code","source":"test_data = pd.read_csv(test_data_path, header=None)\nshow_img(test_data, 123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"60f2c48a1923aa302d24e6a0ee019fc9aba950d1"},"cell_type":"code","source":"X_test, y_test = img_label_load(test_data_path) # loads images and orients for model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2a8b2f8b90f9ee6aa1cc169b58b637da49735ab7"},"cell_type":"code","source":"def run_prediction(idx):\n    result = np.argmax(model.predict(X_test[idx:idx+1]))\n    print('Prediction: ', result, ', Char: ', class_mapping[result])\n    print('Label: ', test_data.values[idx,0])\n    show_img(test_data, idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30c86d25f4147a803278fa9e5838449656f2095a"},"cell_type":"code","source":"import random\n\nfor _ in range(1,10):\n    idx = random.randint(0, 47-1)\n    run_prediction(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be1a2897df65088691d96c59331ff34420e22d45"},"cell_type":"code","source":"show_img(test_data, 123)\nnp.argmax(y_test[123])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0420fa5ad096f37736656c22f687f91e5dd0c38e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fee62a1450c90e70a7ed40b5e84f51560d9b60c"},"cell_type":"markdown","source":"## Export model to TF SavedModel for CMLE Prediction\nhttps://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator "},{"metadata":{"trusted":true,"_uuid":"ab45a0f245c342cdb9d360e95db8dacbd8d0e7bf","collapsed":true},"cell_type":"code","source":"# First, convert Keras Model to TensorFlow Estimator\nmodel_input_name = model.input_names[0]\nestimator_model = keras.estimator.model_to_estimator(keras_model=model, model_dir=\"./estimator_model\")\nprint(model_input_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c74172a358b5b969353b8d64288999ab7bd0a7b"},"cell_type":"code","source":"# Next, export the TensorFlow Estimator to SavedModel\n\nfrom functools import partial\nimport tensorflow as tf\n\ndef serving_input_receiver_fn():\n    input_ph = tf.placeholder(tf.string, shape=[None], name='image_binary')\n    images = tf.map_fn(partial(tf.image.decode_image, channels=1), input_ph, dtype=tf.uint8)\n    images = tf.cast(images, tf.float32) / 255.\n    images.set_shape([None, 28, 28, 1])\n\n    # the first key is the name of first layer of the (keras) model. \n    # The second key is the name of the key that will be passed in the prediction request\n    return tf.estimator.export.ServingInputReceiver({model_input_name: images}, {'bytes': input_ph})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1c5aa3adcb0186f38f8f1f79f134d62a39c471a","collapsed":true},"cell_type":"code","source":"export_path = estimator_model.export_savedmodel('./export', serving_input_receiver_fn=serving_input_receiver_fn)\nexport_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2695e4eda1240fb4c3b5eaa7b3b2f18ec3418324"},"cell_type":"markdown","source":"## Keras exports"},{"metadata":{"trusted":true,"_uuid":"3e78e9886eb9570bae22ec07a37b0d04131ab77b","collapsed":true},"cell_type":"code","source":"with open('model.json', 'w') as f:\n    f.write(model.to_json())\nmodel.save_weights('./model.h5')\n\nmodel.save('./full_model.h5')\n!ls -lh","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09bcb1baf98680d49505b3efe5c3dd9ac6c5892d"},"cell_type":"markdown","source":"## Plot loss and accuracy"},{"metadata":{"trusted":true,"_uuid":"3a7d93c115273e615b5f98d0df54eb0ce66b2a81","collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nprint(history.history.keys())\n\n# accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='lower right')\nplt.show()\n\n# loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f757aeaf7ee6fe53d399ebd441a5e932d2e866d5"},"cell_type":"markdown","source":"## Create some output files for sending to Cloud ML Engine's online prediction"},{"metadata":{"trusted":true,"_uuid":"69a18e30291bbf3ba37b066b49166fa4bc78bd17","collapsed":true},"cell_type":"code","source":"from PIL import Image\n\ndef export_png(row_num, data=test_data):\n    array = np.transpose(data.values[row_num,1:].reshape(28, 28), axes=[1,0])\n    img = Image.fromarray(array.astype(np.uint8))\n    filename = 'class_' + str(data.values[row_num,0]) + '_label_' + str(class_mapping[data.values[row_num,0]]) + '.png'\n    img.save(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2b87f9b93adb46198b1c6738546e84d93a1d29fc"},"cell_type":"code","source":"export_png(149)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e2d1fc97f431fb792ac22af08c642d0ef1112d86"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20286272a7137159a31ac24ee3999ed79ba82b8b","collapsed":true},"cell_type":"code","source":"import base64\nimport json\n\nimg_filename = 'class_19_label_J.png'\njson_filename = 'class_19_label_J.json'\n\nwith open(img_filename, 'rb') as img_file :\n    img_str = base64.b64encode(img_file.read())\n    print(str(img_str))\n\n    json_img = {\"image_bytes\":{\"b64\": str(img_str) }}\n    print(type(json_img['image_bytes']['b64']))\n\nwith open(json_filename, 'w') as outfile:\n    json.dump(json_img, outfile)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}