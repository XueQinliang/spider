{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b4b199f1-0806-820e-a569-e324f9c75445"},"source":"## Diabetes Classifications\n\nI have tested a series of algorithms except for Log Regression, for this algorithm please refer to [Mohamed L's Kernel](https://www.kaggle.com/momo062/d/uciml/pima-indians-diabetes-database/79-47-pima-indians-diabetes-log-reg-and-svc)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78a3956e-f633-aa99-53ec-6fe742674cd9"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\ndata = pd.read_csv('../input/diabetes.csv')\ndata.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"933a44c6-dd84-ee1f-efcd-9f2da99be3da"},"outputs":[],"source":"data.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ee26d44-dab8-24fa-79b9-d7d0922839a0"},"outputs":[],"source":"data.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"3dad9d1d-07d2-48fe-e5a4-022dfb83d33a"},"source":"# Visualizing Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"415da1b4-d02b-a94e-d289-87ff0b84d6c3"},"outputs":[],"source":"g = sns.PairGrid(data, vars=['Glucose', 'Insulin', 'BMI'], hue=\"Outcome\", size=2.4)\ng.map_diag(plt.hist)\ng.map_upper(plt.scatter)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.add_legend()\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6e0a940-8f40-ad7b-3945-1e6468dcc1b8"},"outputs":[],"source":"g = sns.PairGrid(data, vars=['Age', 'SkinThickness', 'BloodPressure'], hue=\"Outcome\", size=2.4)\ng.map_diag(plt.hist)\ng.map_upper(plt.scatter)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.add_legend()\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e10cb7f4-6231-3ed3-123c-7cb97a2cb670"},"outputs":[],"source":"g = sns.PairGrid(data, vars=['Pregnancies', 'DiabetesPedigreeFunction'], hue=\"Outcome\", size=3.5)\ng.map_diag(plt.hist)\ng.map_upper(plt.scatter)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.add_legend()\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61d7487f-b0e9-1a3e-3c10-32cdc4b450fa"},"outputs":[],"source":"columns = ['Glucose', 'Age', 'BloodPressure', 'Insulin','BMI','SkinThickness' ,'Pregnancies',  'DiabetesPedigreeFunction']\nn_cols = 2\nn_rows = 4\nidx = 0\n\nfor i in range(n_rows):\n    fg,ax = plt.subplots(nrows=1,ncols=n_cols,sharey=True,figsize=(8, 2.4))\n    for j in range(n_cols):\n        sns.violinplot(x = data.Outcome, y=data[columns[idx]], ax=ax[j]) \n        idx += 1\n        if idx >= 8:\n            break"},{"cell_type":"markdown","metadata":{"_cell_guid":"7790af18-672d-658b-9d0a-58648dd63b13"},"source":"# Preprocessing Data"},{"cell_type":"markdown","metadata":{"_cell_guid":"1644806e-0bf7-5d6b-3c20-751bbe32c9d2"},"source":"From the above data exploration, we saw an outlier of SkinThickness"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"048157d7-1cfc-13b7-feca-5c73b00a03ee"},"outputs":[],"source":"# remove the Outlier of skin thickness\n\nmax_skinthickness = data.SkinThickness.max()\ndata = data[data.SkinThickness!=max_skinthickness]"},{"cell_type":"markdown","metadata":{"_cell_guid":"54e71609-f0de-5bba-4934-80a62e7aeb9a"},"source":"Replace zero value of Glucose, BloodPressure, SkinThickness, Insulin, BMI with mean for each class\n\nCredit to Mohamed L for his kerner [here](https://www.kaggle.com/momo062/d/uciml/pima-indians-diabetes-database/79-47-pima-indians-diabetes-log-reg-and-svc) and  Atul A for his kerner [here](https://www.kaggle.com/atulnet/d/uciml/pima-indians-diabetes-database/pima-diabetes-keras-implementation)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5afee8c1-eb83-2903-a6a2-fff868ece647"},"outputs":[],"source":"# create a helper function\ndef replace_zero(df, field, target):\n    mean_by_target = df.loc[df[field] != 0, [field, target]].groupby(target).mean()\n    data.loc[(df[field] == 0)&(df[target] == 0), field] = mean_by_target.iloc[0][0]\n    data.loc[(df[field] == 0)&(df[target] == 1), field] = mean_by_target.iloc[1][0]\n\n    # run the function\nfor col in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']:   \n    replace_zero(data, col, 'Outcome')    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f921e69-7226-89f1-491b-12335a325eab"},"outputs":[],"source":"data.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c272e053-40f2-f69d-7b60-fa17f34e9a96"},"source":"We can see that one record was removed and zero values were successfully replaced"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"573001e7-75f5-ca99-343a-7f28a51fc477"},"outputs":[],"source":"# split data\n\nX = data.iloc[:,:-1]\ny = data.iloc[:, -1]\n\nfrom sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=100)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.size)\nprint(y_test.size)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2e084b42-6a13-0ad4-2291-7e732a8a881b"},"source":"# Testing Algorithms"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11d014c0-ad0b-4856-9277-eb2c53a8d112"},"outputs":[],"source":"# load algorithms\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors  import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import f1_score\n\n# helper functions\ndef train_clf(clf, X_train, y_train):\n    \n    return clf.fit(X_train, y_train)\n    \ndef pred_clf(clf, features, target):\n    \n    y_pred = clf.predict(features)\n    return f1_score(target.values, y_pred, pos_label = 1)\n\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n    \n    train_clf(clf, X_train, y_train)\n    \n    print(\"F1 score for training set is: {:.4f}\".format(pred_clf(clf, X_train, y_train)))\n    print(\"F1 score for testing set is: {:.4f}\\n\".format(pred_clf(clf, X_test, y_test)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0565d8ee-05c7-93f5-eae9-ac45fe86d88f"},"outputs":[],"source":"# load algorithms\nnb = GaussianNB()\nknn = KNeighborsClassifier()\ndtc = DecisionTreeClassifier(random_state=0)\nrfc = RandomForestClassifier(random_state=0)\nabc = AdaBoostClassifier(random_state=0)\ngbc = GradientBoostingClassifier(random_state=0)\n\nalgorithms = [nb,knn, dtc, rfc, abc, gbc]\n\nfor clf in algorithms:\n    \"\"\"\n    print(\"\\n{}: \\n\".format(clf.__class__.__name__))\n    \n    # create training data from first 100, then 200, then 300\n    #for n in [179, 358, 537]:\n        #train_predict(clf, X_train[:n], y_train[:n], X_test, y_test)\n    \"\"\"        \n    print(\"{}:\".format(clf))\n    train_predict(clf, X_train, y_train, X_test, y_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"777300f4-3a02-16e4-2254-eefe81d6b49f"},"source":"# Optimizing KNN model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0bce8930-82eb-fe7b-9133-40f45dbc5128"},"outputs":[],"source":"# split training set into training and testing set\nX_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_train, y_train, test_size = 0.3, random_state=100)\nfor n in range(3,10):    \n    knn = KNeighborsClassifier(n_neighbors=n)\n    print(\"Number of neighbors is: {}\".format(n))\n    train_predict(knn, X_train_cv, y_train_cv, X_test_cv, y_test_cv)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d28cdaa-9260-c792-f9da-0470c4f4bd2f"},"outputs":[],"source":"from sklearn.metrics import accuracy_score\n\nknn = KNeighborsClassifier(n_neighbors=8)\nclf_ = knn.fit(X_train, y_train)\ny_pred = clf_.predict(X_test)\nprint('Accuracy is {}'.format(accuracy_score(y_test,y_pred )))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7beadd4-22ca-3386-02c4-d1259b12fdbe"},"outputs":[],"source":"knn"},{"cell_type":"markdown","metadata":{"_cell_guid":"0c41cab7-645b-3758-66f6-06fb1083c361"},"source":"It is worth to note that KNN does not give stable accuracy"},{"cell_type":"markdown","metadata":{"_cell_guid":"3e59a7ef-32fd-2df7-115e-838d7729f45e"},"source":"# Optimizing Essemble Algorithms"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59139696-1eb6-cfcc-c1b0-98b79fc7b0d5"},"outputs":[],"source":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import KFold\nparams = {'n_estimators':1200, 'max_depth':9, 'subsample':0.5, 'learning_rate':0.01, 'min_samples_leaf':1, 'random_state':0}\ngbc = GradientBoostingClassifier(**params)\n\nn_estimators = 10\nclf = gbc\n\n# split training set into training and testing set\nX_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_train, y_train, test_size = 0.3, random_state=100)\n\nX_train_cv = X_train_cv.reset_index(drop=True, inplace=False)\ny_train_cv = y_train_cv.reset_index(drop=True, inplace=False)\n\n\nclf.fit(X_train_cv,y_train_cv)\n#score = f1_score(y_train, clf.predict(X_train), pos_label = 1)\nacc = clf.score(X_test_cv, y_test_cv)\n    \nn_estimators = params['n_estimators']\nx = np.arange(n_estimators) + 1\n\n\"\"\" The following part code was stole from sklearn \"\"\"\ndef heldout_score(clf, X_test_cv, y_test_cv):\n    \"\"\"compute deviance scores on ``X_test`` and ``y_test``. \"\"\"\n    score = np.zeros((n_estimators,), dtype=np.float64)\n    for i, y_pred in enumerate(clf.staged_decision_function(X_test_cv)):\n        score[i] = clf.loss_(y_test_cv, y_pred)\n    return score\n\n\ndef cv_estimate(n_splits=10):\n    cv = KFold(n_splits=n_splits)\n    cv_clf = clf\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n    for train, test in cv.split(X_train_cv):\n        cv_clf.fit(X_train_cv.iloc[train], y_train_cv[train])\n        val_scores += heldout_score(cv_clf, X_train_cv.iloc[test], y_train_cv[test])\n    val_scores /= n_splits\n    return val_scores\n\n\n# Estimate best n_estimator using cross-validation\ncv_score = cv_estimate(3)\n\n# Compute best n_estimator for test data\ntest_score = heldout_score(clf, X_test_cv, y_test_cv)\n\n# negative cumulative sum of oob improvements\ncumsum = -np.cumsum(clf.oob_improvement_)\n\n# min loss according to OOB\noob_best_iter = x[np.argmin(cumsum)]\n\n# min loss according to test (normalize such that first loss is 0)\ntest_score -= test_score[0]\ntest_best_iter = x[np.argmin(test_score)]\n\n# min loss according to cv (normalize such that first loss is 0)\ncv_score -= cv_score[0]\ncv_best_iter = x[np.argmin(cv_score)]\n    \n# color brew for the three curves\noob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))\ntest_color = list(map(lambda x: x / 256.0, (127, 201, 127)))\ncv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))\n\n# plot curves and vertical lines for best iterations\nplt.plot(x, cumsum, label='OOB loss', color=oob_color)\nplt.plot(x, test_score, label='Test loss', color=test_color)\nplt.plot(x, cv_score, label='CV loss', color=cv_color)\nplt.axvline(x=oob_best_iter, color=oob_color)\nplt.axvline(x=test_best_iter, color=test_color)\nplt.axvline(x=cv_best_iter, color=cv_color)\n\n# add three vertical lines to xticks\nxticks = plt.xticks()\nxticks_pos = np.array(xticks[0].tolist() +\n                        [oob_best_iter, cv_best_iter, test_best_iter])\nxticks_label = np.array(list(map(lambda t: int(t), xticks[0])) +\n                        ['OOB', 'CV', 'Test'])\nind = np.argsort(xticks_pos)\nxticks_pos = xticks_pos[ind]\nxticks_label = xticks_label[ind]\nplt.xticks(xticks_pos, xticks_label)\n\nplt.legend(loc='upper right')\nplt.ylabel('normalized loss')\nplt.xlabel('number of iterations')\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"eccffc7a-d3f2-3506-1a2e-dd4ed2e77bc7"},"source":"From above cross validation, we can see the best n_estimator is around 290."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"70adf94b-aedc-ac52-996e-604f70a199fa"},"outputs":[],"source":"from sklearn.metrics import accuracy_score\n\nparams = {'max_depth':9, 'subsample':0.5, 'learning_rate':0.01, 'min_samples_leaf':1, 'random_state':0}\ngbc = GradientBoostingClassifier(n_estimators=290, **params)\nclf_ = gbc.fit(X_train, y_train)\ny_pred = clf_.predict(X_test)\nprint('Accuracy is {}'.format(accuracy_score(y_test,y_pred )))\ntrain_predict(gbc, X_train, y_train, X_test, y_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"eed7b6da-41bf-9648-8e8b-3c6c4cb5a01d"},"source":"## Show the features' importance"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"143de00d-072d-67e0-de8b-dd4d6a6e6c4d"},"outputs":[],"source":"gbc.feature_importances_"},{"cell_type":"markdown","metadata":{"_cell_guid":"a3a98db6-6a03-7c38-400f-5a5eec38e59c"},"source":"## The best accuracy is 0.8658. \n\n# More coming ..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36bd249c-6961-e330-9386-21b57547c618"},"outputs":[],"source":"import xgboost as xgb"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}