{"cells":[{"metadata":{"_uuid":"000533f61265c736b3457a4363894d429a762742"},"cell_type":"markdown","source":"<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRPuCl72q5-misMtB7Lvit9uodiEhjGtfZmm3Sb5y7wC26X2XhP\" width=\"450px\" height=\"450px\"/> "},{"metadata":{"_uuid":"296c8144ace369bd3000851d65ea5f8c3005a434"},"cell_type":"markdown","source":"<img src=\"https://pbs.twimg.com/profile_images/679147586746322946/RQ78ao4T_400x400.png\" width=\"50px\" height=\"50px\"/>"},{"metadata":{"_uuid":"f1b2fab7fbea09024348d4f722f13357c55f5015"},"cell_type":"markdown","source":"<img src=\"https://qph.fs.quoracdn.net/main-qimg-7c9b7670c90b286160a88cb599d1b733\" width=\"450px\" height=\"450px\"/>"},{"metadata":{"_uuid":"7c6eb1604e91e83763bb10a73cd413e658f2b386"},"cell_type":"markdown","source":"# K-Means Clustering vs. Logistic Regression"},{"metadata":{"_uuid":"16ec2001ee39ca4d9e20acb3a2a2f63732c52364"},"cell_type":"markdown","source":"## Contents\n1. [Introduction:](#1)\n1. [Imports:](#2)\n1. [Read the Data:](#3)\n1. [Exploration:](#4)\n1. [Encoding:](#5)\n1. [Cluster Analysis:](#6)\n1. [Classification:](#7)\n1. [Model Evaluation:](#8)\n1. [Verdict](#9)\n1. [Closing Remarks:](#10)"},{"metadata":{"_uuid":"032e0b996c245aaf1b9f02ea02088cfabd4f6558"},"cell_type":"markdown","source":"<a id=\"1\"> \n## 1. Introduction:"},{"metadata":{"_uuid":"e77f4e6fbdf1b75a2f905598c5e47b3b7effa4bf"},"cell_type":"markdown","source":"In this notebook, we will be comparing two very different machine learning models on the [Mushroom Classification Dataset](https://www.kaggle.com/uciml/mushroom-classification) for the task of predicting whether a given mushroom is **<font color=\"purple\">poisonous</font>** or **<font color=\"green\">edible</font>**. \n\nThe first model will be [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) without any parameter tuning, and the second model will be [K-Means Clustering](https://en.wikipedia.org/wiki/K-means_clustering)."},{"metadata":{"_uuid":"8541042ce7b8e2c777360a743d4f6a255474cf43"},"cell_type":"markdown","source":"But how exactly does one go about comparing a [Supervised Learning](https://en.wikipedia.org/wiki/Supervised_learning) Algorithm to an [Unsupervised Learning](https://en.wikipedia.org/wiki/Unsupervised_learning) Algorithm (especially when it comes to the task of [binary classification](https://en.wikipedia.org/wiki/Binary_classification))? \n\nAs usual, for the Supervised Learning Algorithm (Logistic Regression), we will simply train the model on 80% of the mushroom data, and then test it's performance on the remaining 20%.\n\nAnd as for the Unsupervised Learning Algorithm (K-Means Clustering): I've found that if we cluster the data (with it's labels removed) into two different clusters, then one cluster ends up holding most of the **<font color=\"purple\">poisonous</font>** mushrooms, while the other cluster ends up holding most of the **<font color=\"green\">edible</font>** mushrooms. So, to build our binary classifier, we will cluster that same 80% of mushroom data mentioned above into two clusters, and then classify the remaining 20% of mushrooms as **<font color=\"purple\">poisonous</font>** or **<font color=\"green\">edible</font>** depending on which cluster they belong to.\n\nAnd at the end, we will compare the performances of the two algorithms on that test data to see who comes out on top!"},{"metadata":{"_uuid":"003d8cc581a3ecc4d28d1b91e316721ee5880de5"},"cell_type":"markdown","source":"Which model do you think will be the winner?"},{"metadata":{"_uuid":"5d5043bd750279be082a13abb9e48a9da43d0e99"},"cell_type":"markdown","source":"<a id=\"2\">\n## 2. Imports:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"82bc5499299b1be95f83de33bfb23fd4f5736c33"},"cell_type":"markdown","source":"<a id=\"3\">\n## 3. Read the data:"},{"metadata":{"trusted":true,"_uuid":"cc132dfc2d972153e1fff90abf759b5b0b79817d","collapsed":true},"cell_type":"code","source":"orig = pd.read_csv('../input/mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"654de4dd234bfd0b6ed2d224ad92dac0d9f6ff9d"},"cell_type":"code","source":"orig.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"af127b240a01f8714ba62070d20158f8f099f2ed"},"cell_type":"markdown","source":"Divide our data into '*predictors*,' `X` and '*labels*,' `y`:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1b63de8a06a8267f3938c78ac78bc387c1842a11"},"cell_type":"code","source":"#The 'class' column contains our labels.\n#It tells us whether the mushroom is 'poisonous' or 'edible'.\nX = orig.drop(['class'], axis=1)\ny = orig['class']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c61471daf219ad6353d4dde68c4a6383fc144e91"},"cell_type":"markdown","source":"Now before we encode each of our categorical variables in `X` & `y` with numbers so that the our learning algorithms can work with them, we will first do a bit of exploration."},{"metadata":{"_uuid":"ab459aed2d524eef6002f1934f7ae574c36d420c"},"cell_type":"markdown","source":"<a id=\"4\">\n## 4. Exploration:"},{"metadata":{"_uuid":"1a339caa679ed1bed8cf0e3aad58582a31f2500a"},"cell_type":"markdown","source":"Let's take a look at the values contained within each of `X`'s attributes, so we can get a better picture of the data we're working with:"},{"metadata":{"trusted":true,"_uuid":"bb7c8c6e1ecccea2b236674b91c3c02918775d48","scrolled":false},"cell_type":"code","source":"for attr in X.columns:\n    print('\\n*', attr, '*')\n    print(X[attr].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e82f98a2c67e7b65b6bb519c63298908adccb29"},"cell_type":"markdown","source":"Two things to note here: \n\nFirst, the `veil-type` variable has only one value, '**p**', meaning, every mushroom has the same `veil-type`. And because, every mushrrom has that same `veil-type`: that column doesn't tell us anything useful - so we can drop that column."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"de8b5fd45e6f2157c83b272a875515a56e425436"},"cell_type":"code","source":"X.drop(['veil-type'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14b0b9227582688f5165bcd822b60dd75de15232"},"cell_type":"markdown","source":"Second, the `stalk-root` variable has a '**?**' value for it's missing values. Rather than impute this missing value, I will divide the dataset into two sections: **(1)** *where `X['stalk-root']==?`* and **(2)** *where `X['stalk-root']!=?`*. Then, I will analyze the distribution of each variable within those two data sets to determine if they are similar.\n\nI'm no mushroom expert, so I would expect that if the distributions vary greatly for each variable, then the fact that the `stalk-root`'s are missing for some of the mushrooms -- *and not missing for the others* --may turn out to be useful/relevant information."},{"metadata":{"trusted":true,"_uuid":"3cfa42fbad9f32ff55f4d9b7380250bd1c04bc22"},"cell_type":"code","source":"for attr in X.columns:\n    #Format subplots\n    fig, ax = plt.subplots(1,2)\n    plt.subplots_adjust(right=2)\n    \n    #Construct values to count in each column\n    a=set(X[X['stalk-root']=='?'][attr])\n    b=set(X[X['stalk-root']!='?'][attr])\n    c = a.union(b)\n    c = np.sort(np.array(list(c)))\n    \n    #Build each subplot\n    sns.countplot(x=X[X['stalk-root']=='?'][attr], order=c, ax=ax[0]).set_title('stalk-root == ?')\n    sns.countplot(x=X[X['stalk-root']!='?'][attr], order=c, ax=ax[1]).set_title('stalk-root != ?')\n    \n    #Plot the plots\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"110e61d63a34ef31740d5949a417e1a3945f4035"},"cell_type":"markdown","source":"Since many of the distributions vary greatly, and because"},{"metadata":{"trusted":true,"_uuid":"66b29e8c646bd62ea1051c21ec21d94ffe82f814"},"cell_type":"code","source":"print( (len(X[X['stalk-root']=='?']) / len(X))*100, '%', sep='') ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0231bf2793d7f600435b68a044741d634a839a63"},"cell_type":"markdown","source":"of the mushrooms have the value **'?'** for their `stalk-root`, we will not impute the **'?'** values, rather, we will encode them just as we would the rest of the values in that column."},{"metadata":{"_uuid":"8e995a363cac4c20164ab77c0d2b9420670a4b3f"},"cell_type":"markdown","source":"<a id=\"5\">\n## 5. Encoding:"},{"metadata":{"_uuid":"5dfbe0c10a4bdc0591fe75ddcb10b53f1b41fdcd"},"cell_type":"markdown","source":"We'll use a simple binary encoding for variables that hold only 2 possible values, and a [one-hot-encoding](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding) for variables that hold 3 or more possible values."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ad6aff68f62f41b9bc888b1472bb5bcfe8d27b3e"},"cell_type":"code","source":"#For columns with only two values\nfor col in X.columns:\n    if len(X[col].value_counts()) == 2:\n        le = LabelEncoder()\n        X[col] = le.fit_transform(X[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5752df8494fe5ba62e40c9136e9e6b1e18f812df"},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"760fe23d2ce8885329a5fb9fda0df72d1ef906e6"},"cell_type":"markdown","source":"And now we 'one-hot-encode' the rest of the variables:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3ce9c7abf5318872463435504dbd36f03dc8fedc"},"cell_type":"code","source":"X = pd.get_dummies(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a21c15e7ebd066ae41c7b6c36b21094d47c459f9"},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccc6c7d59c40abdab4c22a4bf56a84e3ed842e2e"},"cell_type":"markdown","source":"<a id=\"6\">\n## 6. Cluster Analysis:"},{"metadata":{"_uuid":"4687e494bd2e23802656a958bf34f8d5a5e44685"},"cell_type":"markdown","source":"Now before we get into building our models and testing them against eachother, I just wanted to show you, visually, the result that arises from clustering the mushroom data set."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"54218489a29019bbb73bbe8d061c8198403ffa09"},"cell_type":"code","source":"#Initialize the model\nkmeans = KMeans(n_clusters=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e6c2e2cb1d45461520053314349cfc32a24f856"},"cell_type":"code","source":"#Fit our model on the X dataset\nkmeans.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"05b08a5fd8c7e5a10a8d59af2b9cbe905e4b4c4b"},"cell_type":"code","source":"#Calculate which mushrooms fall into which clusters\nclusters = kmeans.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"158c1364c127a7e11dd1144973c330fcd50c0cf9"},"cell_type":"code","source":"#'cluster_df' will be used as a DataFrame\n#to assist in the visualization\ncluster_df = pd.DataFrame()\n\ncluster_df['cluster'] = clusters\ncluster_df['class'] = y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"950731cd52059e5b997bf687539b71e74dc883cf"},"cell_type":"markdown","source":"Now let's visualize the distribution of **<font color=\"purple\">poisonous</font>** vs. **<font color=\"green\">edible</font>** mushrooms in each cluster."},{"metadata":{"trusted":true,"_uuid":"4394fdf95083745b8cc49bb7238c9592786fcbd4"},"cell_type":"code","source":"sns.factorplot(col='cluster', y=None, x='class', data=cluster_df, kind='count', order=['p','e'], palette=([\"#7d069b\",\"#069b15\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e5d6bf1f1e706c4b0b71b8633c3ed429c166b4e2"},"cell_type":"markdown","source":"Pretty interesting, eh? One cluster mostly contains **<font color=\"green\">edible</font>** mushrooms and the other cluster mostly contains **<font color=\"purple\">poisonous</font>** mushrooms.\n\nSo if we were given a mushroom, and we'd like to predict whether it is **<font color=\"green\">edible</font>** or **<font color=\"purple\">poisonous</font>**, we could first figure out which cluster it belongs to and then make our prediction based off of the percentage of **<font color=\"purple\">poisonous</font>** vs. **<font color=\"green\">edible</font>** mushrooms in that cluster."},{"metadata":{"_uuid":"440b3d411fa9b5a5f76ad653ba752fa30102ce2e"},"cell_type":"markdown","source":"Okay! Now that you have seen how clustering can be used as a classifier for this problem, it's time to get into our testing."},{"metadata":{"_uuid":"47c1a8df44ff22c756946f629eaba96857611052"},"cell_type":"markdown","source":"<a id=\"7\">\n## 7. Classification:"},{"metadata":{"_uuid":"d8d0e15ef00acf140a4b4821c300bbdfdd918db7"},"cell_type":"markdown","source":"But first, we need to encode our `y`-labels numerically so that our model can work with it.\n\nSince each mushroom is either **<font color=\"purple\">poisonous</font>** or **<font color=\"green\">edible</font>**, we will use another simple binary encoding like before:"},{"metadata":{"trusted":true,"_uuid":"ba523d485e11171f502eb7a65b937eef1332da58","collapsed":true},"cell_type":"code","source":"le = LabelEncoder()\ny = le.fit_transform(y)\n\ny","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53fc72bbf04e01fd598599b38cf8360f0cdea69a"},"cell_type":"markdown","source":"So, when `y==1`, the mushroom is **<font color=\"purple\">poisonous</font>**, and when `y==0`, the mushroom is **<font color=\"green\">edible</font>**."},{"metadata":{"_uuid":"c1bd3063cfdb164009ef3c873e26e083ac199231"},"cell_type":"markdown","source":"### Generate 'training' and 'test' sets:"},{"metadata":{"_uuid":"54880016fa34b2b4f7f477201d0dece039bc24f4"},"cell_type":"markdown","source":"We will use sklearn's [`train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to generate our 'training' and 'test' sets"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"300aa988575de8f1044e9bda1cdb82cfef6f9c68"},"cell_type":"code","source":"#Our training set will hold 80% of the data\n#and the test set will hold 20% of the data\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a17f32f01c81bc3f6abc279e993e8811c74cfa6"},"cell_type":"markdown","source":"### Initialize our models:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"78ff79b40415b81276e72e37c987a48ed7ab98ec"},"cell_type":"code","source":"#K-Means Clustering with two clusters\nkmeans = KMeans(n_clusters=2)\n\n#Logistic Regression with no special parameters\nlogreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a9752b992dd29568325a361e257222ca755d4fc"},"cell_type":"markdown","source":"### Fit our models:"},{"metadata":{"trusted":true,"_uuid":"fa7479cd95b0cf62f8265838329d9292f4ed3bb1","collapsed":true},"cell_type":"code","source":"kmeans.fit(train_X)#Note that kmeans is unlabeled...\n\nlogreg.fit(train_X, train_y)#... while logreg IS labeled","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a05990172074744b832139d48a20dcad5a466b3e"},"cell_type":"markdown","source":"### Make our predictions on the 'test' set:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4d5a75ddcc4f51e5aa2206d1392e0f2536d3fa1d"},"cell_type":"code","source":"kmeans_pred = kmeans.predict(test_X)\n\nlogreg_pred = logreg.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb1836fe14cc4ff7b121b2555fd9f97ea9168098"},"cell_type":"markdown","source":"### A little thing about clustering:"},{"metadata":{"_uuid":"52bb0090a5f11a61cb422338e3509b6d35110315"},"cell_type":"markdown","source":"One interesting aspect of K-Means clustering is that it does not always give the same results.\n\nFor example, if we were to run `kmeans.fit(train_X)` multiple times: part of the times, the majority of the **<font color=\"purple\">poisonous</font>** mushrooms will fall into *cluster 0*, and the majority of the **<font color=\"green\">edible</font>** mushrooms will fall into *cluster 1* - and on the other times: vice versa! In fact, if you'd like to observe this phenomenon yourself: fork this kernel and run the code blocks under the [Cluster Analysis](#6) section a few times.\n\nIn order to get around this problem, we will build a second set of predictions from our K-Means model - `kmeans_pred_2`. This second set of predictions will simply be the [bit-wise complement](https://en.wikipedia.org/wiki/Bitwise_operation#NOT) of `kmeans_pred`, and we will use whichever set of predictions gives us a better score as our final prediction set for the K-Means model!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"59c497cec4eb42ba216b34e3bd3a71734b999d85"},"cell_type":"code","source":"kmeans_pred_2 = []\nfor x in kmeans_pred:\n    if x == 1:\n        kmeans_pred_2.append(0)\n    elif x == 0:\n        kmeans_pred_2.append(1)\n        \nkmeans_pred_2 = np.array(kmeans_pred_2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42144097797432f260a61dd8a1286dbcbacac99d"},"cell_type":"markdown","source":"Now, we'll figure out which set of predictions for K-Means we'd like to use.\n\nWe'll use scikit-learn's [`accuracy_score()`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) function to help us decide:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ce8ca73b448df540d201d2bf30881685502f1a7e"},"cell_type":"code","source":"if accuracy_score(kmeans_pred, test_y, normalize=False) < accuracy_score(kmeans_pred_2, test_y, normalize=False):\n    kmeans_pred = kmeans_pred_2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"461b4d080ce0d6cd8e2f60a7c6ebed00b3d348aa"},"cell_type":"markdown","source":"<a id=\"8\">\n## 8. Model Evaluation:"},{"metadata":{"_uuid":"af904d33cfe5f58796ad088bc166566098106fbe"},"cell_type":"markdown","source":"Now for the payoff: let's see which model performed better on the test-set!"},{"metadata":{"_uuid":"c33b7e607bcbc1ec183892351d3b64535924e208"},"cell_type":"markdown","source":"We will determine which model did better by visualizing the amount of predictions that were made correctly vs.  made incorrectly by each model.\nAnd to help us visualize the results, we will build a pandas DataFrame, that we can use to help us make the plot with seaborn."},{"metadata":{"trusted":true,"_uuid":"ce1f55362c98f84b57cc18afaa6f3a3eb24b7b6f","collapsed":true},"cell_type":"code","source":"#This DataFrame will allow us to visualize our results.\nresult_df = pd.DataFrame()\n\n#The column containing the correct class for each mushroom in the test set, 'test_y'.\nresult_df['test_y'] = np.array(test_y) #(don't wanna make that mistake again!)\n\n#The predictions made by K-Means on the test set, 'test_X'.\nresult_df['kmeans_pred'] = kmeans_pred\n#The column below will tell us whether each prediction made by our K-Means model was correct.\nresult_df['kmeans_correct'] = result_df['kmeans_pred'] == result_df['test_y']\n\n#The predictions made by Logistic Regression on the test set, 'test_X'.\nresult_df['logreg_pred'] = logreg_pred\n#The column below will tell us whether each prediction made by our Logistic Regression model was correct.\nresult_df['logreg_correct'] = result_df['logreg_pred'] == result_df['test_y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a29b162bc5a50ecfa41e791b886828de416e9b96","collapsed":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2)\nplt.subplots_adjust(right=2)\nsns.countplot(x=result_df['kmeans_correct'], order=[True,False], ax=ax[0]).set_title('K-Means Clustering')\nsns.countplot(x=result_df['logreg_correct'], order=[True,False], ax=ax[1]).set_title('Logistic Regression')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ed7627edb3663b03b1b27c64f704113c94f8ddb"},"cell_type":"markdown","source":"<a id=\"9\">\n## 9. Verdict:"},{"metadata":{"_uuid":"22398d3eea53ead8081ee26cf778ab0f565f64c6"},"cell_type":"markdown","source":"Judging from the plots above, I'd say that **Logistic Regression** is the clear winner!\n\nBut K-Means Clustering didn't do too bad either; especially when you consider the fact that it's not built for the task of supervised learning, like Logistic Regression is!"},{"metadata":{"_uuid":"5275ec84cc7add4e0b5ef176c22dba8183fc50c3"},"cell_type":"markdown","source":"<a id=\"10\">\n## 10. Closing Remarks:"},{"metadata":{"_uuid":"abfb2be046f1e51dbe29ef91752d121a2f31ed3d"},"cell_type":"markdown","source":"I wasn't sure how this exercsie would turn out at first, but I'm glad to have gone through with it, because it was a ton of fun and I learned a bunch in the process!\n\nIf you've got any feedback for me: please leave a comment below, as I'd love to hear what you've got to say.\nAnd if you found this kernel to be interesting or useful to you, please consider giving it an upvote - I'd appreciate it very much :)\n\nAlso, special thanks to [Konstantin](https://www.kaggle.com/konstantinmasich) for helping me resolve a very difficult problem I had during the development of this kernel."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cc267f7bb36193d93cff4d819e25d0c402ac7e8d"},"cell_type":"markdown","source":"Cheers!\n-*Josh*"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}