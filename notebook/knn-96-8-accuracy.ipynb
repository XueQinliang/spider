{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b568e511-6f29-be11-7cfb-ba041f63cf4e"},"source":"# KNN \nThis problem set have data that is ready to feed to machine learning algorithm.There is no need of data cleaning feature extractions etc etc.This is more of the practice for algortith implementation.\n\nI am going to use k nearest neighbour with differnet values of K.This is my first use of KNN."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba31ea0f-bb14-a572-e642-9a5664a4cfec"},"outputs":[],"source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd8898c0-616f-2145-d7b6-6d0126e870b2"},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc86f964-4443-5a5c-95a3-ae8b97daab26"},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c6efc87-1383-aa41-76ed-91ef9479b744"},"outputs":[],"source":"labeled_group = train.groupby('label')\n\ntotal_observation = len(train['label'])\nfor label in range(0,10):\n    print('Relative frequency of {} = {:.3f} %'.format(label,labeled_group['label'].get_group(label).count()/total_observation*100))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e04d8bb7-b47d-eb7f-ed52-31c862d9a7a0"},"outputs":[],"source":"train.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"d7ace35b-4a5c-0b13-9aeb-1a8c8027301e"},"source":"# Splitting traing data\nI would like to split the traing data in two parts.One for traing and another for testing.Since testing can't be done on test data because it is not lablelled.\n\nSince our data is divided in groups we should perform **stratified sampling**,because it takes data sample from each group.\n\nIn case of simple random sampling it is possible that our traing set may have data points from one group in large number and some other group in very small amount.In this case sample population will not be the true representation of total popuplation.And our model can be biased."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5cab298-0901-3187-7e9e-4359a10691e6"},"outputs":[],"source":"from sklearn.cross_validation import StratifiedShuffleSplit\n\nX = train.drop(['label'], axis = 1)\ny = train['label']\n\nsss = StratifiedShuffleSplit(y, n_iter=3, test_size=0.2)\n\nfor train_index, test_index in sss:\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5476cf6-900b-8c11-fe83-a867d641dd27"},"outputs":[],"source":"print(X_train.shape)\nprint(X_test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c90f7d60-d1ce-2474-c7f1-31a4c3245bc3"},"outputs":[],"source":"print(y_train.shape)\nprint(y_test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee6243d2-b978-44d7-a342-23f60b4bfecb"},"outputs":[],"source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n#k_range = range(1,26)\n#scores = []\n\n# We are going to use differnt values of K as choose the best one as per accuracy_score\n#for k in k_range:\n    #knn = KNeighborsClassifier(n_neighbors=k)\n    #knn.fit(X_train,y_train)\n    #y_pred = knn.predict(X_test)\n    #scores.append(accuracy_score(y_test,y_pred))\n    #print('k {} completed'.format(k))\n    \n\n# Plotting testing accuracy\n#plt.plot(k_range,scores)\n#plt.xlabel('Value of K')\n#plt.ylabel('Testing accuracy')"},{"cell_type":"markdown","metadata":{"_cell_guid":"95887ca0-8549-ef87-00ac-311a8b964321"},"source":"Accuracy is maximum for k=5 and after that accuracy decreases for increase in k.Although above step is good for getting best k but it is computationally expensive .I ran this on my computer so it is commented out.\n\n\nI think there must be a better way to find best K which i don't know yet."},{"cell_type":"markdown","metadata":{"_cell_guid":"683a15bc-3661-51d7-ae1c-d8e14176116a"},"source":"# Traing the model on whole traing data\nWe don't want to waste the valuable training data,so we retrain our model on the entire training data with K = 5"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1fbc52d5-3d07-cf9e-1faa-e48cd1aaecb6"},"outputs":[],"source":"test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ae2f3d5-b7cc-e512-aac7-2c7c71c655ea"},"outputs":[],"source":"print(X.shape)\nprint(y.shape)\nprint(test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e56f812-d54e-fdf2-2222-6d4d27e56860"},"outputs":[],"source":"# Is takes more than 1200 seconds to run so kaggle kernel automatically killed.\n# More efficient way is to process the data in samll batches.\n\n# Model training on entire train data\n# final prediction for test data\n#knn = KNeighborsClassifier(n_neighbors=5)\n#knn.fit(X,y)\n#y_pred = knn.predict(test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82648ecc-02ed-6f72-86b5-686aa4de0802"},"outputs":[],"source":"# save submission to csv\n#pd.DataFrame({\"ImageId\": list(range(1,len(test)+1)),\"Label\": y_pred}).to_csv('Digit_recogniser.csv', index=False,header=True)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}