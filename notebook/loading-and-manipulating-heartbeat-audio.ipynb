{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ff1f01e5-fc8b-80be-b396-7b3f176ffa3c"},"source":"This notebook will show you how to load and manipulate audio data in Python. \n\nThe Heartbeat Sounds dataset is primarily audio-based: all of the heartbeat sounds are stored as WAV files that record either normal or abnormal heartbeats. So let's learn how to load and play with WAVs in Python."},{"cell_type":"markdown","metadata":{"_cell_guid":"4c346003-7a08-35ba-4051-def3baa10229"},"source":"In general, uncompressed audio is stored as a sequence of numbers that indicate the amplitude of the recorded sound pressure at each time point. In the WAV standard, these numbers are packed into a bytestring. The interpretation of this byestring depends primarily on two factors: first, the sampling rate, usually given in Hertz, which indicates how many number samples comprise a second's worth of data; and second, the bit depth (or sample width), which indicates how many bits comprise a single number.\n\nThese parameters, along with other parameters like the number of channels (e.g., is the audio mono or stereo) are stored in the header of the WAV file.\n\nThe `wave` library handles the parsing of WAV file headers, which include the parameters mentioned above. Let's load the `wave` library and use it to open a sound file."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ced2e8b-b399-bb31-5199-287d00557a84"},"outputs":[],"source":"import wave\n\nFNAME = '../input/set_a/normal__201101070538.wav'\n\nf = wave.open(FNAME)\n\n# frames will hold the bytestring representing all the audio frames\nframes = f.readframes(-1)\nprint(frames[:20])"},{"cell_type":"markdown","metadata":{"_cell_guid":"b00a7683-c66e-b11a-8539-aa71153ce508"},"source":"So `frames` now holds the entire bytestring representing all the audio samples in the sound file. We need to unpack this bytestring into an array of numbers that we can actually work with.\n\nThe first question is: how many bytes represent a single observation? In my experience in voice recording, 16-bit and 24-bit are the most common sample widths, but you can find a whole collection [on Wikipedia](https://en.wikipedia.org/wiki/Audio_bit_depth).\n\nPowers of 2 tend to be the easiest to work with, and luckily for us the heartbeat audio seems to be 16-bit. We can check this by using the getsamplewidth() method on the wave file:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17641b31-16d4-5144-eb61-0a861b748132"},"outputs":[],"source":"print(f.getsampwidth())"},{"cell_type":"markdown","metadata":{"_cell_guid":"80474802-4ac0-8892-a869-84da64baf109"},"source":"The result of getsamplewidth() is in bytes, so multiply it by 8 to get the bit depth. Since the result from the call is 2, that means we're looking at a 16-bit file.\n\nWe'll unpack the bytestring by using the `struct` library in Python. `struct` requires a format string based on C format characters, which you can take a look at [on the documentation page for Python's struct library](https://docs.python.org/2/library/struct.html).\n\nWe're in luck with the 16-bit depth, since the `struct` library prefers powers of 2. 16 bits corresponds to 2 bytes, so we'll use the signed format that corresponds to 2 bytes; according to [the C format characters](https://docs.python.org/2/library/struct.html#format-characters), we should use the format character 'h'.\n\nA slight trick in the `struct` library is that it wants its format string to exactly match the expected size, so we have to multiply the format character 'h' by the number of frames in the bytestring:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9cb4cc8a-8352-4e38-101c-413dc17fde7c"},"outputs":[],"source":"import struct\nsamples = struct.unpack('h'*f.getnframes(), frames)\nprint(samples[:10])"},{"cell_type":"markdown","metadata":{"_cell_guid":"999b63a3-69a8-30d0-6738-0bf0b9fc870e"},"source":"To get the timing, we'll grab the sampling rate from the wave object."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0a5d554-79f5-4808-c911-fdd0471eb677"},"outputs":[],"source":"framerate = f.getframerate()\nt = [float(i)/framerate for i in range(len(samples))]\nprint(t[:10])"},{"cell_type":"markdown","metadata":{"_cell_guid":"e034f607-9c4f-2c2a-3c0d-64aa4922504a"},"source":"Now we can take a look at the waveform."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c26ad24c-bb71-8769-2416-c2709ab63493"},"outputs":[],"source":"from pylab import *\nplot(t, samples)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}