{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"3ebb8e8b-4526-24f5-208f-ee2cb9f1596b"},"source":"Hugues Fontenelle\n7 October 2016\n\n# Pima Indians Diabetes Database\n## Predict the onset of diabetes based on diagnostic measures\n\nHi folks. I'm new to this, so let me try out what I've learned so far. Your comments are welcome!"},{"cell_type":"markdown","metadata":{"_cell_guid":"47666c75-d504-bf1d-dfef-12bce1cbe8eb"},"source":"First, let's load the data, and split it in four. It is the fold used the authors of the original paper."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9dbddda0-7005-5721-48d4-5a794ffc3dec"},"outputs":[],"source":"import numpy as np\n\nf = open(\"../input/diabetes.csv\")\nf.readline()  # skip the header\ndata = np.loadtxt(f, delimiter = ',')\nX = data[:, :-1]\ny = data[:, -1]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"aadbd4f8-b043-0899-0c2b-17be3c6a7a94"},"source":"Let's try out a bunch of classifiers, all with default parameters."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24637d51-c27f-9876-7c9d-d2259ef91a15"},"outputs":[],"source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"\n        ]\n\nclassifiers = [\n    KNeighborsClassifier(),\n    SVC(kernel=\"linear\"),\n    SVC(kernel=\"rbf\"),\n    GaussianProcessClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    MLPClassifier(),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()\n]"},{"cell_type":"markdown","metadata":{"_cell_guid":"f8bc9411-67fa-c204-db43-bc45a5704d4c"},"source":"Now run all the classifiers, using 5-fold cross validation."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8890a677-3e71-9ea2-ee9b-ac94446151d4"},"outputs":[],"source":"from sklearn.model_selection import cross_val_score\n\n# iterate over classifiers\nresults = {}\nfor name, clf in zip(names, classifiers):\n    scores = cross_val_score(clf, X_train, y_train, cv=5)\n    results[name] = scores"},{"cell_type":"markdown","metadata":{"_cell_guid":"af9d636c-9704-7425-01dd-d6db580d4c4e"},"source":"Here are the results:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"458d31e6-b1f7-8c9f-ecc2-4729f8a3d617"},"outputs":[],"source":"for name, scores in results.items():\n    print(\"%20s | Accuracy: %0.2f%% (+/- %0.2f%%)\" % (name, 100*scores.mean(), 100*scores.std() * 2))"},{"cell_type":"markdown","metadata":{"_cell_guid":"90315be6-17cf-0c9d-5b3f-cb2b826181a1"},"source":"Seems like a Linear SVM performs best.\nLet's try some parameter optimization."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"114b0360-f423-9d8e-cc99-e1ad84759169"},"outputs":[],"source":"from sklearn.grid_search import GridSearchCV\n\nclf = SVC(kernel=\"linear\")\n\n# prepare a range of values to test\nparam_grid = [\n  {'C': [.01, .1, 1, 10], 'kernel': ['linear']},\n ]\n\ngrid = GridSearchCV(estimator=clf, param_grid=param_grid)\ngrid.fit(X_train, y_train)\nprint(grid)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da7a2382-e12d-1c61-ef87-684907f3dbaf"},"outputs":[],"source":"# summarize the results of the grid search\nprint(\"Best score: %0.2f%%\" % (100*grid.best_score_))\nprint(\"Best estimator for parameter C: %f\" % (grid.best_estimator_.C))"},{"cell_type":"markdown","metadata":{"_cell_guid":"b49c3baf-9dba-417f-9d37-5f6ea30652df"},"source":"Finaly, train the Linear SVM (with param `C=0.1`) on the whole train set, and evaluate on the test set"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa723561-4feb-c20c-971e-55b5fcc1cebc"},"outputs":[],"source":"clf = SVC(kernel=\"linear\", C=0.1)\nclf.fit(X_train, y_train)\ny_eval = clf.predict(X_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1898e1cd-08b7-c044-86d3-77155ab7a818"},"outputs":[],"source":"acc = sum(y_eval == y_test) / float(len(y_test))\nprint(\"Accuracy: %.2f%%\" % (100*acc))"},{"cell_type":"markdown","metadata":{"_cell_guid":"1c626fa2-0c33-69cc-54e8-b6868c907ad9"},"source":"We did it :-)"},{"cell_type":"markdown","metadata":{"_cell_guid":"569c56fb-822a-1d19-38bd-92270a8a7989"},"source":"**edit**\n\nI was _probably_ a bit lucky for this particular fold (`random_state=0`). Why would the accuracy on the test be higher than on the optimized trained set? Let's re-run a 5-fold cv on the whole data:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d80d926e-fa4d-4f34-0836-0594168a6601"},"outputs":[],"source":"clf = SVC(kernel=\"linear\", C=0.1)\nscores_final = cross_val_score(clf, X, y, cv=5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cdbcd59f-97ee-0fce-a25c-d63e11e2bf90"},"outputs":[],"source":"scores_final.mean(), scores_final.std()\nprint(\"Final model | Accuracy: %0.2f%% (+/- %0.2f%%)\" % (100*scores_final.mean(), 100*scores_final.std() * 2))"},{"cell_type":"markdown","metadata":{"_cell_guid":"9bf33d09-cc9b-b5c3-49e5-9da5f49b4c71"},"source":"..which is more realistic!\n\nI am wondering, at which stage do I then use this test set?"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}