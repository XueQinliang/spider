{"cells":[{"metadata":{"_uuid":"e12e1e3c00455b325f0e741f4c408dfe929f19ae"},"cell_type":"markdown","source":"**This is my very first kernel ever,so any feedback of yours will be appreciated.**\n\nThe reason I chose this **A Song of Ice and Fire** dataset as my first step is  I am actually a big fan of the TV serial GoT, and I am so into the ASoIaF world.\n\n**\nIf you have any question,please leave a message below and I will check it.**"},{"metadata":{"_uuid":"6b4315be58e6b2739c5c18537aad948321a7d2c7"},"cell_type":"markdown","source":"This kernel is composed of 2 parts: \n\n**1. Character Death Prediction**       **2.Battle Analysis**\n\nIf you are looking for the Battle Analysis,you can skip the former part.\n\nLet's start the prediction now."},{"metadata":{"_uuid":"8ebd38f4f2456a22c78906e17561ade08ef79956"},"cell_type":"markdown","source":"**1. Character Death Prediction**\n\n**Some necessary libs & Read file**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c729250a76c3809abe1023db8c095e2bedcc7a87"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pandas import Series,DataFrame\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"../input/character-predictions.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"560d681be912016929c04ca5f27fc289e97d955b"},"cell_type":"markdown","source":"Data cleansing\n\nThis part is from https://www.kaggle.com/shaildeliwala/exploratory-analysis-and-predictions\nby Shail Deliwala.\n\n**1)  Culture Induction**: Integrate similar culture into one single value.\n\n**2) Drop**: Drop data worthless for prediction like \"name\",\"dataOfBirth\".\n\n**3) Factorization**: Transform discrete data like \"title\",\"mother\",\"culture\" into numerical data.\n\n**4) Fill NaN**:Fill null value with -1."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8c4b29f46b8c880e1e4f93257f4221e78de8a3f3"},"cell_type":"code","source":"#--------------------- Data Cleansing -----------------------------------------\n\n#-------- culture induction -------\ncult = {\n    'Summer Islands': ['summer islands', 'summer islander', 'summer isles'],\n    'Ghiscari': ['ghiscari', 'ghiscaricari',  'ghis'],\n    'Asshai': [\"asshai'i\", 'asshai'],\n    'Lysene': ['lysene', 'lyseni'],\n    'Andal': ['andal', 'andals'],\n    'Braavosi': ['braavosi', 'braavos'],\n    'Dornish': ['dornishmen', 'dorne', 'dornish'],\n    'Myrish': ['myr', 'myrish', 'myrmen'],\n    'Westermen': ['westermen', 'westerman', 'westerlands'],\n    'Westerosi': ['westeros', 'westerosi'],\n    'Stormlander': ['stormlands', 'stormlander'],\n    'Norvoshi': ['norvos', 'norvoshi'],\n    'Northmen': ['the north', 'northmen'],\n    'Free Folk': ['wildling', 'first men', 'free folk'],\n    'Qartheen': ['qartheen', 'qarth'],\n    'Reach': ['the reach', 'reach', 'reachmen'],\n}\n\ndef get_cult(value):\n    value = value.lower()\n    v = [k for (k, v) in cult.items() if value in v]\n    return v[0] if len(v) > 0 else value.title()\n\ndata.loc[:, \"culture\"] = [get_cult(x) for x in data.culture.fillna(\"\")]\n\n#-------- culture induction -------\n\ndata.drop([\"name\", \"alive\", \"pred\", \"plod\", \"isAlive\", \"dateOfBirth\", \"DateoFdeath\"], 1, inplace = True)\n\ndata.loc[:, \"title\"] = pd.factorize(data.title)[0]\ndata.loc[:, \"culture\"] = pd.factorize(data.culture)[0]\ndata.loc[:, \"mother\"] = pd.factorize(data.mother)[0]\ndata.loc[:, \"father\"] = pd.factorize(data.father)[0]\ndata.loc[:, \"heir\"] = pd.factorize(data.heir)[0]\ndata.loc[:, \"house\"] = pd.factorize(data.house)[0]\ndata.loc[:, \"spouse\"] = pd.factorize(data.spouse)[0]\n\ndata.fillna(value = -1, inplace = True)\n''' $$ The code below usually works as a sample equilibrium. However in this case,\n this equilibirium actually decrease our accuracy, all because the original \nprediction data was released without any sample balancing. $$\n\ndata = data[data.actual == 0].sample(350, random_state = 62).append(data[data.actual == 1].sample(350, random_state = 62)).copy(deep = True).astype(np.float64)\n\n'''\nY = data.actual.values\n\nOdata = data.copy(deep=True)\n\ndata.drop([\"actual\"], 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47371e55b3b4f16966c2b4f53fbdafa47dcc4bfd"},"cell_type":"markdown","source":"**Feature Correlation**\n\nSee the correlation of features on heatmap\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"873d7717e74bcbb762eefbfb9e4ac5b378caaed2"},"cell_type":"code","source":"#------------------ Feature Correlation ---------------------------------------\n\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(30,20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85faccc69411f1348dd8e8a1f35fd8d777741873"},"cell_type":"markdown","source":"**Predicting**\n\nPredicting the death of characters by creating 10 different models of Machine Learning"},{"metadata":{"_uuid":"9836a3913dfd77d213e5da95fdf41ac3595e2ea4"},"cell_type":"markdown","source":"**1.RandomForest**\n\n\n\n\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"02644f7f8193e7d8106359e825bd5410b17b4bdf"},"cell_type":"code","source":"#------------------ Predicting ------------------------------------------------\n\n#---------- 1 RandomForest -----------------\ndata.drop([\"S.No\"], 1, inplace = True)\n''' ATTENTION: This rf algorithm achieves 99%+ accuracy, this is because the \\\noriginal predictor-- the document releaser use exactly the same algorithm to predict!\n    '''\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(data, Y)\n\nprint('RandomForest Accuracy£º(original)\\n',random_forest.score(data, Y))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7963a0ec8b4ff122f8096c2706bc1b8fc2809ba6"},"cell_type":"markdown","source":"**2.DecisionTree**\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"91414f83460422dd2d50685ed67b9f79bfb846cc"},"cell_type":"code","source":"#---------- 2 DecisionTree -----------------\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nDT=DecisionTreeClassifier()\n\nDT.fit(data,Y)\n\nprint('DecisionTree Accuracy£º(original)\\n',DT.score(data, Y))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1d0c6afdba8d93b4423085fe466d5ea64b2fd0e"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"ebd5912eff3510ea92b01b281fcb89dd64c483f8"},"cell_type":"markdown","source":"**3.SVC**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0f23627ca3880e05f2c8902ff083cf5961a8a3d6"},"cell_type":"code","source":"#---------- 3 SVC -----------------\n\nfrom sklearn.svm import SVC, LinearSVC\nsvc = SVC()\n\nsvc.fit(data, Y)\n\nprint('SVC Accuracy£º\\n',svc.score(data, Y))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3675ebe27f01e7ddd743d9422add8ea258f0e6e3"},"cell_type":"markdown","source":"**4.LogisticRegression**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6835391edf7dba5a37f61b79981174b466f3721e"},"cell_type":"code","source":"#---------- 4 LogisticRegression -----------------\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\n\nLR.fit(data, Y)\n\nprint('LogisticRegression Accuracy£º\\n',LR.score(data, Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3b792b53d320d101d2868d1091c8d91c662705d"},"cell_type":"markdown","source":"**5.kNN**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"33e4b255a12b44ae7958aeaf121b0e7b975e2c26"},"cell_type":"code","source":"#---------- 5 kNN -----------------\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(data, Y)\n\nprint('kNN Accuracy£º\\n',knn.score(data, Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"677e7652e97681201068f4b6d5d402ea7fdf16df"},"cell_type":"markdown","source":"**6.NaiveBayes Gaussian**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"105886db84cad819a69b193fe8de7690d9892f0c"},"cell_type":"code","source":"#---------- 6 NaiveBayes Gaussian -----------------\n\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\n\ngaussian.fit(data, Y)\n\nprint('gaussian Accuracy£º\\n',gaussian.score(data, Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60c10eb34814d0c6c82243a8ebf9c49b94da7e8b"},"cell_type":"markdown","source":"**7.LogisticRegression with Cross Validation**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eef3d22f9b26ed99023c123422fe2879e4e7b561"},"cell_type":"code","source":"#---------- 7 LogisticRegression with Cross Validation-----------------\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import cross_validation\npredictors=['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']\nalg=LogisticRegression(random_state=1)\nscores=cross_validation.cross_val_score(alg,Odata[predictors],Odata[\"actual\"],cv=3)\nprint('Logistic CrossValidation Accuracy£º\\n',scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ee34c2b4f184876585e92ad6d2d04193c3930cf"},"cell_type":"markdown","source":"**8.LinearRegression with KFold**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"38f948c8bcdd237fad199d18f05f345a7747cd44"},"cell_type":"code","source":"#---------- 8 LinearRegression-----------------\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_validation import KFold\npredictors=['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']\nalg=LinearRegression()\nkf=KFold(Odata.shape[0],n_folds=3,random_state=1)\npredictions=[]\nfor train,test in kf:\n    train_predictors=(Odata[predictors].iloc[train,:])\n    train_target=Odata[\"actual\"].iloc[train]\n    alg.fit(train_predictors,train_target)\n    test_predictions=alg.predict(Odata[predictors].iloc[test,:])\n    predictions.append(test_predictions)\n    \npredictions=np.concatenate(predictions,axis=0)\npredictions[predictions>.5]=1\npredictions[predictions<=.5]=0\naccuracy=sum(predictions==Odata[\"actual\"])/len(predictions)\nprint('LinearRegression Accuracy£º\\n',accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3051692781523e747d4a13db8cdf3fb839b130a"},"cell_type":"markdown","source":"**9.RandomForest with CrossValidation**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8fa32b786ca86ef83337bb95a2063bf783e5ab63"},"cell_type":"code","source":"#---------- 9 RandomForest with CrossValidation -----------------\n\nfrom sklearn import cross_validation\nfrom sklearn.ensemble import RandomForestClassifier\npredictors=['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']\nalg=RandomForestClassifier(random_state=1,n_estimators=150,min_samples_split=12,min_samples_leaf=1)\nkf=cross_validation.KFold(Odata.shape[0],n_folds=3,random_state=1)\nscores=cross_validation.cross_val_score(alg,Odata[predictors],Odata[\"actual\"],cv=kf)\nprint('RandomForest Accuracy£º\\n',scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94ed70e8db1c4f18648c18c32a8a6fb3758c6365"},"cell_type":"markdown","source":"**10.GradientBoosting**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cd28541ef6719a90a19186b4bac4ab64c18157fa"},"cell_type":"code","source":"#---------- 10 GradientBoosting -----------------\n\nfrom sklearn.cross_validation import KFold\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nalgorithms=[\n        [GradientBoostingClassifier(random_state=1,n_estimators=25,max_depth=3),['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']],\n        [LogisticRegression(random_state=1),['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']]]\n \n \nkf=KFold(Odata.shape[0],n_folds=3,random_state=1)\npredictions=[]\nfor train,test in kf:\n    train_target=Odata[\"actual\"].iloc[train]\n    full_test_predictions=[]\n    for alg,predictors in algorithms:\n        alg.fit(Odata[predictors].iloc[train,:],train_target)\n        test_predictions=alg.predict_proba(Odata[predictors].iloc[test,:].astype(float))[:,1]\n        full_test_predictions.append(test_predictions)\n    test_predictions=(full_test_predictions[0]+full_test_predictions[1])/2\n    test_predictions[test_predictions<=.5]=0\n    test_predictions[test_predictions>.5]=1\n    predictions.append(test_predictions)\n\npredictions=np.concatenate(predictions,axis=0)\naccuracy=sum(predictions==Odata[\"actual\"])/len(predictions)\nprint('GradientBoosting Accuracy: \\n',accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e47e1899bc8a73f314d145c6a80565bcc845ad7c"},"cell_type":"markdown","source":"Here is a try of **softmax** unit on** tensorflow**.\nI am not familar with tf, so this is only an attempt.\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a1dc3c49f375ca21bfa65b93ecbe780206843931"},"cell_type":"code","source":"#---------- 11 TensorflowSoftmax -----------------\n\n\nimport tensorflow as tf\n\ndataset_X = data[['title', 'culture', 'mother', 'father', 'heir', 'house', 'spouse', 'male', 'book1', 'book2', 'book3', 'book4', 'book5', 'isAliveFather', 'isAliveMother', 'isAliveHeir', 'isAliveSpouse', 'isMarried', 'isNoble', 'age', 'numDeadRelations', 'boolDeadRelations', 'isPopular', 'popularity']].as_matrix()\n\ndataset_Y = Odata[['actual']].as_matrix()\n\n\nX = tf.placeholder(tf.float32, shape=[None, 24])\ny = tf.placeholder(tf.float32, shape=[None, 1])\n \nweights = tf.Variable(tf.random_normal([24, 2]), name='weights')\nbias = tf.Variable(tf.zeros([2]), name='bias')\ny_pred = tf.nn.softmax(tf.matmul(X, weights) + bias)\n \ncross_entropy = - tf.reduce_sum(y * tf.log(y_pred + 1e-10), reduction_indices=1)\n\ncost = tf.reduce_mean(cross_entropy)\n \n\ntrain_op = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n \n \nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n \n    for epoch in range(50):\n        total_loss = 0.\n        for i in range(len(dataset_X)):\n            # prepare feed data and run\n            feed_dict = {X: [dataset_X[i]], y: [dataset_Y[i]]}\n            _, loss = sess.run([train_op, cost], feed_dict=feed_dict)\n            total_loss += loss\n        # display loss per epoch\n        print('Epoch: %04d, total loss=%.9f' % (epoch + 1, total_loss))\n    print(\"Train Complete\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08f5d0fab5efa45abf3830d278c317131c599fd1"},"cell_type":"markdown","source":"Now we have tried 10 different ML algorithms and 1 network unit.\n\nLet us do some **bagging** and **boosting**!"},{"metadata":{"_uuid":"befe5a77a6abd2fcaebdc07ab6d7b95ce8479742"},"cell_type":"markdown","source":"**Bagged KNN**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1c4da132e59be6de8e53d3a9ac9f19a5d7078828"},"cell_type":"code","source":"# Bagging\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n# Bagged KNN\n\nfrom sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(data,Y)\nresult=cross_val_score(model,data,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d83f38f7d483257a12bf71ee61413cef6f8c53f0"},"cell_type":"markdown","source":"**Bagged Decision Tree**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d40450d1cb9aac7db4cb03514d016dd074449f23"},"cell_type":"code","source":"# Bagged Decision Tree\n\nmodel=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\nmodel.fit(data,Y)\nresult=cross_val_score(model,data,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:',result.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15abffbf24811d50a77c922a8570d8927f62d451"},"cell_type":"markdown","source":"**AdaBoost**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a8c112773383b995ee2c359470ada032b32dcf46"},"cell_type":"code","source":"#------------------------ Boosting --------------------------------------------\n\nfrom sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\nresult=cross_val_score(ada,data,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4669ea9bfaafef8f62fbd6fe3baaa3637ab143eb"},"cell_type":"markdown","source":"**GradientBoost**\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e9b9dbea7b8bfbe2e58a0672dbe9aea55dc479a8"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\nresult=cross_val_score(grad,data,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c366633a4c9fc73bb91f4ffdf016353f5b2419e"},"cell_type":"markdown","source":"**XGBoost**\n\nThe cross validated score for XGBoost is: 0.7449342115910598"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9ed66d7e0189a9352239fe3bbeceda4f3a68a160"},"cell_type":"code","source":"import xgboost as xg\nxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nresult=cross_val_score(xgboost,data,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for XGBoost is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de15cdbf647ba47b4779b69437b2ec386f85dd6f"},"cell_type":"markdown","source":"**Feature Importance in different alg**\n\nLet us see importance of different features in RF and Boosts\n\n\nFrom all four fig we can see the consistency of them. \n\nAlthough the importance order are not exactly the same,**\"popularity\"** and** \"house\" **matters the most which is not a surprise to us. From the point of literary creating, main characters are put much more attention to than others. So it's easy to figure out what author--George.R.R.Martin are trying to express on the main characters he created.**\"House\" **is very importance to differ fate of people without doubt,people's fate are naturally bond with the rise or fall of a great house.\n\n**\"Title\",\"age\" ,\"culture\"**and**\"book5\" **also matters. It's believed noble,elder and appear later in chapters are more likely to survive,you can find related research in hottest kernels if you want."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"82c9334feda302e8b5774633d9eaabb43b5c8b4d"},"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(data,Y)\npd.Series(model.feature_importances_,data.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\nmodel=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\nmodel.fit(data,Y)\npd.Series(model.feature_importances_,data.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(data,Y)\npd.Series(model.feature_importances_,data.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(data,Y)\npd.Series(model.feature_importances_,data.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a719a6e3257975607232052bf039d87e702ff79"},"cell_type":"markdown","source":"Now we have go through the complete predicting\n\nComing next is the **battle analysis**.\n\n\n\n\n---------------------------------------------------------- **Battle Analysis** ------------------------------------------------------------\n\n\n\n\n**Some necessary lib & Read file**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5671890a4c8b9fb6f9296e41a9be174544c31c44"},"cell_type":"code","source":"#-------------------------------- Battle Analysis -------------------------------------------------\n#----------------------------- Data Cleansing------------------------------------\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pandas import Series,DataFrame\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"../input/battles.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9cf8813882a425fcfe9b76647ffe28d3d575ea2"},"cell_type":"markdown","source":"**Fill NaN**\n\nOne battle lose many values,try to fill it with field knowledge : )\n\nA few battles are fight with no king or in nowhere, fill it up."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"76c402a30298979cff8edab259566fb357614b8e"},"cell_type":"code","source":"data.iat[37,13]='win'\ndata.loc[data[\"name\"]==\"Siege of Winterfell\",\"battle_type\"]='siege'\ndata.loc[data[\"name\"]==\"Siege of Winterfell\",\"major_death\"]=0\ndata.loc[data[\"name\"]==\"Siege of Winterfell\",\"major_capture\"]=0\n#----------------------------------------#\ndata['attacker_king'] = data['attacker_king'].fillna('Without a king')   \ndata['defender_king'] = data['defender_king'].fillna('Without a king')\n  \ndata['defender_1'] = data['defender_1'].fillna('common people')\n\ndata['attacker_commander'] = data['attacker_commander'].fillna('Without a commander')   \ndata['defender_commander'] = data['defender_commander'].fillna('Without a commander')\n  \ndata['location'] = data['location'].fillna('Dont know where')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6fdf15fe202eb77a7c2b1d7a58d11d97a3ceff2"},"cell_type":"markdown","source":"**Add Feature **\n\nCompute battle size for statistics."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2adfd7ef1b3631d796ef22d70641075b0fd215de"},"cell_type":"code","source":"#----- Only if we have to full the size of battle\n#attacker_size_mid = data['attacker_size'].median()  \n#defender_size_mid = data['defender_size'].median()\nfor i in range(1,len(data)):\n    if  np.isnan(data.iloc[i,17]) and np.isnan(data.iloc[i,18]):\n        continue\n    elif np.isnan(data.iloc[i,17]):\n        data.iat[i,17] = data.iat[i,18]\n    elif np.isnan(data.iloc[i,18]):\n        data.iat[i,18] = data.iat[i,17]\n        \ndata['battle_size'] = data['attacker_size'] + data['defender_size']\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f3af658d29de1df86304733b4ef30473b683a87"},"cell_type":"markdown","source":"**Data Analysis**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e3c75408f2328392216dd651642dc055165915fe"},"cell_type":"code","source":"#-------------------------------Data Analysis----------------------------------\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"31623703eb97287b5cc6be27d1ad4a21d05e43db"},"cell_type":"code","source":"# 1 battle type-----------------\nplt.figure(figsize=[10,6])\ndata.battle_type.value_counts().plot(kind='bar')\nplt.title(\"battle type\") \nplt.ylabel(\"count\") \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"13ccfe32d766b7a0e29f335fda8081f797bc7bd2"},"cell_type":"code","source":"# 2 battle region---------------\n\nfig, axis1 = plt.subplots(1,1,figsize=(10,4))\naxis1.set_title(\"region distribution\")\nsns.countplot(x='region', data=data, ax=axis1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"41dc2690b6faa6795563818e0ae4985c146f2d62"},"cell_type":"code","source":"# 3 battle size-----------------\nfig, axis1 = plt.subplots(1,1,figsize=(30,8))\naxis1.set_title(\"battle size\")\nsns.barplot(x='name', y='battle_size', data=data, ax=axis1)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"75db5572d29319d03cf71601c48940680946cc15"},"cell_type":"code","source":"# 4 win&defeat board------------\n\nkingd = data['defender_king']\nkinga = data['attacker_king']\nking = kinga.append(kingd)\nking = king.drop_duplicates()\n\nl = [0]\nl = l * len(king)\nw = dict(zip(king,l))\nd = dict(zip(king,l))\n\nfor i in range(0,len(data)):\n    if  data.iloc[i,13]=='win':\n        w[data.iloc[i,3]]+=1\n        d[data.iloc[i,4]]+=1\n    else:\n        w[data.iloc[i,4]]+=1\n        d[data.iloc[i,3]]+=1\n        \nplt.figure(figsize=[15,6])\nplt.bar(range(len(w)), list(w.values()), align='center')\nplt.xticks(range(len(w)), list(w.keys()))\nplt.show()\nplt.figure(figsize=[15,6])\nplt.bar(range(len(d)), list(d.values()), align='center')\nplt.xticks(range(len(d)), list(d.keys()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# 5 army size that house put\n\nhouse = dict(zip(king,l))\n\nfor i in range(0,len(data)):\n    if data.iloc[i,17] // 1 == data.iloc[i,17]:\n        house[data.iloc[i,3]] += data.iloc[i,17]\n    if data.iloc[i,18] // 1 == data.iloc[i,18]:\n        house[data.iloc[i,4]] += data.iloc[i,18]\n    print(i,house)\n\nplt.figure(figsize=[15,6])\nplt.bar(range(len(house)), list(house.values()), align='center')\nplt.xticks(range(len(house)), list(house.keys()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"821c6c04593c9266f2a07f6a98a0ad6ae589a210"},"cell_type":"markdown","source":"That's all process of Battle Analysis.\n\n\nThanks if you read all the way through,leave a message if you have any question"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}